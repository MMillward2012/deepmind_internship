{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/matthew/Documents/deepmind_internship\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0_setup.ipynb           2_train_finbert.ipynb   3_convert_to_onnx.ipynb\n",
            "1_data_processing.ipynb 3_benchmarks copy.ipynb 4_explainability.ipynb\n",
            "2_train.ipynb           3_benchmarks.ipynb      9_fine_tune.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Setup & Configuration\n",
        "import gc\n",
        "import logging\n",
        "import platform\n",
        "import statistics\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass, asdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "import pandas as pd\n",
        "import psutil\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer\n",
        "import pickle\n",
        "\n",
        "# Configure logging for clear output in the notebook\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkConfig:\n",
        "    \"\"\"Configuration for the entire benchmarking run.\"\"\"\n",
        "    benchmark_iterations: int = 100\n",
        "    warmup_iterations: int = 20\n",
        "    batch_sizes: List[int] = None\n",
        "    accuracy_sample_size: int = 500\n",
        "    test_csv_path: Optional[str] = None\n",
        "    device_mode: str = \"auto\"  # \"auto\", \"cpu\", or \"gpu\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.batch_sizes is None:\n",
        "            self.batch_sizes = [1, 2, 4, 8]\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkResult:\n",
        "    \"\"\"A structured class to hold results from a single benchmark run.\"\"\"\n",
        "    model: str\n",
        "    batch_size: int\n",
        "    avg_latency_ms: float\n",
        "    p95_latency_ms: float\n",
        "    throughput_samples_per_sec: float\n",
        "    peak_memory_mb: float\n",
        "    model_size_mb: float\n",
        "    provider: str\n",
        "    accuracy: Optional[float] = None\n",
        "    f1_score: Optional[float] = None\n",
        "    weighted_accuracy: Optional[float] = None\n",
        "    weighted_f1_score: Optional[float] = None\n",
        "    # UPDATED: Added new fields for confidence and per-class metrics\n",
        "    avg_confidence_correct: Optional[float] = None\n",
        "    avg_confidence_incorrect: Optional[float] = None\n",
        "    per_class_metrics: Optional[Dict] = None\n",
        "\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        # UPDATED: Flatten the per_class_metrics for easier CSV export\n",
        "        flat_dict = asdict(self)\n",
        "        per_class = flat_dict.pop(\"per_class_metrics\", {})\n",
        "        if per_class:\n",
        "            for class_name, metrics in per_class.items():\n",
        "                for metric_name, value in metrics.items():\n",
        "                    flat_dict[f\"{class_name}_{metric_name}\"] = value\n",
        "        return flat_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Hardware & Model Loading Components\n",
        "class ExecutionProviderManager:\n",
        "    \"\"\"Manages ONNX execution providers based on platform and preferences.\"\"\"\n",
        "    @staticmethod\n",
        "    def get_execution_providers(mode: str = \"auto\") -> List: # Return type is now just List\n",
        "        if platform.system() == \"Darwin\" and ort.get_device() == \"ARM64\":\n",
        "            # This provider requires special flags to be enabled\n",
        "            return [\n",
        "                ('CoreMLExecutionProvider', {\n",
        "                    'coreml_flags': 'COREML_FLAG_ENABLE_ON_SUBGRAPH'\n",
        "                }),\n",
        "                'CPUExecutionProvider'\n",
        "            ]\n",
        "\n",
        "        # Fallback for other systems (Linux/Windows with GPU)\n",
        "        available = ort.get_available_providers()\n",
        "        preferences = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
        "        chosen = [p for p in preferences if p in available]\n",
        "        return chosen\n",
        "\n",
        "class ModelLoader:\n",
        "    \"\"\"Handles loading an ONNX model into an inference session.\"\"\"\n",
        "    @staticmethod\n",
        "    def load_onnx_session(onnx_path: Path, providers: List[str]) -> ort.InferenceSession:\n",
        "        opts = ort.SessionOptions()\n",
        "        opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "        return ort.InferenceSession(str(onnx_path), providers=providers, sess_options=opts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Fixed Data Handling Component\n",
        "class DataProcessor:\n",
        "    \"\"\"Handles data preprocessing and batch preparation - FIXED for consistency with training.\"\"\"\n",
        "    def __init__(self, tokenizer, max_length: int = 128):\n",
        "        self.tokenizer, self.max_length = tokenizer, max_length\n",
        "        self.example_inputs = [\"Stocks surged after the company reported record earnings.\"]\n",
        "        self.label_encoder = None\n",
        "\n",
        "    def prepare_batch_inputs(self, texts: List[str]) -> Dict[str, np.ndarray]:\n",
        "        encoding = self.tokenizer(\n",
        "            texts, return_tensors=\"np\", max_length=self.max_length,\n",
        "            padding=\"max_length\", truncation=True\n",
        "        )\n",
        "        return {k: v.astype(np.int64) for k, v in encoding.items()}\n",
        "\n",
        "    def load_label_encoder(self, model_dir: Path) -> LabelEncoder:\n",
        "        \"\"\"Load the label encoder used during training.\"\"\"\n",
        "        label_encoder_path = model_dir / \"label_encoder.pkl\"\n",
        "        if label_encoder_path.exists():\n",
        "            with open(label_encoder_path, 'rb') as f:\n",
        "                self.label_encoder = pickle.load(f)\n",
        "                return self.label_encoder\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def load_test_dataset(self, csv_path: Path, model_dir: Path) -> Tuple[List[str], List[int]]:\n",
        "        \"\"\"Load test dataset using EXACT same preprocessing as training.\"\"\"\n",
        "        self.load_label_encoder(model_dir)\n",
        "        \n",
        "        df = pd.read_csv(csv_path, header=None, names=[\"label\", \"sentence\"], encoding=\"latin-1\")\n",
        "        df[\"sentence\"] = df[\"sentence\"].str.strip('\"')\n",
        "        \n",
        "        if self.label_encoder is not None:\n",
        "            df[\"label_encoded\"] = self.label_encoder.transform(df[\"label\"])\n",
        "        else:\n",
        "            logger.warning(\"Using fallback label encoding - this might cause accuracy issues!\")\n",
        "            unique_labels = sorted(df[\"label\"].unique())\n",
        "            label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "            df[\"label_encoded\"] = df[\"label\"].map(label_to_id)\n",
        "        \n",
        "        _, test_df = train_test_split(\n",
        "            df, test_size=0.25, random_state=42, stratify=df[\"label\"]\n",
        "        )\n",
        "        \n",
        "        return test_df[\"sentence\"].tolist(), test_df[\"label_encoded\"].astype(int).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Performance Measurement Components\n",
        "class PerformanceMonitor:\n",
        "    \"\"\"Monitors system performance during benchmarking.\"\"\"\n",
        "    @staticmethod\n",
        "    def measure_memory_usage() -> float:\n",
        "        return psutil.Process().memory_info().rss / (1024**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model_size_mb(onnx_path: Path) -> float:\n",
        "        return onnx_path.stat().st_size / (1024**2)\n",
        "\n",
        "class LatencyBenchmarker:\n",
        "    \"\"\"Handles the details of latency benchmarking.\"\"\"\n",
        "    def __init__(self, config: BenchmarkConfig):\n",
        "        self.config = config\n",
        "    \n",
        "    def warmup_session(self, session: ort.InferenceSession, inputs: Dict[str, np.ndarray]):\n",
        "        for _ in range(self.config.warmup_iterations):\n",
        "            session.run(None, inputs)\n",
        "    \n",
        "    def measure_latency(self, session: ort.InferenceSession, inputs: Dict[str, np.ndarray]) -> List[float]:\n",
        "        times = []\n",
        "        for _ in range(self.config.benchmark_iterations):\n",
        "            start = time.perf_counter()\n",
        "            session.run(None, inputs)\n",
        "            times.append((time.perf_counter() - start) * 1000)\n",
        "        return times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Enhanced Accuracy Evaluation Component\n",
        "\n",
        "# UPDATED: New comprehensive metrics calculation function\n",
        "def calculate_ordinal_metrics(y_true, y_pred, y_prob, labels_map):\n",
        "    \"\"\"\n",
        "    Calculates standard, ordinally-weighted, confidence, and per-class metrics.\n",
        "    'labels_map' should be an ordered list of class names, e.g., ['negative', 'neutral', 'positive'].\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_prob = np.array(y_prob)\n",
        "\n",
        "    # --- Standard & Ordinal Metrics ---\n",
        "    standard_accuracy = accuracy_score(y_true, y_pred)\n",
        "    standard_f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
        "    \n",
        "    penalized_y_pred = [true if abs(true - pred) <= 1 else pred for true, pred in zip(y_true, y_pred)]\n",
        "    weighted_accuracy = accuracy_score(y_true, penalized_y_pred)\n",
        "    weighted_f1 = f1_score(y_true, penalized_y_pred, average=\"weighted\", zero_division=0)\n",
        "\n",
        "    # --- Confidence Analysis ---\n",
        "    is_correct_mask = (y_true == y_pred)\n",
        "    confidences = y_prob.max(axis=1)\n",
        "    \n",
        "    avg_conf_correct = np.mean(confidences[is_correct_mask]) if np.any(is_correct_mask) else 0.0\n",
        "    avg_conf_incorrect = np.mean(confidences[~is_correct_mask]) if np.any(~is_correct_mask) else 0.0\n",
        "\n",
        "    # --- Per-Class Metrics ---\n",
        "    p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average=None, labels=range(len(labels_map)), zero_division=0)\n",
        "    per_class_metrics = {\n",
        "        labels_map[i]: {\"precision\": p[i], \"recall\": r[i], \"f1_score\": f[i], \"support\": s[i]}\n",
        "        for i in range(len(labels_map))\n",
        "    }\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": standard_accuracy,\n",
        "        \"f1_score\": standard_f1,\n",
        "        \"weighted_accuracy\": weighted_accuracy,\n",
        "        \"weighted_f1_score\": weighted_f1,\n",
        "        \"avg_confidence_correct\": avg_conf_correct,\n",
        "        \"avg_confidence_incorrect\": avg_conf_incorrect,\n",
        "        \"per_class_metrics\": per_class_metrics\n",
        "    }\n",
        "\n",
        "\n",
        "class AccuracyEvaluator:\n",
        "    \"\"\"Handles model accuracy and F1 score evaluation with enhanced debugging.\"\"\"\n",
        "    def __init__(self, session: ort.InferenceSession, data_processor: DataProcessor):\n",
        "        self.session, self.data_processor = session, data_processor\n",
        "    \n",
        "    def evaluate(self, texts: List[str], labels: List[int], batch_size: int, max_samples: int):\n",
        "        num_samples = min(len(texts), max_samples)\n",
        "        eval_texts, eval_labels = texts[:num_samples], labels[:num_samples]\n",
        "        \n",
        "        all_predictions = []\n",
        "        all_probabilities = []\n",
        "        \n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch_texts = eval_texts[i: i + batch_size]\n",
        "            inputs = self.data_processor.prepare_batch_inputs(batch_texts)\n",
        "            \n",
        "            model_inputs = {inp.name for inp in self.session.get_inputs()}\n",
        "            valid_inputs = {k: v for k, v in inputs.items() if k in model_inputs}\n",
        "            \n",
        "            outputs = self.session.run(None, valid_inputs)\n",
        "            logits = outputs[0]\n",
        "            \n",
        "            probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
        "            all_probabilities.extend(probabilities)\n",
        "            \n",
        "            predictions = np.argmax(logits, axis=1)\n",
        "            all_predictions.extend(predictions)\n",
        "        \n",
        "        # UPDATED: Call the new comprehensive metrics function\n",
        "        if self.data_processor.label_encoder:\n",
        "            ordered_labels = self.data_processor.label_encoder.classes_\n",
        "            metrics = calculate_ordinal_metrics(eval_labels, all_predictions, all_probabilities, ordered_labels)\n",
        "            return metrics\n",
        "        else: # Fallback\n",
        "            accuracy = accuracy_score(eval_labels, all_predictions)\n",
        "            f1 = f1_score(eval_labels, all_predictions, average=\"weighted\")\n",
        "            return {\"accuracy\": accuracy, \"f1_score\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Main Orchestrator Class\n",
        "class ONNXModelBenchmarker:\n",
        "    \"\"\"Orchestrates all components to run a benchmark for a single model.\"\"\"\n",
        "    def __init__(self, config: BenchmarkConfig, tokenizer, model_dir: Path):\n",
        "        self.config = config\n",
        "        self.model_dir = model_dir\n",
        "        self.data_processor = DataProcessor(tokenizer, max_length=128)\n",
        "        self.latency_benchmarker = LatencyBenchmarker(config)\n",
        "    \n",
        "    def benchmark_model(self, model_name: str, onnx_path: Path, batch_size: int) -> Optional[BenchmarkResult]:\n",
        "        print(f\"   -> Running benchmark for batch size: {batch_size}\")\n",
        "        try:\n",
        "            providers = ExecutionProviderManager.get_execution_providers(self.config.device_mode)\n",
        "            session = ModelLoader.load_onnx_session(onnx_path, providers)\n",
        "\n",
        "            inputs = self.data_processor.prepare_batch_inputs(self.data_processor.example_inputs * batch_size)\n",
        "            model_inputs = {inp.name for inp in session.get_inputs()}\n",
        "            valid_inputs = {k: v for k, v in inputs.items() if k in model_inputs}\n",
        "\n",
        "            self.latency_benchmarker.warmup_session(session, valid_inputs)\n",
        "            times = self.latency_benchmarker.measure_latency(session, valid_inputs)\n",
        "            \n",
        "            avg_latency = statistics.mean(times)\n",
        "            p95_latency = np.percentile(times, 95)\n",
        "            \n",
        "            metrics = {}\n",
        "            if self.config.test_csv_path:\n",
        "                evaluator = AccuracyEvaluator(session, self.data_processor)\n",
        "                texts, labels = self.data_processor.load_test_dataset(\n",
        "                    Path(self.config.test_csv_path), self.model_dir\n",
        "                )\n",
        "                metrics = evaluator.evaluate(texts, labels, batch_size, self.config.accuracy_sample_size)\n",
        "\n",
        "            # UPDATED: Unpack all metrics from the dictionary into the result object\n",
        "            return BenchmarkResult(\n",
        "                model=model_name, batch_size=batch_size,\n",
        "                avg_latency_ms=avg_latency, p95_latency_ms=p95_latency,\n",
        "                throughput_samples_per_sec=(1000 * batch_size) / avg_latency if avg_latency > 0 else 0,\n",
        "                peak_memory_mb=PerformanceMonitor.measure_memory_usage(),\n",
        "                model_size_mb=PerformanceMonitor.get_model_size_mb(onnx_path),\n",
        "                provider=session.get_providers()[0],\n",
        "                accuracy=metrics.get(\"accuracy\"),\n",
        "                f1_score=metrics.get(\"f1_score\"),\n",
        "                weighted_accuracy=metrics.get(\"weighted_accuracy\"),\n",
        "                weighted_f1_score=metrics.get(\"weighted_f1_score\"),\n",
        "                avg_confidence_correct=metrics.get(\"avg_confidence_correct\"),\n",
        "                avg_confidence_incorrect=metrics.get(\"avg_confidence_incorrect\"),\n",
        "                per_class_metrics=metrics.get(\"per_class_metrics\")\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Benchmark failed for {model_name} (batch {batch_size}): {e}\", exc_info=True)\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Results Management\n",
        "class ResultsManager:\n",
        "    \"\"\"Manages benchmark results and reporting.\"\"\"\n",
        "    @staticmethod\n",
        "    def save_results(results: List[BenchmarkResult], output_dir: Path = Path(\"results\")):\n",
        "        if not results: return\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "        df = pd.DataFrame([r.to_dict() for r in results])\n",
        "        df.to_csv(output_dir / \"benchmark_results_debugging.csv\", index=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def print_summary(results: List[BenchmarkResult]):\n",
        "        if not results: return\n",
        "        df = pd.DataFrame([r.to_dict() for r in results if r])\n",
        "        \n",
        "        # UPDATED: Main summary table with new high-level metrics\n",
        "        main_summary_cols = [\n",
        "            \"model\", \"batch_size\", \"provider\", \"avg_latency_ms\", \"accuracy\", \"f1_score\", \n",
        "            \"weighted_f1_score\", \"avg_confidence_correct\", \"avg_confidence_incorrect\"\n",
        "        ]\n",
        "        # Filter out columns that may not exist if a run failed partially\n",
        "        main_summary_cols = [col for col in main_summary_cols if col in df.columns]\n",
        "\n",
        "        print(\"\\n\" + \"=\"*120 + \"\\n📊 BENCHMARK SUMMARY\\n\" + \"=\"*120)\n",
        "        print(df[main_summary_cols].to_string(index=False, float_format=\"%.3f\"))\n",
        "        print(\"=\"*120)\n",
        "\n",
        "        # UPDATED: Separate, detailed report for per-class metrics\n",
        "        print(\"\\n\" + \"=\"*120 + \"\\n🔬 PER-CLASS METRICS (PRECISION / RECALL / F1-SCORE)\\n\" + \"=\"*120)\n",
        "        for _, row in df.iterrows():\n",
        "            print(f\"\\nModel: {row['model']} | Batch Size: {row['batch_size']}\")\n",
        "            \n",
        "            # Reconstruct per-class metrics from flattened CSV columns\n",
        "            class_names = sorted(list(set([k.split('_')[0] for k in df.columns if '_precision' in k])))\n",
        "            \n",
        "            header = f\"{'CLASS':<15}\" + \"\".join([f\"{metric.upper():>12}\" for metric in [\"PRECISION\", \"RECALL\", \"F1-SCORE\", \"SUPPORT\"]])\n",
        "            print(header)\n",
        "            print(\"-\" * len(header))\n",
        "\n",
        "            for name in class_names:\n",
        "                p = row.get(f\"{name}_precision\", 0.0)\n",
        "                r = row.get(f\"{name}_recall\", 0.0)\n",
        "                f1 = row.get(f\"{name}_f1_score\", 0.0)\n",
        "                s = int(row.get(f\"{name}_support\", 0))\n",
        "                print(f\"{name:<15}{p:>12.3f}{r:>12.3f}{f1:>12.3f}{s:>12d}\")\n",
        "        print(\"=\"*120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Model Discovery Functions\n",
        "def discover_models(models_dir: str) -> List[Tuple[str, Path, Path]]:\n",
        "    \"\"\"Discover all available ONNX models, returning (name, onnx_path, model_dir).\"\"\"\n",
        "    valid_models = []\n",
        "    for model_dir in Path(models_dir).iterdir():\n",
        "        if not model_dir.is_dir() or not (model_dir / \"onnx\").exists(): \n",
        "            continue\n",
        "        \n",
        "        # Add both standard and quantized models if they exist\n",
        "        standard_path = model_dir / \"onnx\" / \"model.onnx\"\n",
        "        if standard_path.exists():\n",
        "            valid_models.append((f\"{model_dir.name}-standard\", standard_path, model_dir))\n",
        "        \n",
        "        # quant_path = model_dir / \"onnx\" / \"model-quantised.onnx\"\n",
        "        # if quant_path.exists():\n",
        "        #     valid_models.append((f\"{model_dir.name}-quant\", quant_path, model_dir))\n",
        "\n",
        "    return valid_models\n",
        "\n",
        "def run_full_benchmark(models_dir: str, config: BenchmarkConfig):\n",
        "    \"\"\"Run the full benchmark suite on all discovered models.\"\"\"\n",
        "    all_results = []\n",
        "    \n",
        "    valid_models = discover_models(models_dir)\n",
        "    if not valid_models: return\n",
        "\n",
        "    for model_name, onnx_path, model_dir in valid_models:\n",
        "        print(f\"\\n--- ⏳ Now processing: {model_name} ---\")\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load tokenizer for {model_name}: {e}\")\n",
        "            continue\n",
        "        \n",
        "        for batch_size in config.batch_sizes:\n",
        "            # Create a new benchmarker instance for EACH run to prevent state leakage\n",
        "            benchmarker = ONNXModelBenchmarker(config, tokenizer, model_dir)\n",
        "            \n",
        "            result = benchmarker.benchmark_model(model_name, onnx_path, batch_size)\n",
        "            if result: all_results.append(result)\n",
        "    \n",
        "    if all_results:\n",
        "        ResultsManager.print_summary(all_results)\n",
        "        ResultsManager.save_results(all_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ⏳ Now processing: all-MiniLM-L6-v2-financial-sentiment-standard ---\n",
            "   -> Running benchmark for batch size: 1\n",
            "   -> Running benchmark for batch size: 4\n",
            "   -> Running benchmark for batch size: 8\n",
            "\n",
            "--- ⏳ Now processing: distilbert-financial-sentiment-standard ---\n",
            "   -> Running benchmark for batch size: 1\n",
            "   -> Running benchmark for batch size: 4\n",
            "   -> Running benchmark for batch size: 8\n",
            "\n",
            "--- ⏳ Now processing: finbert-tone-financial-sentiment-standard ---\n",
            "   -> Running benchmark for batch size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matthew/Documents/deepmind_internship/venv-py311/lib/python3.11/site-packages/sklearn/base.py:440: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   -> Running benchmark for batch size: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matthew/Documents/deepmind_internship/venv-py311/lib/python3.11/site-packages/sklearn/base.py:440: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   -> Running benchmark for batch size: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/matthew/Documents/deepmind_internship/venv-py311/lib/python3.11/site-packages/sklearn/base.py:440: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- ⏳ Now processing: tinybert-financial-classifier-standard ---\n",
            "   -> Running benchmark for batch size: 1\n",
            "   -> Running benchmark for batch size: 4\n",
            "   -> Running benchmark for batch size: 8\n",
            "\n",
            "--- ⏳ Now processing: mobilebert-uncased-financial-sentiment-standard ---\n",
            "   -> Running benchmark for batch size: 1\n",
            "   -> Running benchmark for batch size: 4\n",
            "   -> Running benchmark for batch size: 8\n",
            "\n",
            "========================================================================================================================\n",
            "📊 BENCHMARK SUMMARY\n",
            "========================================================================================================================\n",
            "                                          model  batch_size             provider  avg_latency_ms  accuracy  f1_score  weighted_f1_score  avg_confidence_correct  avg_confidence_incorrect\n",
            "  all-MiniLM-L6-v2-financial-sentiment-standard           1 CPUExecutionProvider          28.146     0.786     0.789              0.960                   0.786                     0.637\n",
            "  all-MiniLM-L6-v2-financial-sentiment-standard           4 CPUExecutionProvider         101.021     0.786     0.789              0.960                   0.786                     0.637\n",
            "  all-MiniLM-L6-v2-financial-sentiment-standard           8 CPUExecutionProvider         212.438     0.786     0.789              0.960                   0.786                     0.637\n",
            "        distilbert-financial-sentiment-standard           1 CPUExecutionProvider         110.869     0.834     0.835              0.982                   0.927                     0.788\n",
            "        distilbert-financial-sentiment-standard           4 CPUExecutionProvider         366.949     0.834     0.835              0.982                   0.927                     0.788\n",
            "        distilbert-financial-sentiment-standard           8 CPUExecutionProvider         827.108     0.834     0.835              0.982                   0.927                     0.788\n",
            "      finbert-tone-financial-sentiment-standard           1 CPUExecutionProvider         188.630     0.843     0.844              0.981                   0.947                     0.841\n",
            "      finbert-tone-financial-sentiment-standard           4 CPUExecutionProvider         743.395     0.843     0.844              0.981                   0.947                     0.841\n",
            "      finbert-tone-financial-sentiment-standard           8 CPUExecutionProvider        1576.040     0.843     0.844              0.981                   0.947                     0.841\n",
            "         tinybert-financial-classifier-standard           1 CPUExecutionProvider          13.260     0.791     0.793              0.973                   0.754                     0.645\n",
            "         tinybert-financial-classifier-standard           4 CPUExecutionProvider          45.237     0.791     0.793              0.973                   0.754                     0.645\n",
            "         tinybert-financial-classifier-standard           8 CPUExecutionProvider          90.126     0.791     0.793              0.973                   0.754                     0.645\n",
            "mobilebert-uncased-financial-sentiment-standard           1 CPUExecutionProvider          62.806     0.817     0.816              0.979                   0.871                     0.727\n",
            "mobilebert-uncased-financial-sentiment-standard           4 CPUExecutionProvider         214.026     0.817     0.816              0.979                   0.871                     0.727\n",
            "mobilebert-uncased-financial-sentiment-standard           8 CPUExecutionProvider         375.237     0.817     0.816              0.979                   0.871                     0.727\n",
            "========================================================================================================================\n",
            "\n",
            "========================================================================================================================\n",
            "🔬 PER-CLASS METRICS (PRECISION / RECALL / F1-SCORE)\n",
            "========================================================================================================================\n",
            "\n",
            "Model: all-MiniLM-L6-v2-financial-sentiment-standard | Batch Size: 1\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.653       0.845       0.736         129\n",
            "neutral               0.885       0.817       0.849         601\n",
            "positive              0.669       0.689       0.679         270\n",
            "\n",
            "Model: all-MiniLM-L6-v2-financial-sentiment-standard | Batch Size: 4\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.653       0.845       0.736         129\n",
            "neutral               0.885       0.817       0.849         601\n",
            "positive              0.669       0.689       0.679         270\n",
            "\n",
            "Model: all-MiniLM-L6-v2-financial-sentiment-standard | Batch Size: 8\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.653       0.845       0.736         129\n",
            "neutral               0.885       0.817       0.849         601\n",
            "positive              0.669       0.689       0.679         270\n",
            "\n",
            "Model: distilbert-financial-sentiment-standard | Batch Size: 1\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.743       0.876       0.804         129\n",
            "neutral               0.899       0.849       0.873         601\n",
            "positive              0.751       0.781       0.766         270\n",
            "\n",
            "Model: distilbert-financial-sentiment-standard | Batch Size: 4\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.743       0.876       0.804         129\n",
            "neutral               0.899       0.849       0.873         601\n",
            "positive              0.751       0.781       0.766         270\n",
            "\n",
            "Model: distilbert-financial-sentiment-standard | Batch Size: 8\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.743       0.876       0.804         129\n",
            "neutral               0.899       0.849       0.873         601\n",
            "positive              0.751       0.781       0.766         270\n",
            "\n",
            "Model: finbert-tone-financial-sentiment-standard | Batch Size: 1\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.697       0.891       0.782         129\n",
            "neutral               0.904       0.862       0.882         601\n",
            "positive              0.802       0.778       0.789         270\n",
            "\n",
            "Model: finbert-tone-financial-sentiment-standard | Batch Size: 4\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.697       0.891       0.782         129\n",
            "neutral               0.904       0.862       0.882         601\n",
            "positive              0.802       0.778       0.789         270\n",
            "\n",
            "Model: finbert-tone-financial-sentiment-standard | Batch Size: 8\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.697       0.891       0.782         129\n",
            "neutral               0.904       0.862       0.882         601\n",
            "positive              0.802       0.778       0.789         270\n",
            "\n",
            "Model: tinybert-financial-classifier-standard | Batch Size: 1\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.655       0.853       0.741         129\n",
            "neutral               0.869       0.820       0.844         601\n",
            "positive              0.709       0.696       0.703         270\n",
            "\n",
            "Model: tinybert-financial-classifier-standard | Batch Size: 4\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.655       0.853       0.741         129\n",
            "neutral               0.869       0.820       0.844         601\n",
            "positive              0.709       0.696       0.703         270\n",
            "\n",
            "Model: tinybert-financial-classifier-standard | Batch Size: 8\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.655       0.853       0.741         129\n",
            "neutral               0.869       0.820       0.844         601\n",
            "positive              0.709       0.696       0.703         270\n",
            "\n",
            "Model: mobilebert-uncased-financial-sentiment-standard | Batch Size: 1\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.705       0.907       0.793         129\n",
            "neutral               0.868       0.862       0.865         601\n",
            "positive              0.768       0.674       0.718         270\n",
            "\n",
            "Model: mobilebert-uncased-financial-sentiment-standard | Batch Size: 4\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.705       0.907       0.793         129\n",
            "neutral               0.868       0.862       0.865         601\n",
            "positive              0.768       0.674       0.718         270\n",
            "\n",
            "Model: mobilebert-uncased-financial-sentiment-standard | Batch Size: 8\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.705       0.907       0.793         129\n",
            "neutral               0.868       0.862       0.865         601\n",
            "positive              0.768       0.674       0.718         270\n",
            "========================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Cell 9: Execute the Benchmark\n",
        "# --- Define configuration and run the benchmark ---\n",
        "MODELS_DIRECTORY = \"models\"\n",
        "ACCURACY_DATASET_PATH = \"data/FinancialPhraseBank/all-data.csv\"\n",
        "\n",
        "# Create the configuration for this run\n",
        "benchmark_config = BenchmarkConfig(\n",
        "    batch_sizes=[1, 4, 8],\n",
        "    test_csv_path=ACCURACY_DATASET_PATH,\n",
        "    benchmark_iterations=50,\n",
        "    accuracy_sample_size=1000\n",
        ")\n",
        "\n",
        "# Run the benchmark\n",
        "run_full_benchmark(MODELS_DIRECTORY, benchmark_config)\n",
        "\n",
        "logger.info(\"\\n--- Benchmark Finished ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPEaWosSVJk00KG/SJV/J7B",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
