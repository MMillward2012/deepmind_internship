{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/matthew/Documents/deepmind_internship\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 Configuration loaded! Run all cells to execute benchmark with these settings.\n"
          ]
        }
      ],
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "# 🎯 BENCHMARK CONFIGURATION - CHANGE THESE SETTINGS TO CONTROL YOUR BENCHMARK\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# 1. MODEL SELECTION - Choose which models to benchmark\n",
        "# Simply comment/uncomment the models you want to test:\n",
        "TARGET_MODELS = [\n",
        "    \"tinybert-financial-classifier\",\n",
        "    \"tinybert-financial-classifier-fine-tuned\", \n",
        "    \"tinybert-financial-classifier-pruned\",\n",
        "    # \"finbert-tone-financial-sentiment\",\n",
        "    # \"distilbert-financial-sentiment\",\n",
        "    # \"all-MiniLM-L6-v2-financial-sentiment\",\n",
        "    # \"mobilebert-uncased-financial-sentiment\",\n",
        "    # \"SmolLM2-360M-Instruct-financial-sentiment\",\n",
        "]\n",
        "\n",
        "# Set to None to test ALL models found in the models/ directory:\n",
        "# TARGET_MODELS = None\n",
        "\n",
        "# 2. SPEED vs THOROUGHNESS - Choose your trade-off  \n",
        "FAST_MODE = True              # ← CHANGE THIS:\n",
        "                              # True  = Quick testing (15 iterations, batches [1,8])\n",
        "                              # False = Thorough testing (30 iterations, batches [1,4,8,16])\n",
        "\n",
        "# 3. ACCURACY EVALUATION - Include accuracy metrics?\n",
        "INCLUDE_ACCURACY = True       # ← CHANGE THIS:\n",
        "                              # True  = Include accuracy evaluation (slower)\n",
        "                              # False = Skip accuracy evaluation (faster)\n",
        "\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "# 🔧 ADVANCED SETTINGS - Only change these if you know what you're doing\n",
        "# ═══════════════════════════════════════════════════════════════════════════════\n",
        "\n",
        "# Data paths (usually don't need to change these)\n",
        "MODELS_DIRECTORY = \"models\"\n",
        "ACCURACY_DATASET_PATH = \"data/FinancialPhraseBank/all-data.csv\"\n",
        "\n",
        "# Custom batch sizes (override the fast/slow mode defaults)\n",
        "CUSTOM_BATCH_SIZES = None     # ← Example: [1, 2, 4] or leave as None to use defaults\n",
        "\n",
        "# Custom iteration counts (override the fast/slow mode defaults)  \n",
        "CUSTOM_ITERATIONS = None      # ← Example: 50 or leave as None to use defaults\n",
        "CUSTOM_WARMUP = None          # ← Example: 20 or leave as None to use defaults\n",
        "CUSTOM_SAMPLE_SIZE = None     # ← Example: 1000 or leave as None to use defaults\n",
        "\n",
        "print(\"📋 Configuration loaded! Run all cells to execute benchmark with these settings.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "README.md        \u001b[34mdata\u001b[m\u001b[m             \u001b[34mnotebooks\u001b[m\u001b[m        \u001b[34mresults\u001b[m\u001b[m\n",
            "\u001b[34manalysis_results\u001b[m\u001b[m \u001b[34mmodels\u001b[m\u001b[m           requirements.txt \u001b[34mvenv-py311\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Setup & Configuration\n",
        "import gc\n",
        "import logging\n",
        "import platform\n",
        "import statistics\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass, asdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "import pandas as pd\n",
        "import psutil\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import AutoTokenizer\n",
        "import pickle\n",
        "\n",
        "# Configure logging for clear output in the notebook\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkConfig:\n",
        "    \"\"\"Configuration for the entire benchmarking run.\"\"\"\n",
        "    benchmark_iterations: int = 100\n",
        "    warmup_iterations: int = 20\n",
        "    batch_sizes: List[int] = None\n",
        "    accuracy_sample_size: int = 500\n",
        "    test_csv_path: Optional[str] = None\n",
        "    device_mode: str = \"auto\"  # \"auto\", \"cpu\", or \"gpu\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.batch_sizes is None:\n",
        "            self.batch_sizes = [1, 2, 4, 8]\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkResult:\n",
        "    \"\"\"A structured class to hold results from a single benchmark run.\"\"\"\n",
        "    model: str\n",
        "    batch_size: int\n",
        "    avg_latency_ms: float\n",
        "    p95_latency_ms: float\n",
        "    throughput_samples_per_sec: float\n",
        "    peak_memory_mb: float\n",
        "    model_size_mb: float\n",
        "    provider: str\n",
        "    accuracy: Optional[float] = None\n",
        "    f1_score: Optional[float] = None\n",
        "    weighted_accuracy: Optional[float] = None\n",
        "    weighted_f1_score: Optional[float] = None\n",
        "    # UPDATED: Added new fields for confidence and per-class metrics\n",
        "    avg_confidence_correct: Optional[float] = None\n",
        "    avg_confidence_incorrect: Optional[float] = None\n",
        "    per_class_metrics: Optional[Dict] = None\n",
        "\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        # UPDATED: Flatten the per_class_metrics for easier CSV export\n",
        "        flat_dict = asdict(self)\n",
        "        per_class = flat_dict.pop(\"per_class_metrics\", {})\n",
        "        if per_class:\n",
        "            for class_name, metrics in per_class.items():\n",
        "                for metric_name, value in metrics.items():\n",
        "                    flat_dict[f\"{class_name}_{metric_name}\"] = value\n",
        "        return flat_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Hardware & Model Loading Components\n",
        "class ExecutionProviderManager:\n",
        "    \"\"\"Manages ONNX execution providers based on platform and preferences.\"\"\"\n",
        "    @staticmethod\n",
        "    def get_execution_providers(mode: str = \"auto\") -> List: # Return type is now just List\n",
        "        # Force CPU-only for consistent benchmarking (uncomment to enable CoreML/GPU)\n",
        "        return ['CPUExecutionProvider']\n",
        "        \n",
        "        # Original hardware acceleration code (commented out):\n",
        "        # if platform.system() == \"Darwin\" and ort.get_device() == \"ARM64\":\n",
        "        #     # Optimized CoreML settings for maximum speed\n",
        "        #     return [\n",
        "        #         ('CoreMLExecutionProvider', {\n",
        "        #             'coreml_flags': 'COREML_FLAG_ENABLE_ON_SUBGRAPH',\n",
        "        #             'coreml_compute_units': 'ALL'  # Use all available compute units\n",
        "        #         }),\n",
        "        #         'CPUExecutionProvider'\n",
        "        #     ]\n",
        "        # \n",
        "        # # Fallback for other systems (Linux/Windows with GPU)\n",
        "        # available = ort.get_available_providers()\n",
        "        # preferences = [\"CoreMLExecutionProvider\", \"CPUExecutionProvider\"]\n",
        "        # chosen = [p for p in preferences if p in available]\n",
        "        # print(chosen)\n",
        "        # return chosen\n",
        "\n",
        "class ModelLoader:\n",
        "    \"\"\"Handles loading an ONNX model into an inference session.\"\"\"\n",
        "    @staticmethod\n",
        "    def load_onnx_session(onnx_path: Path, providers: List[str]) -> ort.InferenceSession:\n",
        "        opts = ort.SessionOptions()\n",
        "        \n",
        "        # Speed-optimized settings for financial trading latency\n",
        "        opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "        opts.enable_profiling = False\n",
        "        opts.enable_mem_pattern = False  # Disable for speed\n",
        "        opts.enable_cpu_mem_arena = False  # Disable for speed\n",
        "        \n",
        "        # Aggressive threading for maximum speed\n",
        "        opts.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
        "        opts.inter_op_num_threads = 1  \n",
        "        opts.intra_op_num_threads = 0  # Use all available cores\n",
        "        \n",
        "        return ort.InferenceSession(str(onnx_path), providers=providers, sess_options=opts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Fixed Data Handling Component\n",
        "class DataProcessor:\n",
        "    \"\"\"Handles data preprocessing and batch preparation - FIXED for consistency with training.\"\"\"\n",
        "    def __init__(self, tokenizer, max_length: int = 128):\n",
        "        self.tokenizer, self.max_length = tokenizer, max_length\n",
        "        self.example_inputs = [\"Stocks surged after the company reported record earnings.\"]\n",
        "        self.label_encoder = None\n",
        "\n",
        "    def prepare_batch_inputs(self, texts: List[str]) -> Dict[str, np.ndarray]:\n",
        "        encoding = self.tokenizer(\n",
        "            texts, return_tensors=\"np\", max_length=self.max_length,\n",
        "            padding=\"max_length\", truncation=True\n",
        "        )\n",
        "        return {k: v.astype(np.int64) for k, v in encoding.items()}\n",
        "\n",
        "    def load_label_encoder(self, model_dir: Path) -> LabelEncoder:\n",
        "        \"\"\"Load the label encoder used during training.\"\"\"\n",
        "        label_encoder_path = model_dir / \"label_encoder.pkl\"\n",
        "        if label_encoder_path.exists():\n",
        "            with open(label_encoder_path, 'rb') as f:\n",
        "                self.label_encoder = pickle.load(f)\n",
        "                return self.label_encoder\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def load_test_dataset(self, csv_path: Path, model_dir: Path) -> Tuple[List[str], List[int]]:\n",
        "        \"\"\"Load test dataset using EXACT same preprocessing as training.\"\"\"\n",
        "        self.load_label_encoder(model_dir)\n",
        "        \n",
        "        df = pd.read_csv(csv_path, header=None, names=[\"label\", \"sentence\"], encoding=\"latin-1\")\n",
        "        df[\"sentence\"] = df[\"sentence\"].str.strip('\"')\n",
        "        \n",
        "        if self.label_encoder is not None:\n",
        "            df[\"label_encoded\"] = self.label_encoder.transform(df[\"label\"])\n",
        "        else:\n",
        "            logger.warning(\"Using fallback label encoding - this might cause accuracy issues!\")\n",
        "            unique_labels = sorted(df[\"label\"].unique())\n",
        "            label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "            df[\"label_encoded\"] = df[\"label\"].map(label_to_id)\n",
        "        \n",
        "        _, test_df = train_test_split(\n",
        "            df, test_size=0.25, random_state=42, stratify=df[\"label\"]\n",
        "        )\n",
        "        \n",
        "        return test_df[\"sentence\"].tolist(), test_df[\"label_encoded\"].astype(int).tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Performance Measurement Components\n",
        "class PerformanceMonitor:\n",
        "    \"\"\"Monitors system performance during benchmarking.\"\"\"\n",
        "    @staticmethod\n",
        "    def measure_memory_usage() -> float:\n",
        "        return psutil.Process().memory_info().rss / (1024**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model_size_mb(onnx_path: Path) -> float:\n",
        "        return onnx_path.stat().st_size / (1024**2)\n",
        "\n",
        "class LatencyBenchmarker:\n",
        "    \"\"\"Handles the details of latency benchmarking.\"\"\"\n",
        "    def __init__(self, config: BenchmarkConfig):\n",
        "        self.config = config\n",
        "    \n",
        "    def warmup_session(self, session: ort.InferenceSession, inputs: Dict[str, np.ndarray]):\n",
        "        # Ultra-minimal warmup - just enough to avoid first-run penalty\n",
        "        for _ in range(3):  # Reduced to absolute minimum\n",
        "            session.run(None, inputs)\n",
        "    \n",
        "    def measure_latency(self, session: ort.InferenceSession, inputs: Dict[str, np.ndarray]) -> List[float]:\n",
        "        # Maximum speed measurement - no overhead\n",
        "        times = []\n",
        "        \n",
        "        for _ in range(self.config.benchmark_iterations):\n",
        "            start = time.perf_counter()\n",
        "            session.run(None, inputs)\n",
        "            times.append((time.perf_counter() - start) * 1000)\n",
        "        \n",
        "        return times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Enhanced Accuracy Evaluation Component\n",
        "\n",
        "# UPDATED: New comprehensive metrics calculation function\n",
        "def calculate_ordinal_metrics(y_true, y_pred, y_prob, labels_map):\n",
        "    \"\"\"\n",
        "    Calculates standard, ordinally-weighted, confidence, and per-class metrics.\n",
        "    'labels_map' should be an ordered list of class names, e.g., ['negative', 'neutral', 'positive'].\n",
        "    \"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_prob = np.array(y_prob)\n",
        "\n",
        "    # --- Standard & Ordinal Metrics ---\n",
        "    standard_accuracy = accuracy_score(y_true, y_pred)\n",
        "    standard_f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
        "    \n",
        "    penalized_y_pred = [true if abs(true - pred) <= 1 else pred for true, pred in zip(y_true, y_pred)]\n",
        "    weighted_accuracy = accuracy_score(y_true, penalized_y_pred)\n",
        "    weighted_f1 = f1_score(y_true, penalized_y_pred, average=\"weighted\", zero_division=0)\n",
        "\n",
        "    # --- Confidence Analysis ---\n",
        "    is_correct_mask = (y_true == y_pred)\n",
        "    confidences = y_prob.max(axis=1)\n",
        "    \n",
        "    avg_conf_correct = np.mean(confidences[is_correct_mask]) if np.any(is_correct_mask) else 0.0\n",
        "    avg_conf_incorrect = np.mean(confidences[~is_correct_mask]) if np.any(~is_correct_mask) else 0.0\n",
        "\n",
        "    # --- Per-Class Metrics ---\n",
        "    p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average=None, labels=range(len(labels_map)), zero_division=0)\n",
        "    per_class_metrics = {\n",
        "        labels_map[i]: {\"precision\": p[i], \"recall\": r[i], \"f1_score\": f[i], \"support\": s[i]}\n",
        "        for i in range(len(labels_map))\n",
        "    }\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": standard_accuracy,\n",
        "        \"f1_score\": standard_f1,\n",
        "        \"weighted_accuracy\": weighted_accuracy,\n",
        "        \"weighted_f1_score\": weighted_f1,\n",
        "        \"avg_confidence_correct\": avg_conf_correct,\n",
        "        \"avg_confidence_incorrect\": avg_conf_incorrect,\n",
        "        \"per_class_metrics\": per_class_metrics\n",
        "    }\n",
        "\n",
        "\n",
        "class AccuracyEvaluator:\n",
        "    \"\"\"Handles model accuracy and F1 score evaluation with enhanced debugging.\"\"\"\n",
        "    def __init__(self, session: ort.InferenceSession, data_processor: DataProcessor):\n",
        "        self.session, self.data_processor = session, data_processor\n",
        "    \n",
        "    def evaluate(self, texts: List[str], labels: List[int], batch_size: int, max_samples: int):\n",
        "        num_samples = min(len(texts), max_samples)\n",
        "        eval_texts, eval_labels = texts[:num_samples], labels[:num_samples]\n",
        "        \n",
        "        all_predictions = []\n",
        "        all_probabilities = []\n",
        "        \n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch_texts = eval_texts[i: i + batch_size]\n",
        "            inputs = self.data_processor.prepare_batch_inputs(batch_texts)\n",
        "            \n",
        "            model_inputs = {inp.name for inp in self.session.get_inputs()}\n",
        "            valid_inputs = {k: v for k, v in inputs.items() if k in model_inputs}\n",
        "            \n",
        "            outputs = self.session.run(None, valid_inputs)\n",
        "            logits = outputs[0]\n",
        "            \n",
        "            probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)\n",
        "            all_probabilities.extend(probabilities)\n",
        "            \n",
        "            predictions = np.argmax(logits, axis=1)\n",
        "            all_predictions.extend(predictions)\n",
        "        \n",
        "        # UPDATED: Call the new comprehensive metrics function\n",
        "        if self.data_processor.label_encoder:\n",
        "            ordered_labels = self.data_processor.label_encoder.classes_\n",
        "            metrics = calculate_ordinal_metrics(eval_labels, all_predictions, all_probabilities, ordered_labels)\n",
        "            return metrics\n",
        "        else: # Fallback\n",
        "            accuracy = accuracy_score(eval_labels, all_predictions)\n",
        "            f1 = f1_score(eval_labels, all_predictions, average=\"weighted\")\n",
        "            return {\"accuracy\": accuracy, \"f1_score\": f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ONNXModelBenchmarker:\n",
        "    \"\"\"Orchestrates all components to run a benchmark for a single model.\"\"\"\n",
        "    def __init__(self, config: BenchmarkConfig, tokenizer, model_dir: Path):\n",
        "        self.config = config\n",
        "        self.model_dir = model_dir\n",
        "        self.data_processor = DataProcessor(tokenizer, max_length=128)\n",
        "        self.latency_benchmarker = LatencyBenchmarker(config)\n",
        "    \n",
        "    # UPDATED: Added 'model_name' as a parameter\n",
        "    def _run_inference_and_get_metrics(self, session, model_name, onnx_path, batch_size):\n",
        "        \"\"\"Helper function to run the core benchmark and accuracy logic.\"\"\"\n",
        "        inputs = self.data_processor.prepare_batch_inputs(self.data_processor.example_inputs * batch_size)\n",
        "        model_inputs = {inp.name for inp in session.get_inputs()}\n",
        "        valid_inputs = {k: v for k, v in inputs.items() if k in model_inputs}\n",
        "\n",
        "        self.latency_benchmarker.warmup_session(session, valid_inputs)\n",
        "        times = self.latency_benchmarker.measure_latency(session, valid_inputs)\n",
        "        \n",
        "        avg_latency = statistics.mean(times)\n",
        "        p95_latency = np.percentile(times, 95)\n",
        "        std_latency = statistics.stdev(times) if len(times) > 1 else 0.0\n",
        "        \n",
        "        # Simplified output for speed\n",
        "        print(f\"   -> {avg_latency:.1f}ms avg ({std_latency:.1f}ms std)\")\n",
        "        \n",
        "        metrics = {}\n",
        "        if self.config.test_csv_path:\n",
        "            evaluator = AccuracyEvaluator(session, self.data_processor)\n",
        "            texts, labels = self.data_processor.load_test_dataset(\n",
        "                Path(self.config.test_csv_path), self.model_dir\n",
        "            )\n",
        "            metrics = evaluator.evaluate(texts, labels, batch_size, self.config.accuracy_sample_size)\n",
        "\n",
        "        return BenchmarkResult(\n",
        "            model=model_name, batch_size=batch_size,\n",
        "            avg_latency_ms=avg_latency, p95_latency_ms=p95_latency,\n",
        "            throughput_samples_per_sec=(1000 * batch_size) / avg_latency if avg_latency > 0 else 0,\n",
        "            peak_memory_mb=PerformanceMonitor.measure_memory_usage(),\n",
        "            model_size_mb=PerformanceMonitor.get_model_size_mb(onnx_path),\n",
        "            provider=session.get_providers()[0],\n",
        "            accuracy=metrics.get(\"accuracy\"),\n",
        "            f1_score=metrics.get(\"f1_score\"),\n",
        "            weighted_accuracy=metrics.get(\"weighted_accuracy\"),\n",
        "            weighted_f1_score=metrics.get(\"weighted_f1_score\"),\n",
        "            avg_confidence_correct=metrics.get(\"avg_confidence_correct\"),\n",
        "            avg_confidence_incorrect=metrics.get(\"avg_confidence_incorrect\"),\n",
        "            per_class_metrics=metrics.get(\"per_class_metrics\")\n",
        "        )\n",
        "\n",
        "    def benchmark_model(self, model_name: str, onnx_path: Path, batch_size: int) -> Optional[BenchmarkResult]:\n",
        "        print(f\"   -> Running benchmark for batch size: {batch_size}\")\n",
        "        \n",
        "        try:\n",
        "            print(\"   -> Attempting with optimal providers (CoreML)...\")\n",
        "            providers = ExecutionProviderManager.get_execution_providers(self.config.device_mode)\n",
        "            session = ModelLoader.load_onnx_session(onnx_path, providers)\n",
        "            # UPDATED: Pass 'model_name' to the helper function\n",
        "            return self._run_inference_and_get_metrics(session, model_name, onnx_path, batch_size)\n",
        "        \n",
        "        except ort.capi.onnxruntime_pybind11_state.Fail as e:\n",
        "            print(f\"   -> ⚠️  CoreML execution failed. Retrying with CPU-only provider.\")\n",
        "            try:\n",
        "                providers = ['CPUExecutionProvider']\n",
        "                session = ModelLoader.load_onnx_session(onnx_path, providers)\n",
        "                # UPDATED: Pass 'model_name' to the helper function\n",
        "                return self._run_inference_and_get_metrics(session, model_name, onnx_path, batch_size)\n",
        "            except Exception as cpu_e:\n",
        "                logger.error(f\"❌ Benchmark failed on CPU fallback for {model_name}: {cpu_e}\", exc_info=True)\n",
        "                return None\n",
        "        \n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ An unexpected error occurred for {model_name}: {e}\", exc_info=True)\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Results Management\n",
        "class ResultsManager:\n",
        "    \"\"\"Manages benchmark results and reporting.\"\"\"\n",
        "    @staticmethod\n",
        "    def save_results(results: List[BenchmarkResult], output_dir: Path = Path(\"results\")):\n",
        "        if not results: return\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "        df = pd.DataFrame([r.to_dict() for r in results])\n",
        "        df.to_csv(output_dir / \"benchmark_results_cpu.csv\", index=False)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def print_summary(results: List[BenchmarkResult]):\n",
        "        if not results: return\n",
        "        df = pd.DataFrame([r.to_dict() for r in results if r])\n",
        "        \n",
        "        # UPDATED: Main summary table with new high-level metrics\n",
        "        main_summary_cols = [\n",
        "            \"model\", \"batch_size\", \"provider\", \"avg_latency_ms\", \"accuracy\", \"f1_score\", \n",
        "            \"weighted_f1_score\", \"avg_confidence_correct\", \"avg_confidence_incorrect\"\n",
        "        ]\n",
        "        # Filter out columns that may not exist if a run failed partially\n",
        "        main_summary_cols = [col for col in main_summary_cols if col in df.columns]\n",
        "\n",
        "        print(\"\\n\" + \"=\"*120 + \"\\n📊 BENCHMARK SUMMARY\\n\" + \"=\"*120)\n",
        "        print(df[main_summary_cols].to_string(index=False, float_format=\"%.3f\"))\n",
        "        print(\"=\"*120)\n",
        "\n",
        "        # UPDATED: Separate, detailed report for per-class metrics\n",
        "        print(\"\\n\" + \"=\"*120 + \"\\n🔬 PER-CLASS METRICS (PRECISION / RECALL / F1-SCORE)\\n\" + \"=\"*120)\n",
        "        for _, row in df.iterrows():\n",
        "            print(f\"\\nModel: {row['model']} | Batch Size: {row['batch_size']}\")\n",
        "            \n",
        "            # Reconstruct per-class metrics from flattened CSV columns\n",
        "            class_names = sorted(list(set([k.split('_')[0] for k in df.columns if '_precision' in k])))\n",
        "            \n",
        "            header = f\"{'CLASS':<15}\" + \"\".join([f\"{metric.upper():>12}\" for metric in [\"PRECISION\", \"RECALL\", \"F1-SCORE\", \"SUPPORT\"]])\n",
        "            print(header)\n",
        "            print(\"-\" * len(header))\n",
        "\n",
        "            for name in class_names:\n",
        "                p = row.get(f\"{name}_precision\", 0.0)\n",
        "                r = row.get(f\"{name}_recall\", 0.0)\n",
        "                f1 = row.get(f\"{name}_f1_score\", 0.0)\n",
        "                s = int(row.get(f\"{name}_support\", 0))\n",
        "                print(f\"{name:<15}{p:>12.3f}{r:>12.3f}{f1:>12.3f}{s:>12d}\")\n",
        "        print(\"=\"*120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Model Discovery Functions\n",
        "def discover_models(models_dir: str, target_models: List[str] = None) -> List[Tuple[str, Path, Path]]:\n",
        "    \"\"\"Discover ONNX models with flexible filtering.\"\"\"\n",
        "    valid_models = []\n",
        "    \n",
        "    for model_dir in Path(models_dir).iterdir():\n",
        "        if not model_dir.is_dir() or not (model_dir / \"onnx\").exists(): \n",
        "            continue\n",
        "        \n",
        "        # If target_models is specified, only process those models\n",
        "        if target_models and model_dir.name not in target_models:\n",
        "            continue\n",
        "            \n",
        "        # Add standard models if they exist\n",
        "        standard_path = model_dir / \"onnx\" / \"model.onnx\"\n",
        "        if standard_path.exists():\n",
        "            valid_models.append((f\"{model_dir.name}-standard\", standard_path, model_dir))\n",
        "            print(f\"✅ Found: {model_dir.name}\")\n",
        "        \n",
        "        # quant_path = model_dir / \"onnx\" / \"model-quantised.onnx\"\n",
        "        # if quant_path.exists():\n",
        "        #     valid_models.append((f\"{model_dir.name}-quant\", quant_path, model_dir))\n",
        "\n",
        "    return valid_models\n",
        "\n",
        "def run_full_benchmark(models_dir: str, config: BenchmarkConfig, target_models: List[str] = None):\n",
        "    \"\"\"Run the full benchmark suite on discovered models with optional filtering.\"\"\"\n",
        "    all_results = []\n",
        "    \n",
        "    valid_models = discover_models(models_dir, target_models)\n",
        "    if not valid_models: \n",
        "        print(\"❌ No valid models found!\")\n",
        "        return\n",
        "\n",
        "    for model_name, onnx_path, model_dir in valid_models:\n",
        "        print(f\"\\n--- ⏳ Now processing: {model_name} ---\")\n",
        "        \n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load tokenizer for {model_name}: {e}\")\n",
        "            continue\n",
        "        \n",
        "        for batch_size in config.batch_sizes:\n",
        "            # Create a new benchmarker instance for EACH run to prevent state leakage\n",
        "            benchmarker = ONNXModelBenchmarker(config, tokenizer, model_dir)\n",
        "            \n",
        "            result = benchmarker.benchmark_model(model_name, onnx_path, batch_size)\n",
        "            if result: all_results.append(result)\n",
        "    \n",
        "    if all_results:\n",
        "        ResultsManager.print_summary(all_results)\n",
        "        ResultsManager.save_results(all_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 BENCHMARK CONFIGURATION\n",
            "==================================================\n",
            "Models to test: ['tinybert-financial-classifier', 'tinybert-financial-classifier-fine-tuned', 'tinybert-financial-classifier-pruned']\n",
            "Fast Mode: True\n",
            "Include Accuracy: True\n",
            "Batch Sizes: [1, 8]\n",
            "Iterations: 15\n",
            "Warmup: 5\n",
            "Accuracy Sample Size: 300\n",
            "==================================================\n",
            "🚀 Starting benchmark...\n",
            "✅ Found: tinybert-financial-classifier-fine-tuned\n",
            "✅ Found: tinybert-financial-classifier\n",
            "✅ Found: tinybert-financial-classifier-pruned\n",
            "\n",
            "--- ⏳ Now processing: tinybert-financial-classifier-fine-tuned-standard ---\n",
            "   -> Running benchmark for batch size: 1\n",
            "   -> Attempting with optimal providers (CoreML)...\n",
            "   -> 12.5ms avg (2.8ms std)\n",
            "   -> 12.5ms avg (2.8ms std)\n",
            "   -> Running benchmark for batch size: 8\n",
            "   -> Attempting with optimal providers (CoreML)...\n",
            "   -> Running benchmark for batch size: 8\n",
            "   -> Attempting with optimal providers (CoreML)...\n",
            "   -> 111.7ms avg (14.0ms std)\n",
            "   -> 111.7ms avg (14.0ms std)\n",
            "\n",
            "--- ⏳ Now processing: tinybert-financial-classifier-standard ---\n",
            "   -> Running benchmark for batch size: 1\n",
            "   -> Attempting with optimal providers (CoreML)...\n",
            "\n",
            "--- ⏳ Now processing: tinybert-financial-classifier-standard ---\n",
            "   -> Running benchmark for batch size: 1\n",
            "   -> Attempting with optimal providers (CoreML)...\n",
            "   -> 12.2ms avg (4.7ms std)\n",
            "   -> 12.2ms avg (4.7ms std)\n",
            "   -> Running benchmark for batch size: 8\n",
            "   -> Attempting with optimal providers (CoreML)...\n",
            "   -> Running benchmark for batch size: 8\n",
            "   -> Attempting with optimal providers (CoreML)...\n",
            "   -> 128.1ms avg (40.1ms std)\n",
            "   -> 128.1ms avg (40.1ms std)\n",
            "\n",
            "--- ⏳ Now processing: tinybert-financial-classifier-pruned-standard ---\n",
            "   -> Running benchmark for batch size: 1\n",
            "   -> Attempting with optimal providers (CoreML)...\n",
            "\n",
            "--- ⏳ Now processing: tinybert-financial-classifier-pruned-standard ---\n",
            "   -> Running benchmark for batch size: 1\n",
            "   -> Attempting with optimal providers (CoreML)...\n",
            "   -> 10.7ms avg (1.8ms std)\n",
            "   -> 10.7ms avg (1.8ms std)\n",
            "   -> Running benchmark for batch size: 8\n",
            "   -> Attempting with optimal providers (CoreML)...\n",
            "   -> Running benchmark for batch size: 8\n",
            "   -> Attempting with optimal providers (CoreML)...\n",
            "   -> 104.5ms avg (7.0ms std)\n",
            "   -> 104.5ms avg (7.0ms std)\n",
            "\n",
            "========================================================================================================================\n",
            "📊 BENCHMARK SUMMARY\n",
            "========================================================================================================================\n",
            "                                            model  batch_size             provider  avg_latency_ms  accuracy  f1_score  weighted_f1_score  avg_confidence_correct  avg_confidence_incorrect\n",
            "tinybert-financial-classifier-fine-tuned-standard           1 CPUExecutionProvider          12.467     0.867     0.868              0.980                   0.986                     0.943\n",
            "tinybert-financial-classifier-fine-tuned-standard           8 CPUExecutionProvider         111.685     0.867     0.868              0.980                   0.986                     0.943\n",
            "           tinybert-financial-classifier-standard           1 CPUExecutionProvider          12.204     0.777     0.778              0.964                   0.743                     0.649\n",
            "           tinybert-financial-classifier-standard           8 CPUExecutionProvider         128.138     0.777     0.778              0.964                   0.743                     0.649\n",
            "    tinybert-financial-classifier-pruned-standard           1 CPUExecutionProvider          10.674     0.843     0.844              0.983                   0.931                     0.824\n",
            "    tinybert-financial-classifier-pruned-standard           8 CPUExecutionProvider         104.494     0.843     0.844              0.983                   0.931                     0.824\n",
            "========================================================================================================================\n",
            "\n",
            "========================================================================================================================\n",
            "🔬 PER-CLASS METRICS (PRECISION / RECALL / F1-SCORE)\n",
            "========================================================================================================================\n",
            "\n",
            "Model: tinybert-financial-classifier-fine-tuned-standard | Batch Size: 1\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.821       0.800       0.810          40\n",
            "neutral               0.926       0.873       0.899         173\n",
            "positive              0.786       0.885       0.832          87\n",
            "\n",
            "Model: tinybert-financial-classifier-fine-tuned-standard | Batch Size: 8\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.821       0.800       0.810          40\n",
            "neutral               0.926       0.873       0.899         173\n",
            "positive              0.786       0.885       0.832          87\n",
            "\n",
            "Model: tinybert-financial-classifier-standard | Batch Size: 1\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.627       0.800       0.703          40\n",
            "neutral               0.855       0.815       0.834         173\n",
            "positive              0.714       0.690       0.702          87\n",
            "\n",
            "Model: tinybert-financial-classifier-standard | Batch Size: 8\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.627       0.800       0.703          40\n",
            "neutral               0.855       0.815       0.834         173\n",
            "positive              0.714       0.690       0.702          87\n",
            "\n",
            "Model: tinybert-financial-classifier-pruned-standard | Batch Size: 1\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.767       0.825       0.795          40\n",
            "neutral               0.883       0.873       0.878         173\n",
            "positive              0.802       0.793       0.798          87\n",
            "\n",
            "Model: tinybert-financial-classifier-pruned-standard | Batch Size: 8\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.767       0.825       0.795          40\n",
            "neutral               0.883       0.873       0.878         173\n",
            "positive              0.802       0.793       0.798          87\n",
            "========================================================================================================================\n",
            "✅ Benchmark complete! Check the results above and the CSV file in the results/ folder.\n",
            "\n",
            "========================================================================================================================\n",
            "📊 BENCHMARK SUMMARY\n",
            "========================================================================================================================\n",
            "                                            model  batch_size             provider  avg_latency_ms  accuracy  f1_score  weighted_f1_score  avg_confidence_correct  avg_confidence_incorrect\n",
            "tinybert-financial-classifier-fine-tuned-standard           1 CPUExecutionProvider          12.467     0.867     0.868              0.980                   0.986                     0.943\n",
            "tinybert-financial-classifier-fine-tuned-standard           8 CPUExecutionProvider         111.685     0.867     0.868              0.980                   0.986                     0.943\n",
            "           tinybert-financial-classifier-standard           1 CPUExecutionProvider          12.204     0.777     0.778              0.964                   0.743                     0.649\n",
            "           tinybert-financial-classifier-standard           8 CPUExecutionProvider         128.138     0.777     0.778              0.964                   0.743                     0.649\n",
            "    tinybert-financial-classifier-pruned-standard           1 CPUExecutionProvider          10.674     0.843     0.844              0.983                   0.931                     0.824\n",
            "    tinybert-financial-classifier-pruned-standard           8 CPUExecutionProvider         104.494     0.843     0.844              0.983                   0.931                     0.824\n",
            "========================================================================================================================\n",
            "\n",
            "========================================================================================================================\n",
            "🔬 PER-CLASS METRICS (PRECISION / RECALL / F1-SCORE)\n",
            "========================================================================================================================\n",
            "\n",
            "Model: tinybert-financial-classifier-fine-tuned-standard | Batch Size: 1\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.821       0.800       0.810          40\n",
            "neutral               0.926       0.873       0.899         173\n",
            "positive              0.786       0.885       0.832          87\n",
            "\n",
            "Model: tinybert-financial-classifier-fine-tuned-standard | Batch Size: 8\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.821       0.800       0.810          40\n",
            "neutral               0.926       0.873       0.899         173\n",
            "positive              0.786       0.885       0.832          87\n",
            "\n",
            "Model: tinybert-financial-classifier-standard | Batch Size: 1\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.627       0.800       0.703          40\n",
            "neutral               0.855       0.815       0.834         173\n",
            "positive              0.714       0.690       0.702          87\n",
            "\n",
            "Model: tinybert-financial-classifier-standard | Batch Size: 8\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.627       0.800       0.703          40\n",
            "neutral               0.855       0.815       0.834         173\n",
            "positive              0.714       0.690       0.702          87\n",
            "\n",
            "Model: tinybert-financial-classifier-pruned-standard | Batch Size: 1\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.767       0.825       0.795          40\n",
            "neutral               0.883       0.873       0.878         173\n",
            "positive              0.802       0.793       0.798          87\n",
            "\n",
            "Model: tinybert-financial-classifier-pruned-standard | Batch Size: 8\n",
            "CLASS             PRECISION      RECALL    F1-SCORE     SUPPORT\n",
            "---------------------------------------------------------------\n",
            "negative              0.767       0.825       0.795          40\n",
            "neutral               0.883       0.873       0.878         173\n",
            "positive              0.802       0.793       0.798          87\n",
            "========================================================================================================================\n",
            "✅ Benchmark complete! Check the results above and the CSV file in the results/ folder.\n"
          ]
        }
      ],
      "source": [
        "# Cell 10: Configuration Processing (uses settings from Cell 2)\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    \"\"\"Automatically configures benchmark based on settings from Cell 2.\"\"\"\n",
        "    \n",
        "    def get_models_and_config(self):\n",
        "        # Use the TARGET_MODELS directly - no complex mapping needed!\n",
        "        target_models = TARGET_MODELS\n",
        "        \n",
        "        # Performance settings with custom overrides\n",
        "        iterations = CUSTOM_ITERATIONS if CUSTOM_ITERATIONS is not None else (15 if FAST_MODE else 30)\n",
        "        warmup = CUSTOM_WARMUP if CUSTOM_WARMUP is not None else (5 if FAST_MODE else 10)\n",
        "        batch_sizes = CUSTOM_BATCH_SIZES if CUSTOM_BATCH_SIZES is not None else ([1, 8] if FAST_MODE else [1, 4, 8, 16])\n",
        "        sample_size = CUSTOM_SAMPLE_SIZE if CUSTOM_SAMPLE_SIZE is not None else (300 if FAST_MODE else 500)\n",
        "        \n",
        "        # Build configuration\n",
        "        config = BenchmarkConfig(\n",
        "            batch_sizes=batch_sizes,\n",
        "            test_csv_path=ACCURACY_DATASET_PATH if INCLUDE_ACCURACY else None,\n",
        "            benchmark_iterations=iterations,\n",
        "            warmup_iterations=warmup,\n",
        "            accuracy_sample_size=sample_size\n",
        "        )\n",
        "        \n",
        "        return target_models, config\n",
        "\n",
        "# Create configuration\n",
        "experiment = ExperimentConfig()\n",
        "target_models, benchmark_config = experiment.get_models_and_config()\n",
        "\n",
        "# Display what will be run\n",
        "print(\"🎯 BENCHMARK CONFIGURATION\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Models to test: {target_models or 'ALL AVAILABLE MODELS'}\")\n",
        "print(f\"Fast Mode: {FAST_MODE}\")\n",
        "print(f\"Include Accuracy: {INCLUDE_ACCURACY}\")\n",
        "print(f\"Batch Sizes: {benchmark_config.batch_sizes}\")\n",
        "print(f\"Iterations: {benchmark_config.benchmark_iterations}\")\n",
        "print(f\"Warmup: {benchmark_config.warmup_iterations}\")\n",
        "if INCLUDE_ACCURACY:\n",
        "    print(f\"Accuracy Sample Size: {benchmark_config.accuracy_sample_size}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Execute the benchmark\n",
        "print(\"🚀 Starting benchmark...\")\n",
        "run_full_benchmark(MODELS_DIRECTORY, benchmark_config, target_models)\n",
        "print(\"✅ Benchmark complete! Check the results above and the CSV file in the results/ folder.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Checking for quantized model from fine-tuning notebook...\n",
            "⚠️  To test the quantized model, run this cell in the fine-tuning notebook:\n",
            "   test_pytorch_model_speed(quantized_model, tokenizer, 'quantized-tinybert')\n",
            "\n",
            "🎯 Expected result with quantization: ~2-4ms latency\n",
            "🎯 Current ONNX models: ~8-10ms latency\n",
            "\n",
            "💡 The 3x speed improvement comes from:\n",
            "   ✅ INT8 quantization (vs FP32)\n",
            "   ✅ Optimized PyTorch operations\n",
            "   ✅ No ONNX conversion overhead\n"
          ]
        }
      ],
      "source": [
        "# 🚀 QUICK TEST: Direct PyTorch quantized model latency\n",
        "# This tests the quantized model from the fine-tuning notebook WITHOUT ONNX conversion\n",
        "\n",
        "def test_pytorch_model_speed(model, tokenizer, model_name=\"test-model\", num_iterations=50):\n",
        "    \"\"\"Quick latency test for PyTorch models\"\"\"\n",
        "    import time\n",
        "    import torch\n",
        "    import statistics\n",
        "    \n",
        "    print(f\"\\n⚡ Testing {model_name} speed directly in PyTorch...\")\n",
        "    \n",
        "    # Prepare test input\n",
        "    test_text = \"Stocks surged after the company reported record earnings.\"\n",
        "    inputs = tokenizer(test_text, return_tensors=\"pt\", padding=\"max_length\", \n",
        "                      truncation=True, max_length=128)\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    # Warmup\n",
        "    with torch.no_grad():\n",
        "        for _ in range(5):\n",
        "            _ = model(**inputs)\n",
        "    \n",
        "    # Measure latency\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_iterations):\n",
        "            start = time.perf_counter()\n",
        "            _ = model(**inputs)\n",
        "            end = time.perf_counter()\n",
        "            times.append((end - start) * 1000)\n",
        "    \n",
        "    avg_latency = statistics.mean(times)\n",
        "    std_latency = statistics.stdev(times) if len(times) > 1 else 0.0\n",
        "    p95_latency = sorted(times)[int(0.95 * len(times))]\n",
        "    \n",
        "    print(f\"   📊 Average latency: {avg_latency:.2f}ms (±{std_latency:.2f}ms)\")\n",
        "    print(f\"   📊 P95 latency: {p95_latency:.2f}ms\")\n",
        "    print(f\"   📊 Throughput: {1000/avg_latency:.1f} samples/sec\")\n",
        "    \n",
        "    return avg_latency\n",
        "\n",
        "# Test if we have access to the quantized model from the fine-tuning notebook\n",
        "print(\"🔍 Checking for quantized model from fine-tuning notebook...\")\n",
        "\n",
        "# Try to import the quantized model - this requires the fine-tuning notebook to be run first\n",
        "try:\n",
        "    # We'll need to run this in the fine-tuning notebook context\n",
        "    print(\"⚠️  To test the quantized model, run this cell in the fine-tuning notebook:\")\n",
        "    print(\"   test_pytorch_model_speed(quantized_model, tokenizer, 'quantized-tinybert')\")\n",
        "    print()\n",
        "    print(\"🎯 Expected result with quantization: ~2-4ms latency\")\n",
        "    print(\"🎯 Current ONNX models: ~8-10ms latency\")\n",
        "    print()\n",
        "    print(\"💡 The 3x speed improvement comes from:\")\n",
        "    print(\"   ✅ INT8 quantization (vs FP32)\")\n",
        "    print(\"   ✅ Optimized PyTorch operations\")\n",
        "    print(\"   ✅ No ONNX conversion overhead\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   Model not available in this notebook context\")\n",
        "    print(\"   Run the quantization in the fine-tuning notebook first!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPEaWosSVJk00KG/SJV/J7B",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
