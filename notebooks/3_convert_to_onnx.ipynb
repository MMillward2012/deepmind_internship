{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0_setup.ipynb            3_convert_to_onnx.ipynb  6_fine_tune.ipynb\n",
            "1_data_processing.ipynb  4_benchmarks.ipynb       6_fine_tune_backup.ipynb\n",
            "2_train_models.ipynb     5_explainability.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/matthew/Documents/deepmind_internship\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Discovering and validating models...\n",
            "   - Checking tinybert-financial-classifier-fine-tuned...\n",
            "   - ‚úÖ tinybert-financial-classifier-fine-tuned - Valid and loadable\n",
            "   - Checking all-MiniLM-L6-v2-financial-sentiment...\n",
            "   - ‚úÖ all-MiniLM-L6-v2-financial-sentiment - Valid and loadable\n",
            "   - Checking distilbert-financial-sentiment...\n",
            "   - ‚úÖ tinybert-financial-classifier-fine-tuned - Valid and loadable\n",
            "   - Checking all-MiniLM-L6-v2-financial-sentiment...\n",
            "   - ‚úÖ all-MiniLM-L6-v2-financial-sentiment - Valid and loadable\n",
            "   - Checking distilbert-financial-sentiment...\n",
            "   - ‚úÖ distilbert-financial-sentiment - Valid and loadable\n",
            "   - Checking finbert-tone-financial-sentiment...\n",
            "   - ‚úÖ finbert-tone-financial-sentiment - Valid and loadable\n",
            "   - ‚úÖ distilbert-financial-sentiment - Valid and loadable\n",
            "   - Checking finbert-tone-financial-sentiment...\n",
            "   - ‚úÖ finbert-tone-financial-sentiment - Valid and loadable\n",
            "   - Checking SmolLM2-360M-Instruct-financial-sentiment...\n",
            "   - Checking SmolLM2-360M-Instruct-financial-sentiment...\n",
            "   - ‚úÖ SmolLM2-360M-Instruct-financial-sentiment - Valid and loadable\n",
            "   - Checking tinybert-financial-classifier...\n",
            "   - ‚úÖ tinybert-financial-classifier - Valid and loadable\n",
            "   - Checking tinybert-financial-classifier-pruned...\n",
            "   - ‚úÖ SmolLM2-360M-Instruct-financial-sentiment - Valid and loadable\n",
            "   - Checking tinybert-financial-classifier...\n",
            "   - ‚úÖ tinybert-financial-classifier - Valid and loadable\n",
            "   - Checking tinybert-financial-classifier-pruned...\n",
            "   - ‚úÖ tinybert-financial-classifier-pruned - Valid and loadable\n",
            "   - Checking mobilebert-uncased-financial-sentiment...\n",
            "   - ‚úÖ tinybert-financial-classifier-pruned - Valid and loadable\n",
            "   - Checking mobilebert-uncased-financial-sentiment...\n",
            "   - ‚úÖ mobilebert-uncased-financial-sentiment - Valid and loadable\n",
            "\n",
            "‚úÖ Found 8 valid and loadable models.\n",
            "Loading data from data/FinancialPhraseBank/all-data.csv...\n",
            "‚úÖ Created a calibration dataset with 100 samples.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: tinybert-financial-classifier-fine-tuned\n",
            "   - üì¶ ONNX model not found. Starting export...\n",
            "   - Wrapping model for ONNX export...\n",
            "   - üöÄ Exporting to ONNX (Opset 17)...\n",
            "   - ‚úÖ mobilebert-uncased-financial-sentiment - Valid and loadable\n",
            "\n",
            "‚úÖ Found 8 valid and loadable models.\n",
            "Loading data from data/FinancialPhraseBank/all-data.csv...\n",
            "‚úÖ Created a calibration dataset with 100 samples.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: tinybert-financial-classifier-fine-tuned\n",
            "   - üì¶ ONNX model not found. Starting export...\n",
            "   - Wrapping model for ONNX export...\n",
            "   - üöÄ Exporting to ONNX (Opset 17)...\n",
            "   - ‚úÖ Model successfully exported to model.onnx\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: all-MiniLM-L6-v2-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: distilbert-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: finbert-tone-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: SmolLM2-360M-Instruct-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: tinybert-financial-classifier\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: tinybert-financial-classifier-pruned\n",
            "   - üì¶ ONNX model not found. Starting export...\n",
            "   - Wrapping model for ONNX export...\n",
            "   - üöÄ Exporting to ONNX (Opset 17)...\n",
            "   - ‚úÖ Model successfully exported to model.onnx\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: all-MiniLM-L6-v2-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: distilbert-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: finbert-tone-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: SmolLM2-360M-Instruct-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: tinybert-financial-classifier\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: tinybert-financial-classifier-pruned\n",
            "   - üì¶ ONNX model not found. Starting export...\n",
            "   - Wrapping model for ONNX export...\n",
            "   - üöÄ Exporting to ONNX (Opset 17)...\n",
            "   - ‚úÖ Model successfully exported to model.onnx\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: mobilebert-uncased-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "----------------------------------------------------------------------\n",
            "üéâ All models have been processed.\n",
            "   - ‚úÖ Model successfully exported to model.onnx\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: mobilebert-uncased-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "----------------------------------------------------------------------\n",
            "üéâ All models have been processed.\n"
          ]
        }
      ],
      "source": [
        "# ONNX Conversion Code - Run This First!\n",
        "\n",
        "# Cell 1: Imports\n",
        "import gc\n",
        "import json\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import List\n",
        "\n",
        "# Data & ML\n",
        "import numpy as np\n",
        "import torch\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Suppress ONNX Runtime logging\n",
        "import logging\n",
        "logging.getLogger(\"onnxruntime\").setLevel(logging.ERROR)\n",
        "\n",
        "# Cell 2: Configuration & Model Discovery\n",
        "# Model & ONNX Configuration\n",
        "BASE_DIR = Path(\"models\")\n",
        "ONNX_OPSET_VERSION = 17\n",
        "\n",
        "# Data & Split Configuration\n",
        "DATA_FILE_PATH = Path(\"data/FinancialPhraseBank/all-data.csv\")\n",
        "RANDOM_SEED = 42\n",
        "TEST_SIZE = 0.25 # 25% for the test set\n",
        "\n",
        "def is_valid_model_dir(d: Path) -> bool:\n",
        "    \"\"\"Checks if a directory contains a valid Hugging Face model.\"\"\"\n",
        "    config_file = d / \"config.json\"\n",
        "    model_file_exists = (d / \"pytorch_model.bin\").exists() or (d / \"model.safetensors\").exists()\n",
        "    \n",
        "    if not config_file.exists() or not model_file_exists:\n",
        "        return False\n",
        "    \n",
        "    # Check if the config follows Hugging Face format\n",
        "    try:\n",
        "        with open(config_file, 'r') as f:\n",
        "            config = json.load(f)\n",
        "        \n",
        "        # Valid HF configs should have either 'model_type' with a known architecture\n",
        "        # or 'architectures' field with valid architecture names\n",
        "        has_valid_model_type = config.get('model_type') in [\n",
        "            'bert', 'distilbert', 'roberta', 'albert', 'electra', 'deberta', \n",
        "            'deberta-v2', 'xlnet', 'xlm-roberta', 'camembert', 'flaubert'\n",
        "        ]\n",
        "        \n",
        "        has_valid_architectures = 'architectures' in config and any(\n",
        "            arch.endswith(('ForSequenceClassification', 'Model')) \n",
        "            for arch in config['architectures']\n",
        "        )\n",
        "        \n",
        "        return has_valid_model_type or has_valid_architectures\n",
        "        \n",
        "    except (json.JSONDecodeError, Exception):\n",
        "        return False\n",
        "\n",
        "def can_load_with_transformers(model_dir: Path) -> bool:\n",
        "    \"\"\"Test if a model can actually be loaded by transformers library.\"\"\"\n",
        "    try:\n",
        "        # Try to load tokenizer and model without actually loading the weights\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "        # Just check if we can initialize the model class without loading weights\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_dir, \n",
        "            torch_dtype=torch.float32,\n",
        "            device_map=None\n",
        "        )\n",
        "        del tokenizer, model\n",
        "        gc.collect()\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"   - ‚ö†Ô∏è  Cannot load with transformers: {e}\")\n",
        "        return False\n",
        "\n",
        "def prepare_calibration_data(data_path, test_size, random_seed, num_samples=100):\n",
        "    \"\"\"Loads, splits, and samples the data to create a calibration set.\"\"\"\n",
        "    print(f\"Loading data from {data_path}...\")\n",
        "    df = pd.read_csv(\n",
        "        data_path,\n",
        "        header=None,\n",
        "        names=['sentiment', 'text'],\n",
        "        encoding='latin-1')\n",
        "\n",
        "    # Split data to get the test set\n",
        "    _, test_df = train_test_split(\n",
        "        df, test_size=test_size, random_state=random_seed, stratify=df['sentiment'])\n",
        "\n",
        "    # Sample the calibration set from the test data\n",
        "    calibration_df = test_df.sample(n=num_samples, random_state=random_seed)\n",
        "    print(f\"‚úÖ Created a calibration dataset with {len(calibration_df)} samples.\")\n",
        "    return calibration_df\n",
        "\n",
        "# Find all valid model directories\n",
        "print(\"üîç Discovering and validating models...\")\n",
        "all_model_dirs = [d for d in BASE_DIR.iterdir() if d.is_dir()]\n",
        "valid_model_dirs = []\n",
        "\n",
        "for model_dir in all_model_dirs:\n",
        "    if is_valid_model_dir(model_dir):\n",
        "        print(f\"   - Checking {model_dir.name}...\")\n",
        "        if can_load_with_transformers(model_dir):\n",
        "            valid_model_dirs.append(model_dir)\n",
        "            print(f\"   - ‚úÖ {model_dir.name} - Valid and loadable\")\n",
        "        else:\n",
        "            print(f\"   - ‚ùå {model_dir.name} - Valid format but not loadable with transformers\")\n",
        "    else:\n",
        "        print(f\"   - ‚ùå {model_dir.name} - Invalid Hugging Face model format\")\n",
        "\n",
        "model_dirs = valid_model_dirs\n",
        "print(f\"\\n‚úÖ Found {len(model_dirs)} valid and loadable models.\")\n",
        "\n",
        "# Call the function to prepare data\n",
        "calibration_df = prepare_calibration_data(DATA_FILE_PATH, TEST_SIZE, RANDOM_SEED)\n",
        "\n",
        "\n",
        "# Cell 3: Automated Node Finder\n",
        "def find_final_nodes_to_exclude(onnx_model_path: Path) -> List[str]:\n",
        "    \"\"\"\n",
        "    Analyzes an ONNX model to find the names of the final MatMul or Add nodes\n",
        "    right before the output, tracing backwards past common post-processing nodes.\n",
        "    \"\"\"\n",
        "    nodes_to_exclude = []\n",
        "    try:\n",
        "        model = onnx.load(str(onnx_model_path))\n",
        "        \n",
        "        # Create maps of all node inputs/outputs\n",
        "        output_to_node_map = {out: node for node in model.graph.node for out in node.output}\n",
        "\n",
        "        # Find the final output of the graph\n",
        "        graph_outputs = [output.name for output in model.graph.output]\n",
        "        \n",
        "        for graph_output in graph_outputs:\n",
        "            # Start tracing backwards from the graph's output\n",
        "            current_node = output_to_node_map.get(graph_output)\n",
        "            \n",
        "            # Trace backwards past common non-computational nodes\n",
        "            while current_node and current_node.op_type in ['Softmax', 'LogSoftmax', 'Identity']:\n",
        "                parent_node_output = current_node.input[0]\n",
        "                current_node = output_to_node_map.get(parent_node_output)\n",
        "\n",
        "            # Check if the traced-back node is a good candidate for exclusion\n",
        "            if current_node and (current_node.op_type == 'MatMul' or current_node.op_type == 'Add' or current_node.op_type == 'Gemm'):\n",
        "                nodes_to_exclude.append(current_node.name)\n",
        "                print(f\"   -> üéØ Automatically identified final node to exclude: '{current_node.name}' ({current_node.op_type})\")\n",
        "            else:\n",
        "                print(f\"   -> ‚ö†Ô∏è  Could not automatically identify a final MatMul/Add/Gemm node to exclude for output '{graph_output}'.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   -> ‚ùå Error analyzing ONNX graph: {e}. Proceeding without exclusions.\")\n",
        "\n",
        "    return nodes_to_exclude\n",
        "\n",
        "\n",
        "# Cell 4: ONNX Helper Classes\n",
        "class ONNXExportWrapper(torch.nn.Module):\n",
        "    \"\"\"A wrapper to ensure model output is a simple tensor for ONNX compatibility.\"\"\"\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids, attention_mask=attention_mask, return_dict=False\n",
        "        )\n",
        "        return outputs[0]\n",
        "\n",
        "class TextCalibrationDataReader(CalibrationDataReader):\n",
        "    \"\"\"A robust data reader that adapts to the model's specific inputs.\"\"\"\n",
        "    def __init__(self, data_df: pd.DataFrame, tokenizer, onnx_model_path: Path):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data_list = data_df[\"text\"].tolist()\n",
        "        self.index = 0\n",
        "\n",
        "        # Find the model's required inputs\n",
        "        session = ort.InferenceSession(str(onnx_model_path), providers=[\"CPUExecutionProvider\"])\n",
        "        model_inputs = {input.name for input in session.get_inputs()}\n",
        "\n",
        "        # Tokenize all data and filter to only include the model's inputs\n",
        "        tokenized_data = self.tokenizer(\n",
        "            self.data_list, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"np\"\n",
        "        )\n",
        "        self.feed = {\n",
        "            key: tokenized_data[key] for key in tokenized_data if key in model_inputs\n",
        "        }\n",
        "        self.input_names = list(self.feed.keys())\n",
        "\n",
        "    def get_next(self):\n",
        "        if self.index >= len(self.data_list):\n",
        "            return None\n",
        "\n",
        "        item = {name: self.feed[name][self.index:self.index+1] for name in self.input_names}\n",
        "        self.index += 1\n",
        "        return item\n",
        "\n",
        "# Cell 5: Main Processing & Export Loop\n",
        "def export_model_to_onnx(model, tokenizer, onnx_path: Path, opset_version: int):\n",
        "    \"\"\"Exports a PyTorch model to the ONNX format.\"\"\"\n",
        "    print(\"   - Wrapping model for ONNX export...\")\n",
        "    wrapped_model = ONNXExportWrapper(model)\n",
        "    wrapped_model.eval()\n",
        "    dummy_input = tokenizer(\"This is a sample sentence.\", return_tensors=\"pt\")\n",
        "    print(f\"   - üöÄ Exporting to ONNX (Opset {opset_version})...\")\n",
        "    torch.onnx.export(\n",
        "        model=wrapped_model,\n",
        "        args=(dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
        "        f=str(onnx_path), input_names=[\"input_ids\", \"attention_mask\"], output_names=[\"output\"],\n",
        "        dynamic_axes={\n",
        "            \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "            \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "            \"output\": {0: \"batch_size\"},\n",
        "        },\n",
        "        opset_version=opset_version, do_constant_folding=True,\n",
        "    )\n",
        "    print(f\"   - ‚úÖ Model successfully exported to {onnx_path.name}\")\n",
        "\n",
        "for model_dir in model_dirs:\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"‚è≥ Processing model: {model_dir.name}\")\n",
        "\n",
        "    onnx_dir = model_dir / \"onnx\"\n",
        "    onnx_dir.mkdir(exist_ok=True)\n",
        "    onnx_model_path = onnx_dir / \"model.onnx\"\n",
        "    quantised_model_path = onnx_dir / \"model-quantised.onnx\"\n",
        "\n",
        "    # --- Step 1: Export to ONNX if needed ---\n",
        "    if not onnx_model_path.exists():\n",
        "        print(\"   - üì¶ ONNX model not found. Starting export...\")\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "            export_model_to_onnx(model, tokenizer, onnx_model_path, ONNX_OPSET_VERSION)\n",
        "            del model, tokenizer\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"   - ‚ùå Export failed for {model_dir.name}: {e}\")\n",
        "            continue\n",
        "    else:\n",
        "        print(f\"   - ‚úÖ Standard ONNX model already exists.\")\n",
        "\n",
        "   \n",
        "print(\"-\" * 70)\n",
        "print(\"üéâ All models have been processed.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPEaWosSVJk00KG/SJV/J7B",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
