{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "README.md\n",
            "\u001b[34mSmolLM2-360M-Instruct-financial-sentiment\u001b[m\u001b[m\n",
            "\u001b[34mbenchmark_results\u001b[m\u001b[m\n",
            "\u001b[34mdata\u001b[m\u001b[m\n",
            "\u001b[34mfigures\u001b[m\u001b[m\n",
            "\u001b[34mmodels\u001b[m\u001b[m\n",
            "\u001b[34mnotebooks\u001b[m\u001b[m\n",
            "requirements.txt\n",
            "\u001b[34mresults\u001b[m\u001b[m\n",
            "\u001b[34mresults2\u001b[m\u001b[m\n",
            "\u001b[34mresults23\u001b[m\u001b[m\n",
            "\u001b[34mresults_moreee\u001b[m\u001b[m\n",
            "\u001b[34msrc\u001b[m\u001b[m\n",
            "\u001b[34mvenv-py311\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/matthew/Documents/deepmind_internship\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found 5 valid models.\n",
            "Loading data from data/FinancialPhraseBank/all-data.csv...\n",
            "‚úÖ Created a calibration dataset with 100 samples.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: all-MiniLM-L6-v2-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "   - ‚öñÔ∏è Performing static quantisation for model.onnx...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "/Users/matthew/Documents/deepmind_internship/venv-py311/lib/python3.11/site-packages/onnxruntime/quantization/quant_utils.py:290: RuntimeWarning: overflow encountered in subtract\n",
            "  dr = numpy.array(rmax - rmin, dtype=numpy.float64)\n",
            "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ‚úÖ Statically quantised model saved to model-quantised.onnx\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: distilbert-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "   - ‚öñÔ∏è Performing static quantisation for model.onnx...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ‚úÖ Statically quantised model saved to model-quantised.onnx\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: finbert-tone-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "   - ‚öñÔ∏è Performing static quantisation for model.onnx...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ‚úÖ Statically quantised model saved to model-quantised.onnx\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: tinybert-financial-classifier\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "   - ‚öñÔ∏è Performing static quantisation for model.onnx...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ‚úÖ Statically quantised model saved to model-quantised.onnx\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: mobilebert-uncased-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "   - ‚öñÔ∏è Performing static quantisation for model.onnx...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   - ‚úÖ Statically quantised model saved to model-quantised.onnx\n",
            "----------------------------------------------------------------------\n",
            "üéâ All models have been processed.\n"
          ]
        }
      ],
      "source": [
        "# ONNX Conversion Code - Run This First!\n",
        "\n",
        "# Cell 1: Imports\n",
        "import gc\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import List\n",
        "\n",
        "# Data & ML\n",
        "import numpy as np\n",
        "import torch\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Suppress ONNX Runtime logging\n",
        "import logging\n",
        "logging.getLogger(\"onnxruntime\").setLevel(logging.ERROR)\n",
        "\n",
        "# Cell 2: Configuration & Model Discovery\n",
        "# Model & ONNX Configuration\n",
        "BASE_DIR = Path(\"models\")\n",
        "ONNX_OPSET_VERSION = 17\n",
        "\n",
        "# Data & Split Configuration\n",
        "DATA_FILE_PATH = Path(\"data/FinancialPhraseBank/all-data.csv\")\n",
        "RANDOM_SEED = 42\n",
        "TEST_SIZE = 0.25 # 25% for the test set\n",
        "\n",
        "def is_valid_model_dir(d: Path) -> bool:\n",
        "    \"\"\"Checks if a directory contains a valid Hugging Face model.\"\"\"\n",
        "    config_exists = (d / \"config.json\").exists()\n",
        "    model_file_exists = (d / \"pytorch_model.bin\").exists() or (d / \"model.safetensors\").exists()\n",
        "    return config_exists and model_file_exists\n",
        "\n",
        "def prepare_calibration_data(data_path, test_size, random_seed, num_samples=100):\n",
        "    \"\"\"Loads, splits, and samples the data to create a calibration set.\"\"\"\n",
        "    print(f\"Loading data from {data_path}...\")\n",
        "    df = pd.read_csv(\n",
        "        data_path,\n",
        "        header=None,\n",
        "        names=['sentiment', 'text'],\n",
        "        encoding='latin-1')\n",
        "\n",
        "    # Split data to get the test set\n",
        "    _, test_df = train_test_split(\n",
        "        df, test_size=test_size, random_state=random_seed, stratify=df['sentiment'])\n",
        "\n",
        "    # Sample the calibration set from the test data\n",
        "    calibration_df = test_df.sample(n=num_samples, random_state=random_seed)\n",
        "    print(f\"‚úÖ Created a calibration dataset with {len(calibration_df)} samples.\")\n",
        "    return calibration_df\n",
        "\n",
        "# Find all valid model directories\n",
        "model_dirs = [d for d in BASE_DIR.iterdir() if d.is_dir() and is_valid_model_dir(d)]\n",
        "print(f\"‚úÖ Found {len(model_dirs)} valid models.\")\n",
        "\n",
        "# Call the function to prepare data\n",
        "calibration_df = prepare_calibration_data(DATA_FILE_PATH, TEST_SIZE, RANDOM_SEED)\n",
        "\n",
        "\n",
        "# NEW - Cell 3: Automated Node Finder\n",
        "def find_final_nodes_to_exclude(onnx_model_path: Path) -> List[str]:\n",
        "    \"\"\"\n",
        "    Analyzes an ONNX model to find the names of the final MatMul or Add nodes\n",
        "    right before the output, which are common candidates for exclusion.\n",
        "    \"\"\"\n",
        "    nodes_to_exclude = []\n",
        "    try:\n",
        "        model = onnx.load(str(onnx_model_path))\n",
        "        \n",
        "        # Create a map of all node outputs to the node that produces them\n",
        "        output_to_node_map = {}\n",
        "        for node in model.graph.node:\n",
        "            for output_name in node.output:\n",
        "                output_to_node_map[output_name] = node\n",
        "\n",
        "        # Find the final output of the graph\n",
        "        graph_outputs = [output.name for output in model.graph.output]\n",
        "        \n",
        "        for graph_output in graph_outputs:\n",
        "            # Find the node that produces this graph output\n",
        "            final_node = output_to_node_map.get(graph_output)\n",
        "            if final_node and (final_node.op_type == 'MatMul' or final_node.op_type == 'Add'):\n",
        "                nodes_to_exclude.append(final_node.name)\n",
        "                print(f\"   -> üéØ Automatically identified final node to exclude: '{final_node.name}' ({final_node.op_type})\")\n",
        "                \n",
        "        if not nodes_to_exclude:\n",
        "            print(\"   -> ‚ö†Ô∏è  Could not automatically identify a final MatMul/Add node to exclude.\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"   -> ‚ùå Error analyzing ONNX graph: {e}. Proceeding without exclusions.\")\n",
        "\n",
        "    return nodes_to_exclude\n",
        "\n",
        "\n",
        "# Cell 4: ONNX Helper Classes\n",
        "class ONNXExportWrapper(torch.nn.Module):\n",
        "    \"\"\"A wrapper to ensure model output is a simple tensor for ONNX compatibility.\"\"\"\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids, attention_mask=attention_mask, return_dict=False\n",
        "        )\n",
        "        return outputs[0]\n",
        "\n",
        "class TextCalibrationDataReader(CalibrationDataReader):\n",
        "    \"\"\"A robust data reader that adapts to the model's specific inputs.\"\"\"\n",
        "    def __init__(self, data_df: pd.DataFrame, tokenizer, onnx_model_path: Path):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data_list = data_df[\"text\"].tolist()\n",
        "        self.index = 0\n",
        "\n",
        "        # Find the model's required inputs\n",
        "        session = ort.InferenceSession(str(onnx_model_path), providers=[\"CPUExecutionProvider\"])\n",
        "        model_inputs = {input.name for input in session.get_inputs()}\n",
        "\n",
        "        # Tokenize all data and filter to only include the model's inputs\n",
        "        tokenized_data = self.tokenizer(\n",
        "            self.data_list, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"np\"\n",
        "        )\n",
        "        self.feed = {\n",
        "            key: tokenized_data[key] for key in tokenized_data if key in model_inputs\n",
        "        }\n",
        "        self.input_names = list(self.feed.keys())\n",
        "\n",
        "    def get_next(self):\n",
        "        if self.index >= len(self.data_list):\n",
        "            return None\n",
        "\n",
        "        item = {name: self.feed[name][self.index:self.index+1] for name in self.input_names}\n",
        "        self.index += 1\n",
        "        return item\n",
        "\n",
        "# Cell 5: Main Processing & Export Loop\n",
        "def export_model_to_onnx(model, tokenizer, onnx_path: Path, opset_version: int):\n",
        "    \"\"\"Exports a PyTorch model to the ONNX format.\"\"\"\n",
        "    print(\"   - Wrapping model for ONNX export...\")\n",
        "    wrapped_model = ONNXExportWrapper(model)\n",
        "    wrapped_model.eval()\n",
        "    dummy_input = tokenizer(\"This is a sample sentence.\", return_tensors=\"pt\")\n",
        "    print(f\"   - üöÄ Exporting to ONNX (Opset {opset_version})...\")\n",
        "    torch.onnx.export(\n",
        "        model=wrapped_model,\n",
        "        args=(dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
        "        f=str(onnx_path), input_names=[\"input_ids\", \"attention_mask\"], output_names=[\"output\"],\n",
        "        dynamic_axes={\n",
        "            \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "            \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "            \"output\": {0: \"batch_size\"},\n",
        "        },\n",
        "        opset_version=opset_version, do_constant_folding=True,\n",
        "    )\n",
        "    print(f\"   - ‚úÖ Model successfully exported to {onnx_path.name}\")\n",
        "\n",
        "for model_dir in model_dirs:\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"‚è≥ Processing model: {model_dir.name}\")\n",
        "\n",
        "    onnx_dir = model_dir / \"onnx\"\n",
        "    onnx_dir.mkdir(exist_ok=True)\n",
        "    onnx_model_path = onnx_dir / \"model.onnx\"\n",
        "    quantised_model_path = onnx_dir / \"model-quantised.onnx\"\n",
        "\n",
        "    # --- Step 1: Export to ONNX if needed ---\n",
        "    if not onnx_model_path.exists():\n",
        "        print(\"   - üì¶ ONNX model not found. Starting export...\")\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "            export_model_to_onnx(model, tokenizer, onnx_model_path, ONNX_OPSET_VERSION)\n",
        "            del model, tokenizer\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"   - ‚ùå Export failed for {model_dir.name}: {e}\")\n",
        "            continue\n",
        "    else:\n",
        "        print(f\"   - ‚úÖ Standard ONNX model already exists.\")\n",
        "\n",
        "    # --- Step 2: Perform Static Quantisation if needed ---\n",
        "    if onnx_model_path.exists() and not quantised_model_path.exists():\n",
        "        print(f\"   - ‚öñÔ∏è Performing static quantisation for {onnx_model_path.name}...\")\n",
        "        try:\n",
        "            # UPDATED: Automatically find nodes to exclude and run robust quantization\n",
        "            nodes_to_exclude = find_final_nodes_to_exclude(onnx_model_path)\n",
        "            \n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "            calibration_data_reader = TextCalibrationDataReader(calibration_df, tokenizer, onnx_model_path)\n",
        "            \n",
        "            quantize_static(\n",
        "                model_input=onnx_model_path,\n",
        "                model_output=quantised_model_path,\n",
        "                calibration_data_reader=calibration_data_reader,\n",
        "                nodes_to_exclude=nodes_to_exclude,\n",
        "                op_types_to_quantize=['MatMul', 'Add'],\n",
        "                extra_options={'ActivationSymmetric': True},\n",
        "                weight_type=QuantType.QInt8,\n",
        "            )\n",
        "            \n",
        "            print(f\"   - ‚úÖ Statically quantised model saved to {quantised_model_path.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   - ‚ùå Static quantisation failed for {model_dir.name}: {e}\")\n",
        "    elif quantised_model_path.exists():\n",
        "         print(f\"   - ‚úÖ Statically quantised model already exists.\")\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(\"üéâ All models have been processed.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPEaWosSVJk00KG/SJV/J7B",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
