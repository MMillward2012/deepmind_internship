{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d1729b",
   "metadata": {},
   "source": [
    "# 🚀 Environment Setup & Validation\n",
    "## Financial Sentiment Analysis Pipeline - Production Setup\n",
    "\n",
    "[![Python](https://img.shields.io/badge/Python-3.8%2B-blue?logo=python&logoColor=white)](https://www.python.org/)\n",
    "[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red?logo=pytorch&logoColor=white)](https://pytorch.org/)\n",
    "[![Transformers](https://img.shields.io/badge/🤗%20Transformers-4.0%2B-yellow)](https://huggingface.co/transformers/)\n",
    "\n",
    "---\n",
    "\n",
    "### 📋 Overview\n",
    "\n",
    "This notebook initialises and validates the complete environment for the **Financial Sentiment Analysis Pipeline**. It serves as the foundation for all subsequent notebooks in the workflow.\n",
    "\n",
    "### 🎯 Key Objectives\n",
    "\n",
    "- **Environment Validation**: Verify all dependencies and system requirements\n",
    "- **Configuration Management**: Load and validate centralised pipeline configuration\n",
    "- **Hardware Detection**: Identify and optimise for available compute resources (CPU/GPU/MPS)\n",
    "- **State Management**: Initialise pipeline state tracking for reproducible runs\n",
    "- **Model Verification**: Confirm availability of required models and datasets\n",
    "\n",
    "### 🏗️ Architecture\n",
    "\n",
    "This pipeline follows a **configuration-driven approach** where all settings are centralised in `../config/pipeline_config.json`. This ensures:\n",
    "\n",
    "- ✅ **Reproducible Results**: Consistent configuration across environments\n",
    "- ✅ **Easy Deployment**: Single configuration file for all settings\n",
    "- ✅ **Flexible Scaling**: Easy to add new models or modify parameters\n",
    "- ✅ **Production Ready**: Structured logging and state management\n",
    "\n",
    "### 📁 Directory Structure\n",
    "\n",
    "```\n",
    "deepmind_internship/\n",
    "├── config/\n",
    "│   ├── pipeline_config.json    # Main configuration file\n",
    "│   └── pipeline_state.json     # Runtime state tracking\n",
    "├── notebooks/                  # Production notebooks\n",
    "├── src/                        # Core pipeline utilities\n",
    "├── data/                       # Training and test datasets\n",
    "└── models/                     # Trained model artifacts\n",
    "```\n",
    "\n",
    "### 🚀 Quick Start\n",
    "\n",
    "1. **Prerequisites**: Ensure Python 3.8+ and required dependencies are installed\n",
    "2. **Configuration**: Review and modify `config/pipeline_config.json` as needed\n",
    "3. **Run Setup**: Execute all cells in this notebook to validate environment\n",
    "4. **Proceed**: Continue to `1_data_processing.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "**⚠️ Important**: Run this notebook first before proceeding with any other pipeline components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152fa192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "🔧 PIPELINE INITIALISATION\n",
    "==========================\n",
    "\n",
    "This cell imports all required dependencies and initialises the configuration system.\n",
    "The pipeline uses a centralised configuration approach for reproducible results.\n",
    "\n",
    "Author: DeepMind Internship Project\n",
    "Date: August 2025\n",
    "Version: 1.0.0\n",
    "\"\"\"\n",
    "\n",
    "# Core Python imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directory to Python path for custom imports\n",
    "sys.path.append('../')\n",
    "\n",
    "try:\n",
    "    # Import custom pipeline utilities\n",
    "    from src.pipeline_utils import ConfigManager, StateManager, LoggingManager\n",
    "    print(\"✅ Custom pipeline utilities imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Failed to import pipeline utilities: {e}\")\n",
    "    print(\"💡 Ensure you're running from the notebooks/ directory\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    # Core ML and data processing libraries\n",
    "    import torch\n",
    "    import transformers\n",
    "    import datasets\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(\"✅ Core ML libraries imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Missing required dependencies: {e}\")\n",
    "    print(\"💡 Run: pip install -r requirements.txt\")\n",
    "    raise\n",
    "\n",
    "# Initialise configuration system\n",
    "print(\"\\n🔧 Initialising pipeline configuration...\")\n",
    "\n",
    "# Define file paths\n",
    "config_path = Path(\"../config/pipeline_config.json\")\n",
    "state_path = Path(\"../config/pipeline_state.json\")\n",
    "\n",
    "# Initialize managers with proper parameters\n",
    "config_manager = ConfigManager(config_path=str(config_path))\n",
    "state_manager = StateManager(state_path=str(state_path))\n",
    "\n",
    "# Get the loaded configuration (load_config() is called in __init__)\n",
    "config = config_manager.config\n",
    "\n",
    "# Initialize logging manager with config and component name\n",
    "logging_manager = LoggingManager(config=config_manager, component_name='setup')\n",
    "\n",
    "# Set up remaining variables\n",
    "state = state_manager\n",
    "logger = logging_manager.get_logger()\n",
    "\n",
    "print(\"✅ Configuration system initialised\")\n",
    "print(f\"   📋 Config loaded: {len(config)} top-level sections\")\n",
    "print(f\"   📊 State tracking: {'enabled' if state else 'disabled'}\")\n",
    "print(f\"   📝 Logging: {'configured' if logger else 'basic'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d1fd41",
   "metadata": {},
   "source": [
    "## 🔧 Initial Setup & Dependencies\n",
    "\n",
    "### 📦 Required Libraries\n",
    "\n",
    "The following libraries are essential for the pipeline:\n",
    "\n",
    "- **Core ML**: `torch`, `transformers`, `datasets`\n",
    "- **Data Processing**: `pandas`, `numpy`, `scikit-learn`\n",
    "- **Visualisation**: `matplotlib`, `seaborn`, `plotly`\n",
    "- **Utilities**: `pathlib`, `json`, `logging`\n",
    "\n",
    "### ⚙️ Configuration System\n",
    "\n",
    "This pipeline uses a centralised configuration management system:\n",
    "\n",
    "- **ConfigManager**: Loads and validates `pipeline_config.json`\n",
    "- **StateManager**: Tracks pipeline execution state\n",
    "- **LoggingManager**: Provides structured logging across all components\n",
    "\n",
    "### 🚨 Prerequisites\n",
    "\n",
    "Before proceeding, ensure:\n",
    "\n",
    "1. ✅ All dependencies are installed via `pip install -r requirements.txt`\n",
    "2. ✅ Configuration file `config/pipeline_config.json` exists and is valid\n",
    "3. ✅ Sufficient disk space for models and datasets (~5GB recommended)\n",
    "4. ✅ GPU drivers installed (if using CUDA)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72fa1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate prerequisites and environment\n",
    "logger.info(\"🔍 Validating environment and dependencies...\")\n",
    "\n",
    "print(\"🔍 Environment Validation:\")\n",
    "print(f\"   📍 Current working directory: {os.getcwd()}\")\n",
    "print(f\"   🐍 Python version: {sys.version}\")\n",
    "print(f\"   🤗 Transformers version: {transformers.__version__}\")\n",
    "print(f\"   📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"   🔢 NumPy version: {np.__version__}\")\n",
    "\n",
    "# Device detection and configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"   🔧 Primary device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   🚀 CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   💾 CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"   🍎 Apple Metal Performance Shaders (MPS) available\")\n",
    "else:\n",
    "    print(\"   💻 Using CPU\")\n",
    "\n",
    "print(\"✅ Environment validation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a39852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate configuration and directory structure\n",
    "logger.info(\"📋 Validating pipeline configuration...\")\n",
    "\n",
    "print(\"📋 Configuration Validation:\")\n",
    "\n",
    "# Check if configuration files exist\n",
    "config_file = Path(\"../config/pipeline_config.json\")\n",
    "utils_file = Path(\"../src/pipeline_utils.py\")\n",
    "\n",
    "if config_file.exists():\n",
    "    print(f\"   ✅ Configuration file found: {config_file}\")\n",
    "else:\n",
    "    print(f\"   ❌ Configuration file missing: {config_file}\")\n",
    "\n",
    "if utils_file.exists():\n",
    "    print(f\"   ✅ Utilities module found: {utils_file}\")\n",
    "else:\n",
    "    print(f\"   ❌ Utilities module missing: {utils_file}\")\n",
    "\n",
    "# Validate configuration structure\n",
    "try:\n",
    "    data_config = config.get('data', {})\n",
    "    models_config = config.get('models', {})\n",
    "    training_config = config.get('training', {})\n",
    "    \n",
    "    print(f\"   📊 Data sources configured: {len(data_config.get('datasets', {}))}\")\n",
    "    print(f\"   🤖 Models configured: {len(models_config.get('base_models', []))}\")\n",
    "    print(f\"   🏋️ Training epochs: {training_config.get('num_epochs', 'Not set')}\")\n",
    "    print(\"   ✅ Configuration structure validated\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Configuration validation failed: {str(e)}\")\n",
    "    print(f\"   ❌ Configuration validation failed: {str(e)}\")\n",
    "\n",
    "# Check required directories\n",
    "required_dirs = [\"../data\", \"../models\", \"../results\", \"../config\", \"../src\"]\n",
    "print(\"\\n📁 Directory Structure:\")\n",
    "\n",
    "for dir_path in required_dirs:\n",
    "    path = Path(dir_path)\n",
    "    if path.exists():\n",
    "        print(f\"   ✅ {dir_path}\")\n",
    "    else:\n",
    "        print(f\"   ❌ {dir_path} (will be created as needed)\")\n",
    "        \n",
    "print(\"✅ Configuration validation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model availability and base model verification\n",
    "logger.info(\"🤖 Checking model availability...\")\n",
    "\n",
    "print(\"🤖 Model Availability Check:\")\n",
    "\n",
    "# Check configured models\n",
    "models_config = config.get('models', {})\n",
    "base_models = models_config.get('base_models', [])\n",
    "available_models = []\n",
    "missing_models = []\n",
    "\n",
    "for model_config in base_models:\n",
    "    if model_config.get('enabled', True):\n",
    "        model_name = model_config['name']\n",
    "        model_id = model_config['model_id']\n",
    "        \n",
    "        try:\n",
    "            # Try to load tokenizer to verify model accessibility\n",
    "            from transformers import AutoTokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "            available_models.append(model_name)\n",
    "            print(f\"   ✅ {model_name} ({model_id})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            missing_models.append(model_name)\n",
    "            print(f\"   ❌ {model_name} ({model_id}) - {str(e)[:100]}...\")\n",
    "\n",
    "print(f\"\\n📊 Model Summary:\")\n",
    "print(f\"   ✅ Available: {len(available_models)} models\")\n",
    "print(f\"   ❌ Missing: {len(missing_models)} models\")\n",
    "\n",
    "if missing_models:\n",
    "    print(f\"\\n⚠️ Missing models will be downloaded during training.\")\n",
    "\n",
    "# Check data availability\n",
    "print(f\"\\n📂 Data Availability:\")\n",
    "data_config = config.get('data', {})\n",
    "\n",
    "# Check main raw data path\n",
    "raw_data_path = Path(f\"../{data_config.get('raw_data_path', '')}\")\n",
    "if raw_data_path.exists():\n",
    "    print(f\"   ✅ Main dataset: {raw_data_path}\")\n",
    "else:\n",
    "    print(f\"   ❌ Main dataset: {raw_data_path}\")\n",
    "\n",
    "# Check processed data directory\n",
    "processed_dir = Path(f\"../{data_config.get('processed_data_dir', '')}\")\n",
    "if processed_dir.exists():\n",
    "    print(f\"   ✅ Processed data dir: {processed_dir}\")\n",
    "else:\n",
    "    print(f\"   ❌ Processed data dir: {processed_dir} (will be created)\")\n",
    "\n",
    "print(\"✅ Model availability check completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e46667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize pipeline state and complete setup\n",
    "logger.info(\"🔄 Initializing pipeline state...\")\n",
    "\n",
    "print(\"🔄 Pipeline State Initialization:\")\n",
    "\n",
    "# Initialize or update pipeline state\n",
    "setup_info = {\n",
    "    'setup_timestamp': datetime.now().isoformat(),\n",
    "    'python_version': sys.version,\n",
    "    'device': str(device),\n",
    "    'available_models': available_models if 'available_models' in locals() else [],\n",
    "    'missing_models': missing_models if 'missing_models' in locals() else [],\n",
    "    'configuration_valid': True,\n",
    "    'environment_ready': True\n",
    "}\n",
    "\n",
    "# Mark setup as completed\n",
    "state.mark_step_complete('setup_completed', **setup_info)\n",
    "\n",
    "print(\"   ✅ Pipeline state initialized\")\n",
    "print(f\"   📅 Setup timestamp: {setup_info['setup_timestamp']}\")\n",
    "print(f\"   🔧 Device configured: {setup_info['device']}\")\n",
    "print(f\"   🤖 Models available: {len(setup_info['available_models'])}\")\n",
    "\n",
    "# Display next steps\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"🎉 SETUP COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"📝 Next Steps:\")\n",
    "print(\"1. Run 1_data_processing.ipynb to process and prepare data\")\n",
    "print(\"2. Run 2_train_models.ipynb to train the models\")\n",
    "print(\"3. Continue with the sequential pipeline: 3 → 4 → 5 → 6\")\n",
    "\n",
    "# Safe variable access for configuration summary\n",
    "data_config = locals().get('data_config', config.get('data', {}))\n",
    "models_config = locals().get('models_config', config.get('models', {}))\n",
    "training_config = locals().get('training_config', config.get('training', {}))\n",
    "base_models = locals().get('base_models', models_config.get('base_models', []))\n",
    "\n",
    "print(f\"\\n🔧 Configuration Summary:\")\n",
    "print(f\"   📊 Data source: {data_config.get('raw_data_path', 'Not configured')}\")\n",
    "print(f\"   🤖 Models configured: {len(base_models)}\")\n",
    "print(f\"   🏋️ Training epochs: {training_config.get('num_epochs', 'Not configured')}\")\n",
    "print(f\"   📈 Batch size: {training_config.get('batch_size', 'Not configured')}\")\n",
    "\n",
    "logger.info(\"✅ Environment setup completed successfully\")\n",
    "\n",
    "# Save setup report\n",
    "setup_report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'status': 'completed',\n",
    "    'environment': {\n",
    "        'python_version': sys.version,\n",
    "        'device': str(device),\n",
    "        'torch_version': torch.__version__,\n",
    "        'transformers_version': transformers.__version__\n",
    "    },\n",
    "    'configuration': {\n",
    "        'config_file_exists': locals().get('config_file', Path(\"../config/pipeline_config.json\")).exists(),\n",
    "        'utils_file_exists': locals().get('utils_file', Path(\"../src/pipeline_utils.py\")).exists(),\n",
    "        'models_configured': len(base_models),\n",
    "        'data_source_configured': bool(data_config.get('raw_data_path'))\n",
    "    },\n",
    "    'models': {\n",
    "        'available': available_models if 'available_models' in locals() else [],\n",
    "        'missing': missing_models if 'missing_models' in locals() else []\n",
    "    }\n",
    "}\n",
    "\n",
    "# Ensure results directory exists\n",
    "results_dir = Path(\"../results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save setup report\n",
    "with open(results_dir / 'setup_report.json', 'w') as f:\n",
    "    json.dump(setup_report, f, indent=2)\n",
    "\n",
    "print(f\"\\n📄 Setup report saved to: {results_dir / 'setup_report.json'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
