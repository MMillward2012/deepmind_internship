{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMillward2012/deepmind_internship/blob/main/notebooks/7_benchmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "README.md        \u001b[34mmodels\u001b[m\u001b[m           \u001b[34mresults\u001b[m\u001b[m\n",
            "\u001b[34mdata\u001b[m\u001b[m             \u001b[34mnotebooks\u001b[m\u001b[m        \u001b[34msrc\u001b[m\u001b[m\n",
            "\u001b[34mfigures\u001b[m\u001b[m          requirements.txt \u001b[34mvenv-py311\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "# %cd ..\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RTwrJ4dSirfW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
        "from transformers.onnx import export\n",
        "from transformers.onnx.features import FeaturesManager\n",
        "import onnxruntime as ort\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\"models\")\n",
        "EXAMPLE_INPUT = \"Stocks surged after the company reported record earnings.\"\n",
        "MAX_LENGTH = 128\n",
        "ONNX_OPSET = 13\n",
        "BENCHMARK_ITERATIONS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found models: ['all-MiniLM-L6-v2-financial-sentiment', 'distilbert-financial-sentiment', 'tinybert-financial-classifier', 'mobilebert-uncased-financial-sentiment']\n"
          ]
        }
      ],
      "source": [
        "model_dirs = [d for d in BASE_DIR.iterdir() if d.is_dir()]\n",
        "print(\"Found models:\", [m.name for m in model_dirs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_to_onnx(model_dir, onnx_path, task=\"sequence-classification\"):\n",
        "    config = AutoConfig.from_pretrained(model_dir)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    \n",
        "    model_kind, onnx_config_class = FeaturesManager.check_supported_model_or_raise(config, task=task)\n",
        "    onnx_config = onnx_config_class(config)\n",
        "    \n",
        "    export(\n",
        "        preprocessor=tokenizer,\n",
        "        model=model,\n",
        "        config=onnx_config,\n",
        "        opset=ONNX_OPSET,\n",
        "        output=onnx_path\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def benchmark_onnx(onnx_model_path, tokenizer, quantised=False):\n",
        "    sess = ort.InferenceSession(str(onnx_model_path), providers=[\"CPUExecutionProvider\"])\n",
        "    inputs = tokenizer(EXAMPLE_INPUT, return_tensors=\"np\", max_length=MAX_LENGTH, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # Warm-up\n",
        "    for _ in range(10):\n",
        "        sess.run(None, {\"input_ids\": inputs[\"input_ids\"]})\n",
        "\n",
        "    # Benchmark\n",
        "    times = []\n",
        "    for _ in range(BENCHMARK_ITERATIONS):\n",
        "        start = time.time()\n",
        "        sess.run(None, {\"input_ids\": inputs[\"input_ids\"]})\n",
        "        times.append((time.time() - start) * 1000)\n",
        "\n",
        "    return {\n",
        "        \"avg_latency_ms\": np.mean(times),\n",
        "        \"p99_latency_ms\": np.percentile(times, 99),\n",
        "        \"quantised\": quantised\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for model_dir in model_dirs:\n",
        "    print(f\"\\n‚è≥ Processing {model_dir.name}...\")\n",
        "    \n",
        "    onnx_dir = model_dir / \"onnx\"\n",
        "    onnx_dir.mkdir(exist_ok=True)\n",
        "    onnx_model_path = onnx_dir / \"model.onnx\"\n",
        "    quantised_model_path = onnx_dir / \"model-int8.onnx\"\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "    # Export ONNX if not already done\n",
        "    if not onnx_model_path.exists():\n",
        "        print(\"üì¶ Exporting to ONNX...\")\n",
        "        export_to_onnx(model_dir, onnx_model_path)\n",
        "    else:\n",
        "        print(\"‚úÖ ONNX already exists.\")\n",
        "\n",
        "    # Benchmark original\n",
        "    print(\"üß™ Benchmarking original model...\")\n",
        "    result_fp32 = benchmark_onnx(onnx_model_path, tokenizer, quantised=False)\n",
        "    result_fp32[\"model\"] = model_dir.name\n",
        "    result_fp32[\"size_mb\"] = onnx_model_path.stat().st_size / 1e6\n",
        "    results.append(result_fp32)\n",
        "\n",
        "    # Quantise if not already done\n",
        "    if not quantised_model_path.exists():\n",
        "        print(\"‚öôÔ∏è  Quantising...\")\n",
        "        quantize_dynamic(str(onnx_model_path), str(quantised_model_path), weight_type=QuantType.QInt8)\n",
        "    else:\n",
        "        print(\"‚úÖ Quantised model already exists.\")\n",
        "\n",
        "    # Benchmark quantised\n",
        "    print(\"üß™ Benchmarking quantised model...\")\n",
        "    result_int8 = benchmark_onnx(quantised_model_path, tokenizer, quantised=True)\n",
        "    result_int8[\"model\"] = model_dir.name + \" (INT8)\"\n",
        "    result_int8[\"size_mb\"] = quantised_model_path.stat().st_size / 1e6\n",
        "    results.append(result_int8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(results)\n",
        "df = df[[\"model\", \"size_mb\", \"avg_latency_ms\", \"p99_latency_ms\", \"quantised\"]]\n",
        "df = df.sort_values(by=\"avg_latency_ms\")\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df.style.format({\n",
        "    \"size_mb\": \"{:.1f}\",\n",
        "    \"avg_latency_ms\": \"{:.2f}\",\n",
        "    \"p99_latency_ms\": \"{:.2f}\"\n",
        "})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPEaWosSVJk00KG/SJV/J7B",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
