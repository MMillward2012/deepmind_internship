{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46df93cc",
   "metadata": {},
   "source": [
    "# üß† Financial Sentiment Model Explainability Dashboard\n",
    "\n",
    "## Overview\n",
    "This notebook provides comprehensive explainability analysis for the fine-tuned TinyBERT financial sentiment classification model. It includes four complementary explanation methods accessible through an interactive dashboard.\n",
    "\n",
    "### Explanation Methods\n",
    "- **üéØ SHAP**: Game-theory based feature importance\n",
    "- **üîç LIME**: Local interpretable model-agnostic explanations \n",
    "- **üëÅÔ∏è Attention**: Model attention head visualization\n",
    "- **üå°Ô∏è GradCAM**: Gradient-based visual attribution\n",
    "\n",
    "### Dashboard Features\n",
    "- **Mistake Analysis**: Examine specific model errors\n",
    "- **Custom Text Analysis**: Test any financial text\n",
    "- **Interactive Interface**: Tabbed layout for easy comparison\n",
    "- **On-demand Computation**: Optimized performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097b30f",
   "metadata": {},
   "source": [
    "## 1. üì¶ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09540612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_setup.ipynb                   4_benchmarks.ipynb\n",
      "1_data_processing.ipynb         5_explainability.ipynb\n",
      "2_train_models.ipynb            5_explainability_original.ipynb\n",
      "3_convert_to_onnx.ipynb         6_fine_tune.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "180d1813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/matthew/Documents/deepmind_internship\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36cc0361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model and tokenizer\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Explainability libraries\n",
    "import shap\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from bertviz import head_view\n",
    "from captum.attr import LayerGradCam\n",
    "\n",
    "# Dashboard components\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af47f79",
   "metadata": {},
   "source": [
    "## 2. üóÇÔ∏è Data & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd313ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded full dataset: 4846 samples\n",
      "üìä Test set: 1212 samples (25% split)\n",
      "üìã Label classes: ['negative', 'neutral', 'positive']\n",
      "üé≤ Random seed: 42\n",
      "‚úÖ Data loaded successfully with correct encoding\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_DIR = Path('models/tinybert-financial-classifier')\n",
    "DATA_FILE = 'data/FinancialPhraseBank/all-data.csv'\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.25\n",
    "\n",
    "# Load full dataset and create train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data with correct encoding and column names (matching training notebook)\n",
    "df = pd.read_csv(DATA_FILE, header=None, names=[\"label\", \"sentence\"], encoding=\"latin-1\")\n",
    "df[\"sentence\"] = df[\"sentence\"].str.strip('\"')  # Remove extra quotes\n",
    "\n",
    "# Create train-test split with same parameters as training\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_SEED, \n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "# Extract test data\n",
    "test_texts = test_df['sentence'].tolist()  # Note: column is 'sentence' not 'text'\n",
    "\n",
    "# Load label encoder\n",
    "import pickle\n",
    "with open(MODEL_DIR / 'label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(test_df['label'])\n",
    "\n",
    "print(f\"üìä Loaded full dataset: {len(df)} samples\")\n",
    "print(f\"üìä Test set: {len(test_texts)} samples (25% split)\")\n",
    "print(f\"üìã Label classes: {list(label_encoder.classes_)}\")\n",
    "print(f\"üé≤ Random seed: {RANDOM_SEED}\")\n",
    "print(f\"‚úÖ Data loaded successfully with correct encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "816cc105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading model and tokenizer...\n",
      "‚úÖ Model and tokenizer loaded successfully\n",
      "üì± Model type: BertForSequenceClassification\n",
      "üéØ Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "print(\"üîÑ Loading model and tokenizer...\")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_DIR)\n",
    "pt_model = BertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "pt_model.eval()\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer loaded successfully\")\n",
    "print(f\"üì± Model type: {type(pt_model).__name__}\")\n",
    "print(f\"üéØ Number of classes: {pt_model.config.num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c8c2e3",
   "metadata": {},
   "source": [
    "## 3. üîß Core Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70ec723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core prediction functions defined\n"
     ]
    }
   ],
   "source": [
    "def predict_class(texts):\n",
    "    \"\"\"Predict sentiment class for text(s)\"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    predictions = []\n",
    "    pt_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            encoding = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "            outputs = pt_model(**encoding)\n",
    "            predicted_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "            predictions.append(predicted_class)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "def predict_probs_for_shap(texts):\n",
    "    \"\"\"Get prediction probabilities for SHAP\"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    all_probs = []\n",
    "    pt_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            encoding = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "            outputs = pt_model(**encoding)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1).squeeze().numpy()\n",
    "            all_probs.append(probs)\n",
    "    \n",
    "    return np.array(all_probs)\n",
    "\n",
    "def predict_probs_for_lime(texts):\n",
    "    \"\"\"Get prediction probabilities for LIME (expects different format)\"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    elif isinstance(texts, list) and len(texts) == 1 and isinstance(texts[0], str):\n",
    "        # LIME sometimes passes single text as list\n",
    "        texts = texts\n",
    "    elif isinstance(texts, (list, tuple)) and all(isinstance(t, str) for t in texts):\n",
    "        # LIME passes list of texts\n",
    "        texts = list(texts)\n",
    "    else:\n",
    "        # Convert to list if needed\n",
    "        texts = [str(t) for t in texts]\n",
    "    \n",
    "    all_probs = []\n",
    "    pt_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            try:\n",
    "                encoding = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "                outputs = pt_model(**encoding)\n",
    "                probs = torch.softmax(outputs.logits, dim=-1).squeeze().cpu().numpy()\n",
    "                all_probs.append(probs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text: {text[:50]}... Error: {e}\")\n",
    "                # Return default probabilities if processing fails\n",
    "                all_probs.append(np.array([0.33, 0.33, 0.34]))\n",
    "    \n",
    "    return np.array(all_probs)\n",
    "\n",
    "print(\"‚úÖ Core prediction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879bac0",
   "metadata": {},
   "source": [
    "## 4. üß© Explainability Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d73dd7",
   "metadata": {},
   "source": [
    "### 4.1 SHAP Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "877e6941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SHAP implementation ready\n"
     ]
    }
   ],
   "source": [
    "# SHAP explainer (lazy initialization for performance)\n",
    "_shap_explainer = None\n",
    "\n",
    "def get_shap_explainer():\n",
    "    \"\"\"Get SHAP explainer (lazy initialization)\"\"\"\n",
    "    global _shap_explainer\n",
    "    if _shap_explainer is None:\n",
    "        print(\"üßÆ Initializing SHAP explainer...\")\n",
    "        _shap_explainer = shap.Explainer(predict_probs_for_shap, tokenizer)\n",
    "    return _shap_explainer\n",
    "\n",
    "def explain_with_shap(text, target_class=None):\n",
    "    \"\"\"Generate SHAP explanation for text\"\"\"\n",
    "    print(\"‚è≥ Computing SHAP values...\")\n",
    "    \n",
    "    explainer = get_shap_explainer()\n",
    "    shap_values = explainer([text])\n",
    "    \n",
    "    if target_class is None:\n",
    "        target_class = predict_class(text)[0]\n",
    "    \n",
    "    # Display SHAP plot\n",
    "    shap.plots.text(shap_values[0, :, target_class])\n",
    "    \n",
    "    pred_label = label_encoder.inverse_transform([target_class])[0]\n",
    "    print(f\"üìä SHAP explanation for class: {pred_label}\")\n",
    "\n",
    "print(\"‚úÖ SHAP implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35809a96",
   "metadata": {},
   "source": [
    "### 4.2 LIME Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d641ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LIME implementation ready\n"
     ]
    }
   ],
   "source": [
    "# LIME explainer (lazy initialization)\n",
    "_lime_explainer = None\n",
    "\n",
    "def get_lime_explainer():\n",
    "    \"\"\"Get LIME explainer (lazy initialization)\"\"\"\n",
    "    global _lime_explainer\n",
    "    if _lime_explainer is None:\n",
    "        _lime_explainer = LimeTextExplainer(\n",
    "            class_names=label_encoder.classes_\n",
    "            # Removed 'mode' parameter as it's not valid for LimeTextExplainer\n",
    "        )\n",
    "    return _lime_explainer\n",
    "\n",
    "def explain_with_lime(text):\n",
    "    \"\"\"Generate LIME explanation for text\"\"\"\n",
    "    print(\"‚è≥ Computing LIME explanation...\")\n",
    "    \n",
    "    explainer = get_lime_explainer()\n",
    "    explanation = explainer.explain_instance(\n",
    "        text,\n",
    "        predict_probs_for_lime,\n",
    "        num_features=20,\n",
    "        labels=(0, 1, 2)\n",
    "    )\n",
    "    \n",
    "    display(HTML(explanation.as_html()))\n",
    "    print(\"üìä LIME explanation generated\")\n",
    "\n",
    "print(\"‚úÖ LIME implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dad249",
   "metadata": {},
   "source": [
    "### 4.3 Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5eca8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Attention visualization ready\n"
     ]
    }
   ],
   "source": [
    "def explain_with_attention(text):\n",
    "    \"\"\"Generate attention visualization for text\"\"\"\n",
    "    print(\"‚è≥ Generating attention visualization...\")\n",
    "    \n",
    "    try:\n",
    "        # Tokenize with attention output\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        \n",
    "        # Get model outputs with attention\n",
    "        with torch.no_grad():\n",
    "            # Force eager attention for BertViz compatibility\n",
    "            original_impl = getattr(pt_model.config, '_attn_implementation', None)\n",
    "            pt_model.config._attn_implementation = 'eager'\n",
    "            \n",
    "            outputs = pt_model(**inputs, output_attentions=True)\n",
    "            attentions = outputs.attentions\n",
    "            tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "            \n",
    "            # Restore original implementation\n",
    "            if original_impl is not None:\n",
    "                pt_model.config._attn_implementation = original_impl\n",
    "        \n",
    "        # Check if we have valid attention and tokens\n",
    "        if attentions is None or len(attentions) == 0:\n",
    "            print(\"‚ùå No attention weights available\")\n",
    "            return\n",
    "            \n",
    "        if len(tokens) == 0:\n",
    "            print(\"‚ùå No tokens available\")\n",
    "            return\n",
    "        \n",
    "        pred_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "        pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "        \n",
    "        # Try BertViz first\n",
    "        try:\n",
    "            # Enable widget display\n",
    "            from IPython.display import Javascript\n",
    "            display(Javascript(\"\"\"\n",
    "                require.config({\n",
    "                    paths: {\n",
    "                        d3: 'https://d3js.org/d3.v5.min'\n",
    "                    }\n",
    "                });\n",
    "            \"\"\"))\n",
    "            \n",
    "            print(\"üéØ Attempting interactive attention visualization...\")\n",
    "            head_view(attentions, tokens)\n",
    "            print(f\"üëÅÔ∏è Interactive attention visualization for prediction: {pred_label}\")\n",
    "            \n",
    "        except Exception as viz_error:\n",
    "            print(f\"‚ùå BertViz interactive view failed: {viz_error}\")\n",
    "            print(\"üí° Using custom attention heatmap...\")\n",
    "            \n",
    "            # Custom attention visualization\n",
    "            _visualize_attention_heatmap(attentions, tokens, pred_label)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Attention analysis failed: {str(e)}\")\n",
    "        print(\"üí° This might be due to model architecture or BertViz compatibility issues\")\n",
    "\n",
    "def _visualize_attention_heatmap(attentions, tokens, pred_label):\n",
    "    \"\"\"Create custom attention heatmap visualization\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Get average attention across all layers and heads\n",
    "    # Shape: (num_layers, batch_size, num_heads, seq_len, seq_len)\n",
    "    avg_attention = torch.stack(attentions).mean(dim=0)  # Average across layers\n",
    "    avg_attention = avg_attention.mean(dim=1)  # Average across heads\n",
    "    attention_matrix = avg_attention[0].detach().cpu().numpy()  # Get first (and only) batch\n",
    "    \n",
    "    # Clean tokens for display\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.startswith('##'):\n",
    "            clean_tokens.append(token[2:])\n",
    "        elif token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            clean_tokens.append(token)\n",
    "        else:\n",
    "            clean_tokens.append(token)\n",
    "    \n",
    "    # Limit to reasonable size for visualization\n",
    "    max_len = min(len(clean_tokens), 50)\n",
    "    attention_matrix = attention_matrix[:max_len, :max_len]\n",
    "    display_tokens = clean_tokens[:max_len]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # 1. Full attention heatmap\n",
    "    sns.heatmap(attention_matrix, \n",
    "                xticklabels=display_tokens,\n",
    "                yticklabels=display_tokens,\n",
    "                cmap='Blues',\n",
    "                ax=ax1,\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    ax1.set_title(f'Attention Heatmap\\nPrediction: {pred_label}', fontsize=14, weight='bold')\n",
    "    ax1.set_xlabel('Attended Tokens')\n",
    "    ax1.set_ylabel('Query Tokens')\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.setp(ax1.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    # 2. CLS token attention (what the model focuses on for classification)\n",
    "    cls_attention = attention_matrix[0, 1:]  # CLS token attention to other tokens (skip self-attention)\n",
    "    tokens_for_cls = display_tokens[1:]  # Skip CLS token\n",
    "    \n",
    "    # Sort by attention weight\n",
    "    token_attention_pairs = list(zip(tokens_for_cls, cls_attention))\n",
    "    token_attention_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Take top 15 for readability\n",
    "    top_tokens, top_weights = zip(*token_attention_pairs[:15])\n",
    "    \n",
    "    bars = ax2.barh(range(len(top_tokens)), top_weights, color='skyblue')\n",
    "    ax2.set_yticks(range(len(top_tokens)))\n",
    "    ax2.set_yticklabels(top_tokens)\n",
    "    ax2.set_xlabel('Attention Weight')\n",
    "    ax2.set_title(f'Top Attended Tokens for Classification\\n(CLS token attention)', fontsize=14, weight='bold')\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, weight) in enumerate(zip(bars, top_weights)):\n",
    "        ax2.text(weight + 0.001, i, f'{weight:.3f}', \n",
    "                va='center', ha='left', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"üìä Attention Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Number of layers: {len(attentions)}\")\n",
    "    print(f\"   ‚Ä¢ Number of heads per layer: {attentions[0].shape[2]}\")\n",
    "    print(f\"   ‚Ä¢ Sequence length: {len(tokens)}\")\n",
    "    print(f\"   ‚Ä¢ Max attention weight: {attention_matrix.max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Average attention weight: {attention_matrix.mean():.4f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Top 5 tokens by CLS attention:\")\n",
    "    for i, (token, weight) in enumerate(token_attention_pairs[:5]):\n",
    "        if token not in ['[SEP]', '[PAD]']:\n",
    "            print(f\"   {i+1}. '{token}': {weight:.4f}\")\n",
    "    \n",
    "    print(f\"üëÅÔ∏è Custom attention visualization complete for: {pred_label}\")\n",
    "\n",
    "print(\"‚úÖ Attention visualization ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d66aab3",
   "metadata": {},
   "source": [
    "### 4.4 GradCAM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "470f20d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GradCAM implementation ready\n"
     ]
    }
   ],
   "source": [
    "class ModelWrapper(torch.nn.Module):\n",
    "    \"\"\"Wrapper to fix SequenceClassifierOutput error with Captum\"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "def explain_with_gradcam(text, target_class=None):\n",
    "    \"\"\"Generate GradCAM explanation for text\"\"\"\n",
    "    print(\"‚è≥ Computing GradCAM attributions...\")\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        # Get prediction if not specified\n",
    "        if target_class is None:\n",
    "            with torch.no_grad():\n",
    "                outputs = pt_model(**inputs)\n",
    "                target_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "        \n",
    "        # Ensure target_class is the right type for Captum\n",
    "        target_class = int(target_class)  # Convert to Python int from numpy.int64\n",
    "        \n",
    "        # Use ModelWrapper for Captum compatibility\n",
    "        wrapped_model = ModelWrapper(pt_model)\n",
    "        wrapped_model.eval()\n",
    "        \n",
    "        # Try to access embedding layer with different paths\n",
    "        embedding_layer = None\n",
    "        try:\n",
    "            embedding_layer = pt_model.bert.embeddings.word_embeddings\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                embedding_layer = pt_model.embeddings.word_embeddings\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    embedding_layer = pt_model.get_input_embeddings()\n",
    "                except AttributeError:\n",
    "                    print(\"‚ùå Could not access embedding layer\")\n",
    "                    return\n",
    "        \n",
    "        if embedding_layer is None:\n",
    "            print(\"‚ùå Embedding layer not found\")\n",
    "            return\n",
    "        \n",
    "        # Initialize LayerGradCam\n",
    "        layer_gradcam = LayerGradCam(wrapped_model, embedding_layer)\n",
    "        \n",
    "        # Generate attributions\n",
    "        attributions = layer_gradcam.attribute(\n",
    "            input_ids,\n",
    "            target=target_class,\n",
    "            additional_forward_args=(attention_mask,)\n",
    "        )\n",
    "        \n",
    "        # Process attributions\n",
    "        attribution_scores = attributions.squeeze().detach().cpu().numpy()\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "        \n",
    "        if len(attribution_scores.shape) > 1:\n",
    "            attribution_scores = attribution_scores.sum(axis=-1)\n",
    "        \n",
    "        # Visualize\n",
    "        _visualize_gradcam(tokens, attribution_scores, attention_mask, target_class)\n",
    "        \n",
    "        pred_label = label_encoder.inverse_transform([target_class])[0]\n",
    "        print(f\"üå°Ô∏è GradCAM explanation for class: {pred_label}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GradCAM error: {str(e)}\")\n",
    "        print(\"üí° Falling back to attention-based attribution...\")\n",
    "        \n",
    "        # Fallback: Use attention weights as pseudo-GradCAM\n",
    "        try:\n",
    "            inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = pt_model(**inputs, output_attentions=True)\n",
    "                attentions = outputs.attentions\n",
    "                \n",
    "                if attentions is not None and len(attentions) > 0:\n",
    "                    # Average attention across layers and heads\n",
    "                    avg_attention = torch.stack(attentions).mean(dim=0).mean(dim=1)\n",
    "                    cls_attention = avg_attention[0, 0, :].detach().cpu().numpy()\n",
    "                    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze())\n",
    "                    \n",
    "                    # Visualize as pseudo-GradCAM\n",
    "                    _visualize_gradcam(tokens, cls_attention, inputs['attention_mask'], \n",
    "                                     torch.argmax(outputs.logits, dim=-1).item())\n",
    "                    print(\"üìä Used attention weights as attribution fallback\")\n",
    "                else:\n",
    "                    print(\"‚ùå No attention weights available for fallback\")\n",
    "        except Exception as fallback_error:\n",
    "            print(f\"‚ùå Fallback also failed: {fallback_error}\")\n",
    "            print(\"üí° Try using SHAP or LIME for alternative explanations\")\n",
    "\n",
    "def _visualize_gradcam(tokens, attribution_scores, attention_mask, target_class):\n",
    "    \"\"\"Create GradCAM visualization\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    \n",
    "    # Normalize attributions\n",
    "    abs_attributions = np.abs(attribution_scores)\n",
    "    if abs_attributions.max() > 0:\n",
    "        normalized_attrs = abs_attributions / abs_attributions.max()\n",
    "    else:\n",
    "        normalized_attrs = abs_attributions\n",
    "    \n",
    "    # Plot tokens with color intensity\n",
    "    colors = plt.cm.Reds(normalized_attrs)\n",
    "    x_positions = []\n",
    "    \n",
    "    for i, (token, attr, color) in enumerate(zip(tokens, normalized_attrs, colors)):\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]'] or attention_mask[0][i].item() == 0:\n",
    "            continue\n",
    "        \n",
    "        clean_token = token.replace('##', '')\n",
    "        if not clean_token.strip():\n",
    "            continue\n",
    "        \n",
    "        x_pos = len(x_positions) * 1.2\n",
    "        x_positions.append(x_pos)\n",
    "        \n",
    "        bbox_props = dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.8)\n",
    "        ax.text(x_pos, 0.5, clean_token, fontsize=11, ha='center', va='center',\n",
    "                bbox=bbox_props, weight='bold' if attr > 0.5 else 'normal')\n",
    "    \n",
    "    # Format plot\n",
    "    if x_positions:\n",
    "        ax.set_xlim(-0.5, max(x_positions) + 0.5)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    pred_label = label_encoder.inverse_transform([target_class])[0]\n",
    "    ax.set_title(f'GradCAM Attribution for Class: {pred_label}\\n(Darker Red = Higher Attribution)', \n",
    "                fontsize=14, pad=30, weight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.Reds, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, orientation='horizontal', shrink=0.6, pad=0.15)\n",
    "    cbar.set_label('Attribution Intensity', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ GradCAM implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f63bc9",
   "metadata": {},
   "source": [
    "## 5. üéõÔ∏è Interactive Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cb7c462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dashboard class defined\n"
     ]
    }
   ],
   "source": [
    "class ExplainabilityDashboard:\n",
    "    \"\"\"Interactive dashboard for model explainability analysis\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.setup_data()\n",
    "        self.create_interface()\n",
    "    \n",
    "    def setup_data(self):\n",
    "        \"\"\"Setup data for mistake analysis\"\"\"\n",
    "        predictions_encoded = predict_class(test_texts)\n",
    "        self.incorrect_indices = np.where(predictions_encoded != true_labels_encoded)[0]\n",
    "        print(f\"üìä Found {len(self.incorrect_indices)} mistakes out of {len(test_texts)} samples\")\n",
    "    \n",
    "    def create_interface(self):\n",
    "        \"\"\"Create the dashboard interface\"\"\"\n",
    "        # Input mode selector\n",
    "        self.input_mode = widgets.ToggleButtons(\n",
    "            options=[('Analyze Mistakes', 'mistakes'), ('Custom Text', 'custom')],\n",
    "            value='mistakes',\n",
    "            description='Analysis Mode:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Mistake selector\n",
    "        mistake_options = [(f\"Mistake {i+1}: {test_texts[idx][:50]}...\", i) \n",
    "                          for i, idx in enumerate(self.incorrect_indices[:20])]  # Limit for performance\n",
    "        self.mistake_selector = widgets.Dropdown(\n",
    "            options=mistake_options,\n",
    "            description='Select Mistake:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Custom text input\n",
    "        self.text_input = widgets.Textarea(\n",
    "            placeholder='Enter financial text to analyze...',\n",
    "            description='Text:',\n",
    "            layout=widgets.Layout(width='100%', height='80px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Control buttons\n",
    "        self.analyze_button = widgets.Button(\n",
    "            description='üöÄ Analyze',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        \n",
    "        self.clear_button = widgets.Button(\n",
    "            description='üßπ Clear',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        \n",
    "        # Output tabs\n",
    "        self.output_tabs = widgets.Tab()\n",
    "        self.method_outputs = {\n",
    "            'SHAP': widgets.Output(),\n",
    "            'LIME': widgets.Output(),\n",
    "            'Attention': widgets.Output(),\n",
    "            'GradCAM': widgets.Output()\n",
    "        }\n",
    "        \n",
    "        self.output_tabs.children = list(self.method_outputs.values())\n",
    "        for i, method in enumerate(self.method_outputs.keys()):\n",
    "            self.output_tabs.set_title(i, f'{method}')\n",
    "        \n",
    "        # Status output\n",
    "        self.status_output = widgets.Output()\n",
    "        \n",
    "        # Event handlers\n",
    "        self.input_mode.observe(self.on_mode_change, names='value')\n",
    "        self.analyze_button.on_click(self.on_analyze)\n",
    "        self.clear_button.on_click(self.on_clear)\n",
    "    \n",
    "    def on_mode_change(self, change):\n",
    "        \"\"\"Handle input mode change\"\"\"\n",
    "        # Update the input container dynamically\n",
    "        if hasattr(self, 'input_container'):\n",
    "            if change['new'] == 'mistakes':\n",
    "                self.input_container.children = [self.input_mode, self.mistake_selector]\n",
    "            else:\n",
    "                self.input_container.children = [self.input_mode, self.text_input]\n",
    "    \n",
    "    def update_interface(self):\n",
    "        \"\"\"Update interface based on mode\"\"\"\n",
    "        # This method is called by on_mode_change\n",
    "        pass\n",
    "    \n",
    "    def on_analyze(self, button):\n",
    "        \"\"\"Handle analyze button click\"\"\"\n",
    "        try:\n",
    "            # Get text and prediction info\n",
    "            if self.input_mode.value == 'mistakes':\n",
    "                mistake_idx = self.mistake_selector.value\n",
    "                sample_idx = self.incorrect_indices[mistake_idx]\n",
    "                text = test_texts[sample_idx]\n",
    "                true_label = label_encoder.inverse_transform([true_labels_encoded[sample_idx]])[0]\n",
    "                pred_class = int(predict_class(text)[0])  # Ensure Python int\n",
    "                pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "            else:\n",
    "                text = self.text_input.value.strip()\n",
    "                if not text:\n",
    "                    with self.status_output:\n",
    "                        clear_output(wait=True)\n",
    "                        print(\"‚ùå Please enter some text to analyze!\")\n",
    "                    return\n",
    "                pred_class = int(predict_class(text)[0])  # Ensure Python int\n",
    "                pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "                true_label = \"Unknown\"\n",
    "            \n",
    "            # Generate explanations\n",
    "            self.generate_explanations(text, pred_label, true_label, pred_class)\n",
    "            \n",
    "        except Exception as e:\n",
    "            with self.status_output:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"‚ùå Error during analysis: {str(e)}\")\n",
    "    \n",
    "    def generate_explanations(self, text, pred_label, true_label, pred_class):\n",
    "        \"\"\"Generate all explanations for the text\"\"\"\n",
    "        # Clear outputs\n",
    "        for output in self.method_outputs.values():\n",
    "            with output:\n",
    "                clear_output()\n",
    "        \n",
    "        # Create header\n",
    "        header_html = f\"\"\"\n",
    "        <div style='background: #f8f9fa; padding: 15px; margin: 10px 0; border-radius: 8px; \n",
    "                    border-left: 4px solid #007bff; box-shadow: 0 2px 8px rgba(0,0,0,0.1);'>\n",
    "            <h4 style='margin: 0 0 10px 0; color: #007bff;'>üìù Analysis Summary</h4>\n",
    "            <p style='margin: 5px 0;'><strong>Text:</strong> <em>\"{text}\"</em></p>\n",
    "            <p style='margin: 5px 0;'><strong>Model Prediction:</strong> \n",
    "               <span style='color: #28a745; font-weight: bold;'>{pred_label}</span></p>\n",
    "            {f'<p style=\"margin: 5px 0;\"><strong>True Label:</strong> <span style=\"color: #dc3545; font-weight: bold;\">{true_label}</span></p>' if true_label != \"Unknown\" else ''}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"üß† Generating explanations...\")\n",
    "        \n",
    "        # SHAP\n",
    "        with self.method_outputs['SHAP']:\n",
    "            display(HTML(header_html))\n",
    "            try:\n",
    "                explain_with_shap(text, pred_class)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå SHAP failed: {str(e)}\")\n",
    "        \n",
    "        # LIME\n",
    "        with self.method_outputs['LIME']:\n",
    "            display(HTML(header_html))\n",
    "            try:\n",
    "                explain_with_lime(text)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå LIME failed: {str(e)}\")\n",
    "                print(\"üí° Common LIME issues:\")\n",
    "                print(\"   - Text preprocessing differences\")\n",
    "                print(\"   - Prediction function format mismatch\")\n",
    "                print(\"   - Try using SHAP instead\")\n",
    "        \n",
    "        # Attention\n",
    "        with self.method_outputs['Attention']:\n",
    "            display(HTML(header_html))\n",
    "            try:\n",
    "                # BertViz doesn't work well in widget contexts, so use custom visualization\n",
    "                print(\"‚è≥ Generating attention visualization...\")\n",
    "                print(\"üí° Using custom heatmap (BertViz widgets don't render in dashboard)\")\n",
    "                \n",
    "                # Get attention data\n",
    "                inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "                with torch.no_grad():\n",
    "                    # Force eager attention for compatibility\n",
    "                    original_impl = getattr(pt_model.config, '_attn_implementation', None)\n",
    "                    pt_model.config._attn_implementation = 'eager'\n",
    "                    \n",
    "                    outputs = pt_model(**inputs, output_attentions=True)\n",
    "                    attentions = outputs.attentions\n",
    "                    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "                    \n",
    "                    # Restore original implementation\n",
    "                    if original_impl is not None:\n",
    "                        pt_model.config._attn_implementation = original_impl\n",
    "                \n",
    "                if attentions is not None and len(attentions) > 0:\n",
    "                    pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "                    _visualize_attention_heatmap(attentions, tokens, pred_label)\n",
    "                else:\n",
    "                    print(\"‚ùå No attention weights available\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Attention failed: {str(e)}\")\n",
    "                print(\"üí° Common Attention issues:\")\n",
    "                print(\"   - BertViz compatibility with model architecture\")\n",
    "                print(\"   - JavaScript widget display problems\")\n",
    "                print(\"   - Try refreshing the notebook kernel\")\n",
    "        \n",
    "        # GradCAM\n",
    "        with self.method_outputs['GradCAM']:\n",
    "            display(HTML(header_html))\n",
    "            try:\n",
    "                explain_with_gradcam(text, pred_class)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå GradCAM failed: {str(e)}\")\n",
    "                print(\"üí° Common GradCAM issues:\")\n",
    "                print(\"   - Model architecture compatibility\")\n",
    "                print(\"   - Captum version mismatch\")\n",
    "                print(\"   - GPU/CPU tensor issues\")\n",
    "        \n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"‚úÖ Analysis complete! Explore the tabs above.\")\n",
    "    \n",
    "    def on_clear(self, button):\n",
    "        \"\"\"Clear all outputs\"\"\"\n",
    "        for output in self.method_outputs.values():\n",
    "            with output:\n",
    "                clear_output()\n",
    "        \n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"üßπ All results cleared! Ready for new analysis.\")\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the dashboard\"\"\"\n",
    "        # Title\n",
    "        title = widgets.HTML(\n",
    "            value=\"\"\"\n",
    "            <div style='text-align: center; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "                        color: white; padding: 20px; border-radius: 10px; margin-bottom: 20px;'>\n",
    "                <h2 style='margin: 0; font-size: 24px;'>üß† Financial Sentiment Explainability Dashboard</h2>\n",
    "                <p style='margin: 10px 0 0 0; opacity: 0.9;'>Comprehensive AI model explanation and analysis</p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        # Dynamic input section that updates based on mode\n",
    "        self.input_container = widgets.VBox([\n",
    "            self.input_mode,\n",
    "            self.mistake_selector if self.input_mode.value == 'mistakes' else self.text_input\n",
    "        ])\n",
    "        \n",
    "        # Controls\n",
    "        controls = widgets.HBox([\n",
    "            self.analyze_button,\n",
    "            self.clear_button\n",
    "        ], layout=widgets.Layout(justify_content='space-between', width='250px'))\n",
    "        \n",
    "        # Main dashboard\n",
    "        dashboard = widgets.VBox([\n",
    "            title,\n",
    "            self.input_container,\n",
    "            controls,\n",
    "            self.status_output,\n",
    "            self.output_tabs\n",
    "        ])\n",
    "        \n",
    "        return dashboard\n",
    "\n",
    "print(\"‚úÖ Dashboard class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cadb6f",
   "metadata": {},
   "source": [
    "## 6. üöÄ Launch Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acd84cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Initializing Explainability Dashboard...\n",
      "üìä Found 253 mistakes out of 1212 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7478edcb8e8a49f99493bd40407da891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"\\n            <div style='text-align: center; background: linear-gradient(135deg, #‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dashboard is ready! Use the interface above to analyze model predictions.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and display dashboard\n",
    "print(\"üéØ Initializing Explainability Dashboard...\")\n",
    "dashboard = ExplainabilityDashboard()\n",
    "display(dashboard.display())\n",
    "print(\"‚úÖ Dashboard is ready! Use the interface above to analyze model predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf7870f",
   "metadata": {},
   "source": [
    "## 7. \udd0d Quick Misclassification Analysis\n",
    "\n",
    "Simple analysis to identify patterns for fine-tuning in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d64ad",
   "metadata": {},
   "source": [
    "### üìã Enhanced Analysis Methodology\n",
    "\n",
    "Our comprehensive fine-tuning and pruning analysis follows a systematic 5-step approach designed to provide actionable insights for model optimization:\n",
    "\n",
    "**üîç Step 1: Basic Performance Analysis**\n",
    "- Generate predictions and confidence scores for the entire test set\n",
    "- Calculate accuracy, error rates, and confidence distributions\n",
    "- Identify misclassified samples and low-confidence predictions\n",
    "- Establish baseline metrics for optimization tracking\n",
    "\n",
    "**üìä Step 2: Per-Class Performance Analysis** \n",
    "- Generate detailed confusion matrix and class-wise metrics\n",
    "- Calculate precision, recall, and F1-scores for each sentiment class\n",
    "- Identify most problematic classes requiring targeted fine-tuning\n",
    "- Analyze error patterns between specific class pairs\n",
    "\n",
    "**üéØ Step 3: Confidence Distribution Analysis**\n",
    "- Analyze prediction confidence across different thresholds\n",
    "- Calculate coverage and accuracy at various confidence levels\n",
    "- Identify low-confidence samples for fine-tuning focus\n",
    "- Assess entropy distribution for pruning strategy recommendations\n",
    "\n",
    "**üìù Step 4: Enhanced Linguistic Pattern Analysis**\n",
    "- Use advanced TF-IDF with trigrams for comprehensive vocabulary analysis\n",
    "- Identify both problematic keywords (higher in errors) and protective keywords (higher in correct predictions)\n",
    "- Analyze n-gram patterns that correlate with model failures\n",
    "- Generate targeted keywords for data augmentation strategies\n",
    "\n",
    "**üíæ Step 5: Fine-Tuning & Pruning Recommendations**\n",
    "- Generate specific learning rate recommendations based on current performance\n",
    "- Identify high-priority samples for hard negative mining\n",
    "- Provide confidence-based pruning strategies with expected performance impact\n",
    "- Suggest targeted data augmentation approaches for problematic patterns\n",
    "- Compile actionable recommendations for immediate implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9371f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Comprehensive Fine-Tuning & Pruning Analysis\n",
      "============================================================\n",
      "üîç Generating comprehensive performance analysis...\n",
      "\n",
      "üìä STEP 1: Basic Performance Analysis\n",
      "   ‚Ä¢ Overall Accuracy: 0.7913\n",
      "   ‚Ä¢ Error Rate: 0.2087\n",
      "   ‚Ä¢ Total Misclassifications: 253\n",
      "   ‚Ä¢ Average Confidence: 0.7314\n",
      "\n",
      "üìà STEP 2: Per-Class Performance Analysis\n",
      "   Per-Class Metrics:\n",
      "     ‚Ä¢ negative: P=0.653, R=0.848, F1=0.738\n",
      "     ‚Ä¢ neutral: P=0.862, R=0.831, F1=0.846\n",
      "     ‚Ä¢ positive: P=0.724, R=0.683, F1=0.703\n",
      "\n",
      "   üéØ Most Problematic Classes (lowest F1): ['positive', 'negative']\n",
      "\n",
      "üéØ STEP 3: Confidence Distribution Analysis\n",
      "   Confidence Distribution:\n",
      "     ‚Ä¢ ‚â•0.5: 95.6% samples, 0.803 accuracy\n",
      "     ‚Ä¢ ‚â•0.7: 64.5% samples, 0.886 accuracy\n",
      "     ‚Ä¢ ‚â•0.8: 39.4% samples, 0.929 accuracy\n",
      "     ‚Ä¢ ‚â•0.9: 0.0% samples, 0.000 accuracy\n",
      "     ‚Ä¢ ‚â•0.95: 0.0% samples, 0.000 accuracy\n",
      "\n",
      "   üìâ Low Confidence Samples (<0.6): 195 (16.1%)\n",
      "\n",
      "üìù STEP 4: Linguistic Pattern Analysis\n",
      "   üö® Top Problematic Keywords (higher in errors):\n",
      "     ‚Ä¢ 'pct': +0.0129\n",
      "     ‚Ä¢ 'solutions': +0.0124\n",
      "     ‚Ä¢ 'compared': +0.0110\n",
      "     ‚Ä¢ 'new': +0.0100\n",
      "     ‚Ä¢ 'increase': +0.0097\n",
      "     ‚Ä¢ 'line': +0.0097\n",
      "     ‚Ä¢ 'higher': +0.0096\n",
      "     ‚Ä¢ 'mln': +0.0096\n",
      "\n",
      "   ‚úÖ Top Protective Keywords (higher in correct):\n",
      "     ‚Ä¢ 'mn': -0.0127\n",
      "     ‚Ä¢ '2007': -0.0123\n",
      "     ‚Ä¢ 'million': -0.0107\n",
      "     ‚Ä¢ 'expected': -0.0102\n",
      "     ‚Ä¢ 'eur0': -0.0102\n",
      "\n",
      "üéØ STEP 5: Fine-Tuning & Pruning Recommendations\n",
      "   üìö Fine-Tuning Recommendations:\n",
      "     ‚Ä¢ Focus on classes: ['positive', 'negative']\n",
      "     ‚Ä¢ Learning rate: 5e-5 to 1e-4 (moderate fine-tuning)\n",
      "     ‚Ä¢ Priority samples: 195 low-confidence + 253 errors\n",
      "\n",
      "   ‚úÇÔ∏è Pruning Recommendations:\n",
      "     ‚Ä¢ Strategy: Conservative pruning (10-20%) - low confidence samples\n",
      "     ‚Ä¢ Confidence threshold: 0.9 (covers 0.0% with 0.000 accuracy)\n",
      "\n",
      "   üîÑ Data Augmentation Recommendations:\n",
      "     ‚Ä¢ Target problematic keywords: ['pct', 'solutions', 'compared']\n",
      "     ‚Ä¢ Focus on: ['positive', 'negative'] classes\n",
      "\n",
      "üíæ ANALYSIS COMPLETE!\n",
      "üìÅ Results saved to:\n",
      "   ‚Ä¢ analysis_results/comprehensive_analysis.json (detailed metrics)\n",
      "   ‚Ä¢ analysis_results/analysis_summary.txt (human-readable)\n",
      "\n",
      "üéØ Ready for Notebook 6: Fine-tuning with targeted improvements!\n",
      "‚è±Ô∏è  Analysis completed in 15.0 seconds\n",
      "   ‚Ä¢ Overall Accuracy: 0.7913\n",
      "   ‚Ä¢ Error Rate: 0.2087\n",
      "   ‚Ä¢ Total Misclassifications: 253\n",
      "   ‚Ä¢ Average Confidence: 0.7314\n",
      "\n",
      "üìà STEP 2: Per-Class Performance Analysis\n",
      "   Per-Class Metrics:\n",
      "     ‚Ä¢ negative: P=0.653, R=0.848, F1=0.738\n",
      "     ‚Ä¢ neutral: P=0.862, R=0.831, F1=0.846\n",
      "     ‚Ä¢ positive: P=0.724, R=0.683, F1=0.703\n",
      "\n",
      "   üéØ Most Problematic Classes (lowest F1): ['positive', 'negative']\n",
      "\n",
      "üéØ STEP 3: Confidence Distribution Analysis\n",
      "   Confidence Distribution:\n",
      "     ‚Ä¢ ‚â•0.5: 95.6% samples, 0.803 accuracy\n",
      "     ‚Ä¢ ‚â•0.7: 64.5% samples, 0.886 accuracy\n",
      "     ‚Ä¢ ‚â•0.8: 39.4% samples, 0.929 accuracy\n",
      "     ‚Ä¢ ‚â•0.9: 0.0% samples, 0.000 accuracy\n",
      "     ‚Ä¢ ‚â•0.95: 0.0% samples, 0.000 accuracy\n",
      "\n",
      "   üìâ Low Confidence Samples (<0.6): 195 (16.1%)\n",
      "\n",
      "üìù STEP 4: Linguistic Pattern Analysis\n",
      "   üö® Top Problematic Keywords (higher in errors):\n",
      "     ‚Ä¢ 'pct': +0.0129\n",
      "     ‚Ä¢ 'solutions': +0.0124\n",
      "     ‚Ä¢ 'compared': +0.0110\n",
      "     ‚Ä¢ 'new': +0.0100\n",
      "     ‚Ä¢ 'increase': +0.0097\n",
      "     ‚Ä¢ 'line': +0.0097\n",
      "     ‚Ä¢ 'higher': +0.0096\n",
      "     ‚Ä¢ 'mln': +0.0096\n",
      "\n",
      "   ‚úÖ Top Protective Keywords (higher in correct):\n",
      "     ‚Ä¢ 'mn': -0.0127\n",
      "     ‚Ä¢ '2007': -0.0123\n",
      "     ‚Ä¢ 'million': -0.0107\n",
      "     ‚Ä¢ 'expected': -0.0102\n",
      "     ‚Ä¢ 'eur0': -0.0102\n",
      "\n",
      "üéØ STEP 5: Fine-Tuning & Pruning Recommendations\n",
      "   üìö Fine-Tuning Recommendations:\n",
      "     ‚Ä¢ Focus on classes: ['positive', 'negative']\n",
      "     ‚Ä¢ Learning rate: 5e-5 to 1e-4 (moderate fine-tuning)\n",
      "     ‚Ä¢ Priority samples: 195 low-confidence + 253 errors\n",
      "\n",
      "   ‚úÇÔ∏è Pruning Recommendations:\n",
      "     ‚Ä¢ Strategy: Conservative pruning (10-20%) - low confidence samples\n",
      "     ‚Ä¢ Confidence threshold: 0.9 (covers 0.0% with 0.000 accuracy)\n",
      "\n",
      "   üîÑ Data Augmentation Recommendations:\n",
      "     ‚Ä¢ Target problematic keywords: ['pct', 'solutions', 'compared']\n",
      "     ‚Ä¢ Focus on: ['positive', 'negative'] classes\n",
      "\n",
      "üíæ ANALYSIS COMPLETE!\n",
      "üìÅ Results saved to:\n",
      "   ‚Ä¢ analysis_results/comprehensive_analysis.json (detailed metrics)\n",
      "   ‚Ä¢ analysis_results/analysis_summary.txt (human-readable)\n",
      "\n",
      "üéØ Ready for Notebook 6: Fine-tuning with targeted improvements!\n",
      "‚è±Ô∏è  Analysis completed in 15.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Universal Model Analysis (PyTorch + ONNX Compatible)\n",
    "import onnxruntime as ort\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class UniversalModelAnalyzer:\n",
    "    \"\"\"Universal analyzer that works with both PyTorch and ONNX models\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, model_type='auto'):\n",
    "        \"\"\"\n",
    "        Initialize analyzer for different model types\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to model directory\n",
    "            model_type: 'pytorch', 'onnx', or 'auto' (auto-detect)\n",
    "        \"\"\"\n",
    "        self.model_path = Path(model_path)\n",
    "        self.model_name = self.model_path.name\n",
    "        \n",
    "        # Auto-detect model type if not specified\n",
    "        if model_type == 'auto':\n",
    "            onnx_path = self.model_path / 'onnx' / 'model.onnx'\n",
    "            if onnx_path.exists():\n",
    "                self.model_type = 'onnx'\n",
    "            else:\n",
    "                self.model_type = 'pytorch'\n",
    "        else:\n",
    "            self.model_type = model_type\n",
    "        \n",
    "        # Load tokenizer (same for both types)\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "        \n",
    "        # Load label encoder\n",
    "        with open(self.model_path / 'label_encoder.pkl', 'rb') as f:\n",
    "            self.label_encoder = pickle.load(f)\n",
    "        \n",
    "        # Load appropriate model\n",
    "        if self.model_type == 'onnx':\n",
    "            onnx_path = self.model_path / 'onnx' / 'model.onnx'\n",
    "            self.session = ort.InferenceSession(str(onnx_path))\n",
    "            print(f\"‚úÖ Loaded ONNX model: {self.model_name}\")\n",
    "        else:\n",
    "            self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "            self.model.eval()\n",
    "            print(f\"‚úÖ Loaded PyTorch model: {self.model_name}\")\n",
    "    \n",
    "    def predict_batch(self, texts, batch_size=32):\n",
    "        \"\"\"Predict classes and probabilities for batch of texts\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            if self.model_type == 'onnx':\n",
    "                batch_preds, batch_probs = self._predict_onnx_batch(batch_texts)\n",
    "            else:\n",
    "                batch_preds, batch_probs = self._predict_pytorch_batch(batch_texts)\n",
    "            \n",
    "            predictions.extend(batch_preds)\n",
    "            probabilities.extend(batch_probs)\n",
    "        \n",
    "        return np.array(predictions), np.array(probabilities)\n",
    "    \n",
    "    def _predict_onnx_batch(self, texts):\n",
    "        \"\"\"ONNX batch prediction\"\"\"\n",
    "        # Tokenize batch\n",
    "        encodings = self.tokenizer(\n",
    "            texts, \n",
    "            return_tensors='np',\n",
    "            max_length=512, \n",
    "            truncation=True, \n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Run ONNX inference\n",
    "        inputs = {\n",
    "            'input_ids': encodings['input_ids'].astype(np.int64),\n",
    "            'attention_mask': encodings['attention_mask'].astype(np.int64)\n",
    "        }\n",
    "        \n",
    "        outputs = self.session.run(None, inputs)\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        # Convert to predictions and probabilities\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        probabilities = self._softmax(logits)\n",
    "        \n",
    "        return predictions.tolist(), probabilities.tolist()\n",
    "    \n",
    "    def _predict_pytorch_batch(self, texts):\n",
    "        \"\"\"PyTorch batch prediction\"\"\"\n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            return_tensors='pt',\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encodings)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        return predictions.cpu().numpy().tolist(), probabilities.cpu().numpy().tolist()\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        \"\"\"Numpy softmax implementation for ONNX\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Initialize the universal analyzer for current model\n",
    "print(\"üîÑ Initializing Universal Model Analyzer...\")\n",
    "analyzer = UniversalModelAnalyzer(MODEL_DIR, 'auto')  # Auto-detect model type\n",
    "\n",
    "def analyze_model_performance():\n",
    "    \"\"\"Comprehensive model performance analysis using universal analyzer\"\"\"\n",
    "    \n",
    "    print(\"üîç Generating comprehensive performance analysis...\")\n",
    "    \n",
    "    # 1. Basic Performance Metrics\n",
    "    print(f\"\\nüìä STEP 1: Basic Performance Analysis ({analyzer.model_type})\")\n",
    "    predictions, probabilities = analyzer.predict_batch(test_texts)\n",
    "    max_probs = np.max(probabilities, axis=1)\n",
    "    \n",
    "    # Misclassification analysis\n",
    "    misclassified_mask = predictions != true_labels_encoded\n",
    "    misclassified_indices = np.where(misclassified_mask)[0]\n",
    "    misclassified_texts = [test_texts[i] for i in misclassified_indices]\n",
    "    \n",
    "    accuracy = np.mean(predictions == true_labels_encoded)\n",
    "    error_rate = 1 - accuracy\n",
    "    \n",
    "    print(f\"   ‚Ä¢ Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Error Rate: {error_rate:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Total Misclassifications: {len(misclassified_texts)}\")\n",
    "    print(f\"   ‚Ä¢ Average Confidence: {np.mean(max_probs):.4f}\")\n",
    "    print(f\"   ‚Ä¢ Model Type: {analyzer.model_type.upper()}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'probabilities': probabilities,\n",
    "        'misclassified_mask': misclassified_mask,\n",
    "        'misclassified_indices': misclassified_indices,\n",
    "        'misclassified_texts': misclassified_texts,\n",
    "        'accuracy': accuracy,\n",
    "        'error_rate': error_rate,\n",
    "        'avg_confidence': np.mean(max_probs),\n",
    "        'model_type': analyzer.model_type\n",
    "    }\n",
    "\n",
    "def analyze_class_performance(performance_data):\n",
    "    \"\"\"Detailed per-class performance metrics\"\"\"\n",
    "    \n",
    "    print(\"\\nüìà STEP 2: Per-Class Performance Analysis\")\n",
    "    \n",
    "    # Get class-wise metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        true_labels_encoded, performance_data['predictions'], average=None\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(true_labels_encoded, performance_data['predictions'])\n",
    "    \n",
    "    class_metrics = {}\n",
    "    print(\"   Per-Class Metrics:\")\n",
    "    for i, class_name in enumerate(analyzer.label_encoder.classes_):\n",
    "        class_metrics[class_name] = {\n",
    "            'precision': float(precision[i]),\n",
    "            'recall': float(recall[i]),\n",
    "            'f1_score': float(f1[i]),\n",
    "            'support': int(support[i]),\n",
    "            'errors': int(np.sum(cm[i]) - cm[i][i])  # Total errors for this class\n",
    "        }\n",
    "        print(f\"     ‚Ä¢ {class_name}: P={precision[i]:.3f}, R={recall[i]:.3f}, F1={f1[i]:.3f}\")\n",
    "    \n",
    "    # Identify most problematic classes (for targeted fine-tuning)\n",
    "    problematic_classes = sorted(class_metrics.items(), key=lambda x: x[1]['f1_score'])[:2]\n",
    "    print(f\"\\n   üéØ Most Problematic Classes (lowest F1): {[c[0] for c in problematic_classes]}\")\n",
    "    \n",
    "    return {\n",
    "        'class_metrics': class_metrics,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'problematic_classes': [c[0] for c in problematic_classes]\n",
    "    }\n",
    "\n",
    "def analyze_confidence_distribution(performance_data):\n",
    "    \"\"\"Analyze prediction confidence for pruning insights\"\"\"\n",
    "    \n",
    "    print(\"\\nüéØ STEP 3: Confidence Distribution Analysis\")\n",
    "    \n",
    "    probabilities = performance_data['probabilities']\n",
    "    max_probs = np.max(probabilities, axis=1)\n",
    "    entropy = -np.sum(probabilities * np.log(probabilities + 1e-10), axis=1)\n",
    "    \n",
    "    # Confidence thresholds for different scenarios\n",
    "    confidence_thresholds = [0.5, 0.7, 0.8, 0.9, 0.95]\n",
    "    confidence_analysis = {}\n",
    "    \n",
    "    print(\"   Confidence Distribution:\")\n",
    "    for threshold in confidence_thresholds:\n",
    "        high_conf_mask = max_probs >= threshold\n",
    "        high_conf_accuracy = np.mean(performance_data['predictions'][high_conf_mask] == true_labels_encoded[high_conf_mask]) if np.any(high_conf_mask) else 0\n",
    "        coverage = np.mean(high_conf_mask)\n",
    "        \n",
    "        confidence_analysis[f\"threshold_{threshold}\"] = {\n",
    "            'accuracy': float(high_conf_accuracy),\n",
    "            'coverage': float(coverage),\n",
    "            'sample_count': int(np.sum(high_conf_mask))\n",
    "        }\n",
    "        print(f\"     ‚Ä¢ ‚â•{threshold}: {coverage:.1%} samples, {high_conf_accuracy:.3f} accuracy\")\n",
    "    \n",
    "    # Low confidence samples (candidates for fine-tuning focus)\n",
    "    low_conf_threshold = 0.6\n",
    "    low_conf_mask = max_probs < low_conf_threshold\n",
    "    low_conf_indices = np.where(low_conf_mask)[0]\n",
    "    \n",
    "    print(f\"\\n   üìâ Low Confidence Samples (<{low_conf_threshold}): {len(low_conf_indices)} ({len(low_conf_indices)/len(test_texts):.1%})\")\n",
    "    \n",
    "    return {\n",
    "        'confidence_analysis': confidence_analysis,\n",
    "        'entropy_stats': {\n",
    "            'mean': float(np.mean(entropy)),\n",
    "            'std': float(np.std(entropy)),\n",
    "            'high_entropy_samples': int(np.sum(entropy > np.percentile(entropy, 90)))\n",
    "        },\n",
    "        'low_confidence_indices': low_conf_indices.tolist(),\n",
    "        'low_confidence_threshold': low_conf_threshold\n",
    "    }\n",
    "\n",
    "def analyze_linguistic_patterns(performance_data):\n",
    "    \"\"\"Enhanced keyword analysis for understanding model failures\"\"\"\n",
    "    \n",
    "    print(\"\\nüìù STEP 4: Linguistic Pattern Analysis\")\n",
    "    \n",
    "    misclassified_texts = performance_data['misclassified_texts']\n",
    "    correctly_classified_texts = [test_texts[i] for i in range(len(test_texts)) if not performance_data['misclassified_mask'][i]]\n",
    "    \n",
    "    if len(misclassified_texts) == 0:\n",
    "        print(\"   ‚úÖ No misclassified texts found!\")\n",
    "        return {\n",
    "            'problematic_keywords': [],\n",
    "            'protective_keywords': [],\n",
    "            'vocabulary_size': 0,\n",
    "            'tfidf_params': {}\n",
    "        }\n",
    "    \n",
    "    # Enhanced TF-IDF analysis\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=500, \n",
    "        stop_words='english', \n",
    "        ngram_range=(1, 3),  # Include trigrams for better context\n",
    "        min_df=2,\n",
    "        max_df=0.8\n",
    "    )\n",
    "    \n",
    "    # Balanced sampling for fair comparison\n",
    "    sample_size = min(len(misclassified_texts), len(correctly_classified_texts))\n",
    "    balanced_correct = correctly_classified_texts[:sample_size]\n",
    "    \n",
    "    all_texts = misclassified_texts + balanced_correct\n",
    "    vectorizer.fit(all_texts)\n",
    "    \n",
    "    # TF-IDF difference analysis\n",
    "    misc_tfidf = vectorizer.transform(misclassified_texts).mean(axis=0).A1\n",
    "    correct_tfidf = vectorizer.transform(balanced_correct).mean(axis=0).A1\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    score_diff = misc_tfidf - correct_tfidf\n",
    "    \n",
    "    # Top problematic and protective features\n",
    "    problematic_indices = score_diff.argsort()[-15:][::-1]  # Top 15 problematic\n",
    "    protective_indices = score_diff.argsort()[:10]  # Top 10 protective\n",
    "    \n",
    "    problematic_keywords = [(feature_names[i], float(score_diff[i])) for i in problematic_indices if score_diff[i] > 0.001]\n",
    "    protective_keywords = [(feature_names[i], float(abs(score_diff[i]))) for i in protective_indices if score_diff[i] < -0.001]\n",
    "    \n",
    "    print(f\"   üö® Top Problematic Keywords (higher in errors):\")\n",
    "    for keyword, score in problematic_keywords[:8]:\n",
    "        print(f\"     ‚Ä¢ '{keyword}': +{score:.4f}\")\n",
    "    \n",
    "    print(f\"\\n   ‚úÖ Top Protective Keywords (higher in correct):\")\n",
    "    for keyword, score in protective_keywords[:5]:\n",
    "        print(f\"     ‚Ä¢ '{keyword}': -{score:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'problematic_keywords': problematic_keywords,\n",
    "        'protective_keywords': protective_keywords,\n",
    "        'vocabulary_size': len(feature_names),\n",
    "        'tfidf_params': {\n",
    "            'max_features': 500,\n",
    "            'ngram_range': [1, 3],\n",
    "            'min_df': 2,\n",
    "            'max_df': 0.8\n",
    "        }\n",
    "    }\n",
    "\n",
    "def generate_fine_tuning_recommendations(performance_data, class_analysis, confidence_analysis, linguistic_analysis):\n",
    "    \"\"\"Generate specific recommendations for fine-tuning and pruning\"\"\"\n",
    "    \n",
    "    print(\"\\nüéØ STEP 5: Fine-Tuning & Pruning Recommendations\")\n",
    "    \n",
    "    recommendations = {\n",
    "        'fine_tuning': {},\n",
    "        'pruning': {},\n",
    "        'data_augmentation': {},\n",
    "        'architecture': {}\n",
    "    }\n",
    "    \n",
    "    # Fine-tuning recommendations\n",
    "    print(\"   üìö Fine-Tuning Recommendations:\")\n",
    "    \n",
    "    # Target problematic classes\n",
    "    problematic_classes = class_analysis['problematic_classes']\n",
    "    recommendations['fine_tuning']['target_classes'] = problematic_classes\n",
    "    print(f\"     ‚Ä¢ Focus on classes: {problematic_classes}\")\n",
    "    \n",
    "    # Learning rate suggestions based on performance\n",
    "    if performance_data['accuracy'] > 0.8:\n",
    "        lr_suggestion = \"1e-5 to 5e-5 (conservative fine-tuning)\"\n",
    "    elif performance_data['accuracy'] > 0.7:\n",
    "        lr_suggestion = \"5e-5 to 1e-4 (moderate fine-tuning)\"\n",
    "    else:\n",
    "        lr_suggestion = \"1e-4 to 5e-4 (aggressive fine-tuning)\"\n",
    "    \n",
    "    recommendations['fine_tuning']['learning_rate'] = lr_suggestion\n",
    "    recommendations['fine_tuning']['model_type'] = performance_data['model_type']\n",
    "    print(f\"     ‚Ä¢ Learning rate: {lr_suggestion}\")\n",
    "    print(f\"     ‚Ä¢ Model type: {performance_data['model_type'].upper()}\")\n",
    "    \n",
    "    # Sample selection for fine-tuning\n",
    "    low_conf_count = len(confidence_analysis['low_confidence_indices'])\n",
    "    recommendations['fine_tuning']['focus_samples'] = {\n",
    "        'low_confidence_count': low_conf_count,\n",
    "        'misclassified_count': len(performance_data['misclassified_indices']),\n",
    "        'strategy': 'Hard negative mining + low confidence samples'\n",
    "    }\n",
    "    print(f\"     ‚Ä¢ Priority samples: {low_conf_count} low-confidence + {len(performance_data['misclassified_indices'])} errors\")\n",
    "    \n",
    "    # Pruning recommendations\n",
    "    print(\"\\n   ‚úÇÔ∏è Pruning Recommendations:\")\n",
    "    \n",
    "    # Confidence-based pruning strategy\n",
    "    high_conf_90 = confidence_analysis['confidence_analysis']['threshold_0.9']\n",
    "    if high_conf_90['coverage'] > 0.7 and high_conf_90['accuracy'] > 0.95:\n",
    "        pruning_strategy = \"Aggressive pruning (30-50%) - high confidence retained\"\n",
    "    elif high_conf_90['coverage'] > 0.5:\n",
    "        pruning_strategy = \"Moderate pruning (20-30%) - good confidence distribution\"\n",
    "    else:\n",
    "        pruning_strategy = \"Conservative pruning (10-20%) - low confidence samples\"\n",
    "    \n",
    "    recommendations['pruning']['strategy'] = pruning_strategy\n",
    "    recommendations['pruning']['confidence_threshold'] = 0.9\n",
    "    recommendations['pruning']['expected_coverage'] = high_conf_90['coverage']\n",
    "    recommendations['pruning']['model_type'] = performance_data['model_type']\n",
    "    print(f\"     ‚Ä¢ Strategy: {pruning_strategy}\")\n",
    "    print(f\"     ‚Ä¢ Confidence threshold: 0.9 (covers {high_conf_90['coverage']:.1%} with {high_conf_90['accuracy']:.3f} accuracy)\")\n",
    "    \n",
    "    # Data augmentation recommendations\n",
    "    print(\"\\n   üîÑ Data Augmentation Recommendations:\")\n",
    "    top_problematic = linguistic_analysis['problematic_keywords'][:5]\n",
    "    recommendations['data_augmentation']['target_keywords'] = [kw[0] for kw in top_problematic]\n",
    "    recommendations['data_augmentation']['methods'] = [\n",
    "        'Synonym replacement for problematic terms',\n",
    "        'Back-translation for class balance',\n",
    "        'Paraphrasing for robustness'\n",
    "    ]\n",
    "    print(f\"     ‚Ä¢ Target problematic keywords: {[kw[0] for kw in top_problematic[:3]]}\")\n",
    "    print(f\"     ‚Ä¢ Focus on: {problematic_classes} classes\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Execute comprehensive analysis\n",
    "print(\"üöÄ Starting Universal Fine-Tuning & Pruning Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Start timing for the entire analysis\n",
    "analysis_start_time = time.time()\n",
    "\n",
    "# Run analysis pipeline\n",
    "performance_data = analyze_model_performance()\n",
    "class_analysis = analyze_class_performance(performance_data)\n",
    "confidence_analysis = analyze_confidence_distribution(performance_data)\n",
    "linguistic_analysis = analyze_linguistic_patterns(performance_data)\n",
    "recommendations = generate_fine_tuning_recommendations(\n",
    "    performance_data, class_analysis, confidence_analysis, linguistic_analysis\n",
    ")\n",
    "\n",
    "def convert_to_json_serializable(obj):\n",
    "    \"\"\"Convert numpy types to JSON serializable Python types\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_to_json_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_json_serializable(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Compile comprehensive results (convert all numpy types to JSON serializable)\n",
    "comprehensive_results = {\n",
    "    'metadata': {\n",
    "        'analysis_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'dataset_size': len(test_texts),\n",
    "        'model_path': str(MODEL_DIR),\n",
    "        'model_name': analyzer.model_name,\n",
    "        'model_type': performance_data['model_type'],\n",
    "        'analysis_type': 'universal_fine_tuning_pruning'\n",
    "    },\n",
    "    'performance_metrics': {\n",
    "        'overall_accuracy': float(performance_data['accuracy']),\n",
    "        'error_rate': float(performance_data['error_rate']),\n",
    "        'avg_confidence': float(performance_data['avg_confidence']),\n",
    "        'total_misclassifications': len(performance_data['misclassified_texts'])\n",
    "    },\n",
    "    'class_analysis': convert_to_json_serializable(class_analysis),\n",
    "    'confidence_analysis': convert_to_json_serializable(confidence_analysis),\n",
    "    'linguistic_analysis': convert_to_json_serializable(linguistic_analysis),\n",
    "    'recommendations': convert_to_json_serializable(recommendations),\n",
    "    'sample_indices': {\n",
    "        'misclassified': [int(x) for x in performance_data['misclassified_indices']],\n",
    "        'low_confidence': [int(x) for x in confidence_analysis['low_confidence_indices']]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "os.makedirs('analysis_results', exist_ok=True)\n",
    "\n",
    "# Detailed JSON for programmatic use\n",
    "with open('analysis_results/comprehensive_analysis.json', 'w') as f:\n",
    "    json.dump(comprehensive_results, f, indent=2)\n",
    "\n",
    "# Human-readable summary\n",
    "summary_text = f\"\"\"\n",
    "UNIVERSAL FINE-TUNING & PRUNING ANALYSIS SUMMARY\n",
    "===============================================\n",
    "\n",
    "üéØ PERFORMANCE OVERVIEW ({performance_data['model_type'].upper()}):\n",
    "‚Ä¢ Overall Accuracy: {performance_data['accuracy']:.1%}\n",
    "‚Ä¢ Error Rate: {performance_data['error_rate']:.1%}\n",
    "‚Ä¢ Average Confidence: {performance_data['avg_confidence']:.3f}\n",
    "‚Ä¢ Model: {analyzer.model_name}\n",
    "\n",
    "üìä KEY METRICS FOR FINE-TUNING:\n",
    "‚Ä¢ Most Problematic Classes: {class_analysis['problematic_classes']}\n",
    "‚Ä¢ Low Confidence Samples: {len(confidence_analysis['low_confidence_indices'])} ({len(confidence_analysis['low_confidence_indices'])/len(test_texts):.1%})\n",
    "‚Ä¢ High-Priority Samples: {len(performance_data['misclassified_indices'])} errors + {len(confidence_analysis['low_confidence_indices'])} low-conf\n",
    "\n",
    "üîß RECOMMENDED FINE-TUNING STRATEGY:\n",
    "‚Ä¢ Learning Rate: {recommendations['fine_tuning']['learning_rate']}\n",
    "‚Ä¢ Focus Areas: {', '.join(class_analysis['problematic_classes'])}\n",
    "‚Ä¢ Target Keywords: {', '.join([kw[0] for kw in linguistic_analysis['problematic_keywords'][:5]])}\n",
    "\n",
    "‚úÇÔ∏è RECOMMENDED PRUNING STRATEGY:\n",
    "‚Ä¢ {recommendations['pruning']['strategy']}\n",
    "‚Ä¢ Confidence Threshold: {recommendations['pruning']['confidence_threshold']}\n",
    "‚Ä¢ Expected Coverage: {recommendations['pruning']['expected_coverage']:.1%}\n",
    "\n",
    "üìã NEXT STEPS:\n",
    "1. Use misclassified samples for hard negative mining\n",
    "2. Focus fine-tuning on {', '.join(class_analysis['problematic_classes'])} classes\n",
    "3. Apply confidence-based pruning with 0.9 threshold\n",
    "4. Monitor performance on high-entropy samples\n",
    "\"\"\"\n",
    "\n",
    "with open('analysis_results/analysis_summary.txt', 'w') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(f\"\\nüíæ ANALYSIS COMPLETE!\")\n",
    "print(f\"üìÅ Results saved to:\")\n",
    "print(f\"   ‚Ä¢ analysis_results/comprehensive_analysis.json (detailed metrics)\")\n",
    "print(f\"   ‚Ä¢ analysis_results/analysis_summary.txt (human-readable)\")\n",
    "print(f\"\\nüéØ Ready for Notebook 6: Fine-tuning with targeted improvements!\")\n",
    "print(f\"‚è±Ô∏è  Analysis completed in {time.time() - analysis_start_time:.1f} seconds\")\n",
    "print(f\"üîß Model Type Used: {performance_data['model_type'].upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575704e",
   "metadata": {},
   "source": [
    "## 8. üìã Summary\n",
    "\n",
    "### ‚úÖ Completed:\n",
    "- **Interactive Dashboard**: SHAP and LIME explanations for any text\n",
    "- **Mistake Analysis**: Analyze specific model errors  \n",
    "- **Misclassification Patterns**: Key insights for fine-tuning\n",
    "\n",
    "### üìä Key Findings:\n",
    "- Error rate: ~20% on test data\n",
    "- Main confusion patterns identified\n",
    "- Problematic keywords extracted\n",
    "\n",
    "### üîú Next Steps:\n",
    "Results saved to `analysis_results/` for **Notebook 6: Fine-tuning with Pruning Methods**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
