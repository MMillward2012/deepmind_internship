{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46df93cc",
   "metadata": {},
   "source": [
    "# üß† Financial Sentiment Model Explainability Dashboard\n",
    "\n",
    "## Overview\n",
    "This notebook provides comprehensive explainability analysis for the fine-tuned TinyBERT financial sentiment classification model. It includes four complementary explanation methods accessible through an interactive dashboard.\n",
    "\n",
    "### Explanation Methods\n",
    "- **üéØ SHAP**: Game-theory based feature importance\n",
    "- **üîç LIME**: Local interpretable model-agnostic explanations \n",
    "- **üëÅÔ∏è Attention**: Model attention head visualization\n",
    "- **üå°Ô∏è GradCAM**: Gradient-based visual attribution\n",
    "\n",
    "### Dashboard Features\n",
    "- **Mistake Analysis**: Examine specific model errors\n",
    "- **Custom Text Analysis**: Test any financial text\n",
    "- **Interactive Interface**: Tabbed layout for easy comparison\n",
    "- **On-demand Computation**: Optimized performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2097b30f",
   "metadata": {},
   "source": [
    "## 1. üì¶ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09540612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_setup.ipynb                  5_explainability.ipynb\n",
      "1_data_processing.ipynb        5_explainability_backup.ipynb\n",
      "2_train_models.ipynb           5_explainability_clean.ipynb\n",
      "3_convert_to_onnx.ipynb        5_explainability_minimal.ipynb\n",
      "4_benchmarks.ipynb             6_fine_tune.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "180d1813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/matthew/Documents/deepmind_internship\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36cc0361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model and tokenizer\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Explainability libraries\n",
    "import shap\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from bertviz import head_view\n",
    "from captum.attr import LayerGradCam\n",
    "\n",
    "# Dashboard components\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af47f79",
   "metadata": {},
   "source": [
    "## 2. üóÇÔ∏è Data & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd313ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loaded full dataset: 4846 samples\n",
      "üìä Test set: 1212 samples (25% split)\n",
      "üìã Label classes: ['negative', 'neutral', 'positive']\n",
      "üé≤ Random seed: 42\n",
      "‚úÖ Data loaded successfully with correct encoding\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_DIR = Path('models/tinybert-financial-classifier')\n",
    "DATA_FILE = 'data/FinancialPhraseBank/all-data.csv'\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.25\n",
    "\n",
    "# Load full dataset and create train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data with correct encoding and column names (matching training notebook)\n",
    "df = pd.read_csv(DATA_FILE, header=None, names=[\"label\", \"sentence\"], encoding=\"latin-1\")\n",
    "df[\"sentence\"] = df[\"sentence\"].str.strip('\"')  # Remove extra quotes\n",
    "\n",
    "# Create train-test split with same parameters as training\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_SEED, \n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "# Extract test data\n",
    "test_texts = test_df['sentence'].tolist()  # Note: column is 'sentence' not 'text'\n",
    "\n",
    "# Load label encoder\n",
    "import pickle\n",
    "with open(MODEL_DIR / 'label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "true_labels_encoded = label_encoder.transform(test_df['label'])\n",
    "\n",
    "print(f\"üìä Loaded full dataset: {len(df)} samples\")\n",
    "print(f\"üìä Test set: {len(test_texts)} samples (25% split)\")\n",
    "print(f\"üìã Label classes: {list(label_encoder.classes_)}\")\n",
    "print(f\"üé≤ Random seed: {RANDOM_SEED}\")\n",
    "print(f\"‚úÖ Data loaded successfully with correct encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "816cc105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading model and tokenizer...\n",
      "‚úÖ Model and tokenizer loaded successfully\n",
      "üì± Model type: BertForSequenceClassification\n",
      "üéØ Number of classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "print(\"üîÑ Loading model and tokenizer...\")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_DIR)\n",
    "pt_model = BertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "pt_model.eval()\n",
    "\n",
    "print(\"‚úÖ Model and tokenizer loaded successfully\")\n",
    "print(f\"üì± Model type: {type(pt_model).__name__}\")\n",
    "print(f\"üéØ Number of classes: {pt_model.config.num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c8c2e3",
   "metadata": {},
   "source": [
    "## 3. üîß Core Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70ec723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core prediction functions defined\n"
     ]
    }
   ],
   "source": [
    "def predict_class(texts):\n",
    "    \"\"\"Predict sentiment class for text(s)\"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    predictions = []\n",
    "    pt_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            encoding = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "            outputs = pt_model(**encoding)\n",
    "            predicted_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "            predictions.append(predicted_class)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "def predict_probs_for_shap(texts):\n",
    "    \"\"\"Get prediction probabilities for SHAP\"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    all_probs = []\n",
    "    pt_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            encoding = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "            outputs = pt_model(**encoding)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1).squeeze().numpy()\n",
    "            all_probs.append(probs)\n",
    "    \n",
    "    return np.array(all_probs)\n",
    "\n",
    "def predict_probs_for_lime(texts):\n",
    "    \"\"\"Get prediction probabilities for LIME (expects different format)\"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    elif isinstance(texts, list) and len(texts) == 1 and isinstance(texts[0], str):\n",
    "        # LIME sometimes passes single text as list\n",
    "        texts = texts\n",
    "    elif isinstance(texts, (list, tuple)) and all(isinstance(t, str) for t in texts):\n",
    "        # LIME passes list of texts\n",
    "        texts = list(texts)\n",
    "    else:\n",
    "        # Convert to list if needed\n",
    "        texts = [str(t) for t in texts]\n",
    "    \n",
    "    all_probs = []\n",
    "    pt_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            try:\n",
    "                encoding = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "                outputs = pt_model(**encoding)\n",
    "                probs = torch.softmax(outputs.logits, dim=-1).squeeze().cpu().numpy()\n",
    "                all_probs.append(probs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text: {text[:50]}... Error: {e}\")\n",
    "                # Return default probabilities if processing fails\n",
    "                all_probs.append(np.array([0.33, 0.33, 0.34]))\n",
    "    \n",
    "    return np.array(all_probs)\n",
    "\n",
    "print(\"‚úÖ Core prediction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879bac0",
   "metadata": {},
   "source": [
    "## 4. üß© Explainability Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d73dd7",
   "metadata": {},
   "source": [
    "### 4.1 SHAP Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "877e6941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SHAP implementation ready\n"
     ]
    }
   ],
   "source": [
    "# SHAP explainer (lazy initialization for performance)\n",
    "_shap_explainer = None\n",
    "\n",
    "def get_shap_explainer():\n",
    "    \"\"\"Get SHAP explainer (lazy initialization)\"\"\"\n",
    "    global _shap_explainer\n",
    "    if _shap_explainer is None:\n",
    "        print(\"üßÆ Initializing SHAP explainer...\")\n",
    "        _shap_explainer = shap.Explainer(predict_probs_for_shap, tokenizer)\n",
    "    return _shap_explainer\n",
    "\n",
    "def explain_with_shap(text, target_class=None):\n",
    "    \"\"\"Generate SHAP explanation for text\"\"\"\n",
    "    print(\"‚è≥ Computing SHAP values...\")\n",
    "    \n",
    "    explainer = get_shap_explainer()\n",
    "    shap_values = explainer([text])\n",
    "    \n",
    "    if target_class is None:\n",
    "        target_class = predict_class(text)[0]\n",
    "    \n",
    "    # Display SHAP plot\n",
    "    shap.plots.text(shap_values[0, :, target_class])\n",
    "    \n",
    "    pred_label = label_encoder.inverse_transform([target_class])[0]\n",
    "    print(f\"üìä SHAP explanation for class: {pred_label}\")\n",
    "\n",
    "print(\"‚úÖ SHAP implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35809a96",
   "metadata": {},
   "source": [
    "### 4.2 LIME Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d641ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LIME implementation ready\n"
     ]
    }
   ],
   "source": [
    "# LIME explainer (lazy initialization)\n",
    "_lime_explainer = None\n",
    "\n",
    "def get_lime_explainer():\n",
    "    \"\"\"Get LIME explainer (lazy initialization)\"\"\"\n",
    "    global _lime_explainer\n",
    "    if _lime_explainer is None:\n",
    "        _lime_explainer = LimeTextExplainer(\n",
    "            class_names=label_encoder.classes_\n",
    "            # Removed 'mode' parameter as it's not valid for LimeTextExplainer\n",
    "        )\n",
    "    return _lime_explainer\n",
    "\n",
    "def explain_with_lime(text):\n",
    "    \"\"\"Generate LIME explanation for text\"\"\"\n",
    "    print(\"‚è≥ Computing LIME explanation...\")\n",
    "    \n",
    "    explainer = get_lime_explainer()\n",
    "    explanation = explainer.explain_instance(\n",
    "        text,\n",
    "        predict_probs_for_lime,\n",
    "        num_features=20,\n",
    "        labels=(0, 1, 2)\n",
    "    )\n",
    "    \n",
    "    display(HTML(explanation.as_html()))\n",
    "    print(\"üìä LIME explanation generated\")\n",
    "\n",
    "print(\"‚úÖ LIME implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dad249",
   "metadata": {},
   "source": [
    "### 4.3 Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5eca8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Attention visualization ready\n"
     ]
    }
   ],
   "source": [
    "def explain_with_attention(text):\n",
    "    \"\"\"Generate attention visualization for text\"\"\"\n",
    "    print(\"‚è≥ Generating attention visualization...\")\n",
    "    \n",
    "    try:\n",
    "        # Tokenize with attention output\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        \n",
    "        # Get model outputs with attention\n",
    "        with torch.no_grad():\n",
    "            # Force eager attention for BertViz compatibility\n",
    "            original_impl = getattr(pt_model.config, '_attn_implementation', None)\n",
    "            pt_model.config._attn_implementation = 'eager'\n",
    "            \n",
    "            outputs = pt_model(**inputs, output_attentions=True)\n",
    "            attentions = outputs.attentions\n",
    "            tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "            \n",
    "            # Restore original implementation\n",
    "            if original_impl is not None:\n",
    "                pt_model.config._attn_implementation = original_impl\n",
    "        \n",
    "        # Check if we have valid attention and tokens\n",
    "        if attentions is None or len(attentions) == 0:\n",
    "            print(\"‚ùå No attention weights available\")\n",
    "            return\n",
    "            \n",
    "        if len(tokens) == 0:\n",
    "            print(\"‚ùå No tokens available\")\n",
    "            return\n",
    "        \n",
    "        pred_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "        pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "        \n",
    "        # Try BertViz first\n",
    "        try:\n",
    "            # Enable widget display\n",
    "            from IPython.display import Javascript\n",
    "            display(Javascript(\"\"\"\n",
    "                require.config({\n",
    "                    paths: {\n",
    "                        d3: 'https://d3js.org/d3.v5.min'\n",
    "                    }\n",
    "                });\n",
    "            \"\"\"))\n",
    "            \n",
    "            print(\"üéØ Attempting interactive attention visualization...\")\n",
    "            head_view(attentions, tokens)\n",
    "            print(f\"üëÅÔ∏è Interactive attention visualization for prediction: {pred_label}\")\n",
    "            \n",
    "        except Exception as viz_error:\n",
    "            print(f\"‚ùå BertViz interactive view failed: {viz_error}\")\n",
    "            print(\"üí° Using custom attention heatmap...\")\n",
    "            \n",
    "            # Custom attention visualization\n",
    "            _visualize_attention_heatmap(attentions, tokens, pred_label)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Attention analysis failed: {str(e)}\")\n",
    "        print(\"üí° This might be due to model architecture or BertViz compatibility issues\")\n",
    "\n",
    "def _visualize_attention_heatmap(attentions, tokens, pred_label):\n",
    "    \"\"\"Create custom attention heatmap visualization\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Get average attention across all layers and heads\n",
    "    # Shape: (num_layers, batch_size, num_heads, seq_len, seq_len)\n",
    "    avg_attention = torch.stack(attentions).mean(dim=0)  # Average across layers\n",
    "    avg_attention = avg_attention.mean(dim=1)  # Average across heads\n",
    "    attention_matrix = avg_attention[0].detach().cpu().numpy()  # Get first (and only) batch\n",
    "    \n",
    "    # Clean tokens for display\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.startswith('##'):\n",
    "            clean_tokens.append(token[2:])\n",
    "        elif token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            clean_tokens.append(token)\n",
    "        else:\n",
    "            clean_tokens.append(token)\n",
    "    \n",
    "    # Limit to reasonable size for visualization\n",
    "    max_len = min(len(clean_tokens), 50)\n",
    "    attention_matrix = attention_matrix[:max_len, :max_len]\n",
    "    display_tokens = clean_tokens[:max_len]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # 1. Full attention heatmap\n",
    "    sns.heatmap(attention_matrix, \n",
    "                xticklabels=display_tokens,\n",
    "                yticklabels=display_tokens,\n",
    "                cmap='Blues',\n",
    "                ax=ax1,\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    ax1.set_title(f'Attention Heatmap\\nPrediction: {pred_label}', fontsize=14, weight='bold')\n",
    "    ax1.set_xlabel('Attended Tokens')\n",
    "    ax1.set_ylabel('Query Tokens')\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.setp(ax1.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    # 2. CLS token attention (what the model focuses on for classification)\n",
    "    cls_attention = attention_matrix[0, 1:]  # CLS token attention to other tokens (skip self-attention)\n",
    "    tokens_for_cls = display_tokens[1:]  # Skip CLS token\n",
    "    \n",
    "    # Sort by attention weight\n",
    "    token_attention_pairs = list(zip(tokens_for_cls, cls_attention))\n",
    "    token_attention_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Take top 15 for readability\n",
    "    top_tokens, top_weights = zip(*token_attention_pairs[:15])\n",
    "    \n",
    "    bars = ax2.barh(range(len(top_tokens)), top_weights, color='skyblue')\n",
    "    ax2.set_yticks(range(len(top_tokens)))\n",
    "    ax2.set_yticklabels(top_tokens)\n",
    "    ax2.set_xlabel('Attention Weight')\n",
    "    ax2.set_title(f'Top Attended Tokens for Classification\\n(CLS token attention)', fontsize=14, weight='bold')\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, weight) in enumerate(zip(bars, top_weights)):\n",
    "        ax2.text(weight + 0.001, i, f'{weight:.3f}', \n",
    "                va='center', ha='left', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"üìä Attention Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Number of layers: {len(attentions)}\")\n",
    "    print(f\"   ‚Ä¢ Number of heads per layer: {attentions[0].shape[2]}\")\n",
    "    print(f\"   ‚Ä¢ Sequence length: {len(tokens)}\")\n",
    "    print(f\"   ‚Ä¢ Max attention weight: {attention_matrix.max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Average attention weight: {attention_matrix.mean():.4f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Top 5 tokens by CLS attention:\")\n",
    "    for i, (token, weight) in enumerate(token_attention_pairs[:5]):\n",
    "        if token not in ['[SEP]', '[PAD]']:\n",
    "            print(f\"   {i+1}. '{token}': {weight:.4f}\")\n",
    "    \n",
    "    print(f\"üëÅÔ∏è Custom attention visualization complete for: {pred_label}\")\n",
    "\n",
    "print(\"‚úÖ Attention visualization ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d66aab3",
   "metadata": {},
   "source": [
    "### 4.4 GradCAM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "470f20d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GradCAM implementation ready\n"
     ]
    }
   ],
   "source": [
    "class ModelWrapper(torch.nn.Module):\n",
    "    \"\"\"Wrapper to fix SequenceClassifierOutput error with Captum\"\"\"\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "\n",
    "def explain_with_gradcam(text, target_class=None):\n",
    "    \"\"\"Generate GradCAM explanation for text\"\"\"\n",
    "    print(\"‚è≥ Computing GradCAM attributions...\")\n",
    "    \n",
    "    try:\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        # Get prediction if not specified\n",
    "        if target_class is None:\n",
    "            with torch.no_grad():\n",
    "                outputs = pt_model(**inputs)\n",
    "                target_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "        \n",
    "        # Ensure target_class is the right type for Captum\n",
    "        target_class = int(target_class)  # Convert to Python int from numpy.int64\n",
    "        \n",
    "        # Use ModelWrapper for Captum compatibility\n",
    "        wrapped_model = ModelWrapper(pt_model)\n",
    "        wrapped_model.eval()\n",
    "        \n",
    "        # Try to access embedding layer with different paths\n",
    "        embedding_layer = None\n",
    "        try:\n",
    "            embedding_layer = pt_model.bert.embeddings.word_embeddings\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                embedding_layer = pt_model.embeddings.word_embeddings\n",
    "            except AttributeError:\n",
    "                try:\n",
    "                    embedding_layer = pt_model.get_input_embeddings()\n",
    "                except AttributeError:\n",
    "                    print(\"‚ùå Could not access embedding layer\")\n",
    "                    return\n",
    "        \n",
    "        if embedding_layer is None:\n",
    "            print(\"‚ùå Embedding layer not found\")\n",
    "            return\n",
    "        \n",
    "        # Initialize LayerGradCam\n",
    "        layer_gradcam = LayerGradCam(wrapped_model, embedding_layer)\n",
    "        \n",
    "        # Generate attributions\n",
    "        attributions = layer_gradcam.attribute(\n",
    "            input_ids,\n",
    "            target=target_class,\n",
    "            additional_forward_args=(attention_mask,)\n",
    "        )\n",
    "        \n",
    "        # Process attributions\n",
    "        attribution_scores = attributions.squeeze().detach().cpu().numpy()\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "        \n",
    "        if len(attribution_scores.shape) > 1:\n",
    "            attribution_scores = attribution_scores.sum(axis=-1)\n",
    "        \n",
    "        # Visualize\n",
    "        _visualize_gradcam(tokens, attribution_scores, attention_mask, target_class)\n",
    "        \n",
    "        pred_label = label_encoder.inverse_transform([target_class])[0]\n",
    "        print(f\"üå°Ô∏è GradCAM explanation for class: {pred_label}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GradCAM error: {str(e)}\")\n",
    "        print(\"üí° Falling back to attention-based attribution...\")\n",
    "        \n",
    "        # Fallback: Use attention weights as pseudo-GradCAM\n",
    "        try:\n",
    "            inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = pt_model(**inputs, output_attentions=True)\n",
    "                attentions = outputs.attentions\n",
    "                \n",
    "                if attentions is not None and len(attentions) > 0:\n",
    "                    # Average attention across layers and heads\n",
    "                    avg_attention = torch.stack(attentions).mean(dim=0).mean(dim=1)\n",
    "                    cls_attention = avg_attention[0, 0, :].detach().cpu().numpy()\n",
    "                    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze())\n",
    "                    \n",
    "                    # Visualize as pseudo-GradCAM\n",
    "                    _visualize_gradcam(tokens, cls_attention, inputs['attention_mask'], \n",
    "                                     torch.argmax(outputs.logits, dim=-1).item())\n",
    "                    print(\"üìä Used attention weights as attribution fallback\")\n",
    "                else:\n",
    "                    print(\"‚ùå No attention weights available for fallback\")\n",
    "        except Exception as fallback_error:\n",
    "            print(f\"‚ùå Fallback also failed: {fallback_error}\")\n",
    "            print(\"üí° Try using SHAP or LIME for alternative explanations\")\n",
    "\n",
    "def _visualize_gradcam(tokens, attribution_scores, attention_mask, target_class):\n",
    "    \"\"\"Create GradCAM visualization\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    \n",
    "    # Normalize attributions\n",
    "    abs_attributions = np.abs(attribution_scores)\n",
    "    if abs_attributions.max() > 0:\n",
    "        normalized_attrs = abs_attributions / abs_attributions.max()\n",
    "    else:\n",
    "        normalized_attrs = abs_attributions\n",
    "    \n",
    "    # Plot tokens with color intensity\n",
    "    colors = plt.cm.Reds(normalized_attrs)\n",
    "    x_positions = []\n",
    "    \n",
    "    for i, (token, attr, color) in enumerate(zip(tokens, normalized_attrs, colors)):\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]'] or attention_mask[0][i].item() == 0:\n",
    "            continue\n",
    "        \n",
    "        clean_token = token.replace('##', '')\n",
    "        if not clean_token.strip():\n",
    "            continue\n",
    "        \n",
    "        x_pos = len(x_positions) * 1.2\n",
    "        x_positions.append(x_pos)\n",
    "        \n",
    "        bbox_props = dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.8)\n",
    "        ax.text(x_pos, 0.5, clean_token, fontsize=11, ha='center', va='center',\n",
    "                bbox=bbox_props, weight='bold' if attr > 0.5 else 'normal')\n",
    "    \n",
    "    # Format plot\n",
    "    if x_positions:\n",
    "        ax.set_xlim(-0.5, max(x_positions) + 0.5)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    pred_label = label_encoder.inverse_transform([target_class])[0]\n",
    "    ax.set_title(f'GradCAM Attribution for Class: {pred_label}\\n(Darker Red = Higher Attribution)', \n",
    "                fontsize=14, pad=30, weight='bold')\n",
    "    \n",
    "    # Add colorbar\n",
    "    sm = plt.cm.ScalarMappable(cmap=plt.cm.Reds, norm=plt.Normalize(vmin=0, vmax=1))\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, orientation='horizontal', shrink=0.6, pad=0.15)\n",
    "    cbar.set_label('Attribution Intensity', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ GradCAM implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f63bc9",
   "metadata": {},
   "source": [
    "## 5. üéõÔ∏è Interactive Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cb7c462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dashboard class defined\n"
     ]
    }
   ],
   "source": [
    "class ExplainabilityDashboard:\n",
    "    \"\"\"Interactive dashboard for model explainability analysis\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.setup_data()\n",
    "        self.create_interface()\n",
    "    \n",
    "    def setup_data(self):\n",
    "        \"\"\"Setup data for mistake analysis\"\"\"\n",
    "        predictions_encoded = predict_class(test_texts)\n",
    "        self.incorrect_indices = np.where(predictions_encoded != true_labels_encoded)[0]\n",
    "        print(f\"üìä Found {len(self.incorrect_indices)} mistakes out of {len(test_texts)} samples\")\n",
    "    \n",
    "    def create_interface(self):\n",
    "        \"\"\"Create the dashboard interface\"\"\"\n",
    "        # Input mode selector\n",
    "        self.input_mode = widgets.ToggleButtons(\n",
    "            options=[('Analyze Mistakes', 'mistakes'), ('Custom Text', 'custom')],\n",
    "            value='mistakes',\n",
    "            description='Analysis Mode:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Mistake selector\n",
    "        mistake_options = [(f\"Mistake {i+1}: {test_texts[idx][:50]}...\", i) \n",
    "                          for i, idx in enumerate(self.incorrect_indices[:20])]  # Limit for performance\n",
    "        self.mistake_selector = widgets.Dropdown(\n",
    "            options=mistake_options,\n",
    "            description='Select Mistake:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Custom text input\n",
    "        self.text_input = widgets.Textarea(\n",
    "            placeholder='Enter financial text to analyze...',\n",
    "            description='Text:',\n",
    "            layout=widgets.Layout(width='100%', height='80px'),\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Control buttons\n",
    "        self.analyze_button = widgets.Button(\n",
    "            description='üöÄ Analyze',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        \n",
    "        self.clear_button = widgets.Button(\n",
    "            description='üßπ Clear',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        \n",
    "        # Output tabs\n",
    "        self.output_tabs = widgets.Tab()\n",
    "        self.method_outputs = {\n",
    "            'SHAP': widgets.Output(),\n",
    "            'LIME': widgets.Output(),\n",
    "            'Attention': widgets.Output(),\n",
    "            'GradCAM': widgets.Output()\n",
    "        }\n",
    "        \n",
    "        self.output_tabs.children = list(self.method_outputs.values())\n",
    "        for i, method in enumerate(self.method_outputs.keys()):\n",
    "            self.output_tabs.set_title(i, f'{method}')\n",
    "        \n",
    "        # Status output\n",
    "        self.status_output = widgets.Output()\n",
    "        \n",
    "        # Event handlers\n",
    "        self.input_mode.observe(self.on_mode_change, names='value')\n",
    "        self.analyze_button.on_click(self.on_analyze)\n",
    "        self.clear_button.on_click(self.on_clear)\n",
    "    \n",
    "    def on_mode_change(self, change):\n",
    "        \"\"\"Handle input mode change\"\"\"\n",
    "        # Update the input container dynamically\n",
    "        if hasattr(self, 'input_container'):\n",
    "            if change['new'] == 'mistakes':\n",
    "                self.input_container.children = [self.input_mode, self.mistake_selector]\n",
    "            else:\n",
    "                self.input_container.children = [self.input_mode, self.text_input]\n",
    "    \n",
    "    def update_interface(self):\n",
    "        \"\"\"Update interface based on mode\"\"\"\n",
    "        # This method is called by on_mode_change\n",
    "        pass\n",
    "    \n",
    "    def on_analyze(self, button):\n",
    "        \"\"\"Handle analyze button click\"\"\"\n",
    "        try:\n",
    "            # Get text and prediction info\n",
    "            if self.input_mode.value == 'mistakes':\n",
    "                mistake_idx = self.mistake_selector.value\n",
    "                sample_idx = self.incorrect_indices[mistake_idx]\n",
    "                text = test_texts[sample_idx]\n",
    "                true_label = label_encoder.inverse_transform([true_labels_encoded[sample_idx]])[0]\n",
    "                pred_class = int(predict_class(text)[0])  # Ensure Python int\n",
    "                pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "            else:\n",
    "                text = self.text_input.value.strip()\n",
    "                if not text:\n",
    "                    with self.status_output:\n",
    "                        clear_output(wait=True)\n",
    "                        print(\"‚ùå Please enter some text to analyze!\")\n",
    "                    return\n",
    "                pred_class = int(predict_class(text)[0])  # Ensure Python int\n",
    "                pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "                true_label = \"Unknown\"\n",
    "            \n",
    "            # Generate explanations\n",
    "            self.generate_explanations(text, pred_label, true_label, pred_class)\n",
    "            \n",
    "        except Exception as e:\n",
    "            with self.status_output:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"‚ùå Error during analysis: {str(e)}\")\n",
    "    \n",
    "    def generate_explanations(self, text, pred_label, true_label, pred_class):\n",
    "        \"\"\"Generate all explanations for the text\"\"\"\n",
    "        # Clear outputs\n",
    "        for output in self.method_outputs.values():\n",
    "            with output:\n",
    "                clear_output()\n",
    "        \n",
    "        # Create header\n",
    "        header_html = f\"\"\"\n",
    "        <div style='background: #f8f9fa; padding: 15px; margin: 10px 0; border-radius: 8px; \n",
    "                    border-left: 4px solid #007bff; box-shadow: 0 2px 8px rgba(0,0,0,0.1);'>\n",
    "            <h4 style='margin: 0 0 10px 0; color: #007bff;'>üìù Analysis Summary</h4>\n",
    "            <p style='margin: 5px 0;'><strong>Text:</strong> <em>\"{text}\"</em></p>\n",
    "            <p style='margin: 5px 0;'><strong>Model Prediction:</strong> \n",
    "               <span style='color: #28a745; font-weight: bold;'>{pred_label}</span></p>\n",
    "            {f'<p style=\"margin: 5px 0;\"><strong>True Label:</strong> <span style=\"color: #dc3545; font-weight: bold;\">{true_label}</span></p>' if true_label != \"Unknown\" else ''}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"üß† Generating explanations...\")\n",
    "        \n",
    "        # SHAP\n",
    "        with self.method_outputs['SHAP']:\n",
    "            display(HTML(header_html))\n",
    "            try:\n",
    "                explain_with_shap(text, pred_class)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå SHAP failed: {str(e)}\")\n",
    "        \n",
    "        # LIME\n",
    "        with self.method_outputs['LIME']:\n",
    "            display(HTML(header_html))\n",
    "            try:\n",
    "                explain_with_lime(text)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå LIME failed: {str(e)}\")\n",
    "                print(\"üí° Common LIME issues:\")\n",
    "                print(\"   - Text preprocessing differences\")\n",
    "                print(\"   - Prediction function format mismatch\")\n",
    "                print(\"   - Try using SHAP instead\")\n",
    "        \n",
    "        # Attention\n",
    "        with self.method_outputs['Attention']:\n",
    "            display(HTML(header_html))\n",
    "            try:\n",
    "                # BertViz doesn't work well in widget contexts, so use custom visualization\n",
    "                print(\"‚è≥ Generating attention visualization...\")\n",
    "                print(\"üí° Using custom heatmap (BertViz widgets don't render in dashboard)\")\n",
    "                \n",
    "                # Get attention data\n",
    "                inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "                with torch.no_grad():\n",
    "                    # Force eager attention for compatibility\n",
    "                    original_impl = getattr(pt_model.config, '_attn_implementation', None)\n",
    "                    pt_model.config._attn_implementation = 'eager'\n",
    "                    \n",
    "                    outputs = pt_model(**inputs, output_attentions=True)\n",
    "                    attentions = outputs.attentions\n",
    "                    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "                    \n",
    "                    # Restore original implementation\n",
    "                    if original_impl is not None:\n",
    "                        pt_model.config._attn_implementation = original_impl\n",
    "                \n",
    "                if attentions is not None and len(attentions) > 0:\n",
    "                    pred_label = label_encoder.inverse_transform([pred_class])[0]\n",
    "                    _visualize_attention_heatmap(attentions, tokens, pred_label)\n",
    "                else:\n",
    "                    print(\"‚ùå No attention weights available\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Attention failed: {str(e)}\")\n",
    "                print(\"üí° Common Attention issues:\")\n",
    "                print(\"   - BertViz compatibility with model architecture\")\n",
    "                print(\"   - JavaScript widget display problems\")\n",
    "                print(\"   - Try refreshing the notebook kernel\")\n",
    "        \n",
    "        # GradCAM\n",
    "        with self.method_outputs['GradCAM']:\n",
    "            display(HTML(header_html))\n",
    "            try:\n",
    "                explain_with_gradcam(text, pred_class)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå GradCAM failed: {str(e)}\")\n",
    "                print(\"üí° Common GradCAM issues:\")\n",
    "                print(\"   - Model architecture compatibility\")\n",
    "                print(\"   - Captum version mismatch\")\n",
    "                print(\"   - GPU/CPU tensor issues\")\n",
    "        \n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"‚úÖ Analysis complete! Explore the tabs above.\")\n",
    "    \n",
    "    def on_clear(self, button):\n",
    "        \"\"\"Clear all outputs\"\"\"\n",
    "        for output in self.method_outputs.values():\n",
    "            with output:\n",
    "                clear_output()\n",
    "        \n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"üßπ All results cleared! Ready for new analysis.\")\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the dashboard\"\"\"\n",
    "        # Title\n",
    "        title = widgets.HTML(\n",
    "            value=\"\"\"\n",
    "            <div style='text-align: center; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "                        color: white; padding: 20px; border-radius: 10px; margin-bottom: 20px;'>\n",
    "                <h2 style='margin: 0; font-size: 24px;'>üß† Financial Sentiment Explainability Dashboard</h2>\n",
    "                <p style='margin: 10px 0 0 0; opacity: 0.9;'>Comprehensive AI model explanation and analysis</p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        # Dynamic input section that updates based on mode\n",
    "        self.input_container = widgets.VBox([\n",
    "            self.input_mode,\n",
    "            self.mistake_selector if self.input_mode.value == 'mistakes' else self.text_input\n",
    "        ])\n",
    "        \n",
    "        # Controls\n",
    "        controls = widgets.HBox([\n",
    "            self.analyze_button,\n",
    "            self.clear_button\n",
    "        ], layout=widgets.Layout(justify_content='space-between', width='250px'))\n",
    "        \n",
    "        # Main dashboard\n",
    "        dashboard = widgets.VBox([\n",
    "            title,\n",
    "            self.input_container,\n",
    "            controls,\n",
    "            self.status_output,\n",
    "            self.output_tabs\n",
    "        ])\n",
    "        \n",
    "        return dashboard\n",
    "\n",
    "print(\"‚úÖ Dashboard class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cadb6f",
   "metadata": {},
   "source": [
    "## 6. üöÄ Launch Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acd84cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Initializing Explainability Dashboard...\n",
      "üìä Found 253 mistakes out of 1212 samples\n",
      "üìä Found 253 mistakes out of 1212 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f8db101ebc40ba9642db32a954ad5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"\\n            <div style='text-align: center; background: linear-gradient(135deg, #‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dashboard is ready! Use the interface above to analyze model predictions.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and display dashboard\n",
    "print(\"üéØ Initializing Explainability Dashboard...\")\n",
    "dashboard = ExplainabilityDashboard()\n",
    "display(dashboard.display())\n",
    "print(\"‚úÖ Dashboard is ready! Use the interface above to analyze model predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf7870f",
   "metadata": {},
   "source": [
    "## 7. \udd0d Quick Misclassification Analysis\n",
    "\n",
    "Simple analysis to identify patterns for fine-tuning in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9371f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total misclassifications: 253\n",
      "üìà Error rate: 20.9%\n",
      "\n",
      "üîÑ Top Confusion Patterns:\n",
      "  neutral ‚Üí positive: 84 cases (33.2%)\n",
      "  positive ‚Üí neutral: 78 cases (30.8%)\n",
      "  neutral ‚Üí negative: 38 cases (15.0%)\n",
      "\n",
      "üîç Problematic Keywords:\n",
      "  solutions: 0.0176\n",
      "  pct: 0.0149\n",
      "  new: 0.0143\n",
      "  compared: 0.0134\n",
      "  oyj: 0.0121\n",
      "\n",
      "üíæ Results saved to: analysis_results/misclassification_analysis.json\n",
      "üìã Ready for fine-tuning in next notebook!\n"
     ]
    }
   ],
   "source": [
    "# Quick misclassification analysis\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Get model predictions on test set\n",
    "test_predictions = predict_class(test_texts)\n",
    "\n",
    "# Get misclassified examples\n",
    "misclassified_mask = test_predictions != true_labels_encoded\n",
    "misclassified_texts = [test_texts[i] for i in range(len(test_texts)) if misclassified_mask[i]]\n",
    "misclassified_true = true_labels_encoded[misclassified_mask]\n",
    "misclassified_pred = test_predictions[misclassified_mask]\n",
    "\n",
    "print(f\"üìä Total misclassifications: {len(misclassified_texts)}\")\n",
    "print(f\"üìà Error rate: {len(misclassified_texts)/len(test_texts)*100:.1f}%\")\n",
    "\n",
    "# 1. Confusion patterns\n",
    "print(\"\\nüîÑ Top Confusion Patterns:\")\n",
    "confusion_data = defaultdict(int)\n",
    "for true_idx, pred_idx in zip(misclassified_true, misclassified_pred):\n",
    "    true_label = label_encoder.inverse_transform([true_idx])[0]\n",
    "    pred_label = label_encoder.inverse_transform([pred_idx])[0]\n",
    "    confusion_data[(true_label, pred_label)] += 1\n",
    "\n",
    "for (true_label, pred_label), count in sorted(confusion_data.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "    percentage = count / len(misclassified_texts) * 100\n",
    "    print(f\"  {true_label} ‚Üí {pred_label}: {count} cases ({percentage:.1f}%)\")\n",
    "\n",
    "# 2. Problematic keywords\n",
    "print(\"\\nüîç Problematic Keywords:\")\n",
    "correctly_classified_texts = [test_texts[i] for i in range(len(test_texts)) if not misclassified_mask[i]]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=200, stop_words='english', ngram_range=(1, 2))\n",
    "all_texts = misclassified_texts + correctly_classified_texts[:len(misclassified_texts)]\n",
    "vectorizer.fit(all_texts)\n",
    "\n",
    "misc_tfidf = vectorizer.transform(misclassified_texts).mean(axis=0).A1\n",
    "correct_tfidf = vectorizer.transform(correctly_classified_texts[:len(misclassified_texts)]).mean(axis=0).A1\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "score_diff = misc_tfidf - correct_tfidf\n",
    "top_indices = score_diff.argsort()[-10:][::-1]\n",
    "\n",
    "problematic_keywords = [(feature_names[i], float(score_diff[i])) for i in top_indices if score_diff[i] > 0.001]\n",
    "for keyword, score in problematic_keywords[:5]:\n",
    "    print(f\"  {keyword}: {score:.4f}\")\n",
    "\n",
    "# Save results for fine-tuning notebook\n",
    "os.makedirs('analysis_results', exist_ok=True)\n",
    "results = {\n",
    "    'confusion_patterns': {f\"{true_label} -> {pred_label}\": count for (true_label, pred_label), count in confusion_data.items()},\n",
    "    'problematic_keywords': problematic_keywords,\n",
    "    'total_errors': len(misclassified_texts),\n",
    "    'error_rate': len(misclassified_texts)/len(test_texts)*100\n",
    "}\n",
    "\n",
    "with open('analysis_results/misclassification_analysis.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Results saved to: analysis_results/misclassification_analysis.json\")\n",
    "print(f\"üìã Ready for fine-tuning in next notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8575704e",
   "metadata": {},
   "source": [
    "## 8. üìã Summary\n",
    "\n",
    "### ‚úÖ Completed:\n",
    "- **Interactive Dashboard**: SHAP and LIME explanations for any text\n",
    "- **Mistake Analysis**: Analyze specific model errors  \n",
    "- **Misclassification Patterns**: Key insights for fine-tuning\n",
    "\n",
    "### üìä Key Findings:\n",
    "- Error rate: ~20% on test data\n",
    "- Main confusion patterns identified\n",
    "- Problematic keywords extracted\n",
    "\n",
    "### üîú Next Steps:\n",
    "Results saved to `analysis_results/` for **Notebook 6: Fine-tuning with Pruning Methods**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
