{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏋️ Model Training Pipeline\n",
    "## Financial Sentiment Analysis - Production Model Training\n",
    "\n",
    "[![Training](https://img.shields.io/badge/Stage-Model%20Training-orange?logo=pytorch&logoColor=white)]()\n",
    "[![Multi-Model](https://img.shields.io/badge/Support-Multi%20Model-purple)]()\n",
    "[![Hardware Optimised](https://img.shields.io/badge/Hardware-CPU%2FGPU%2FMPS-green)]()\n",
    "\n",
    "---\n",
    "\n",
    "### 📋 Overview\n",
    "\n",
    "This notebook orchestrates the training of multiple transformer models for financial sentiment analysis. It features intelligent hardware detection, dynamic optimisation, and comprehensive model evaluation.\n",
    "\n",
    "### 🎯 Key Features\n",
    "\n",
    "- **🤖 Multi-Model Training**: Support for BERT, DistilBERT, TinyBERT, FinBERT, and SmolLM variants\n",
    "- **⚡ Hardware Optimisation**: Automatic detection and optimisation for CPU, GPU (CUDA), and Apple Silicon (MPS)\n",
    "- **📊 Dynamic Batch Sizing**: Intelligent batch size adjustment based on available memory\n",
    "- **🎛️ Configuration Driven**: All hyperparameters controlled via `pipeline_config.json`\n",
    "- **📈 Progress Monitoring**: Real-time training metrics and logging\n",
    "- **💾 Checkpoint Management**: Automatic model saving and recovery\n",
    "\n",
    "### 🏗️ Supported Model Architectures\n",
    "\n",
    "| Model Family | Size | Memory Requirements | Training Time | Best Use Case |\n",
    "|--------------|------|-------------------|---------------|---------------|\n",
    "| **TinyBERT** | 14.5M | ~1GB | Fast | Resource-constrained deployment |\n",
    "| **DistilBERT** | 66M | ~2GB | Moderate | Balanced performance/efficiency |\n",
    "| **FinBERT** | 110M | ~3GB | Moderate | Finance-specific tasks |\n",
    "| **SmolLM2** | 135M | ~1.5GB | Fast | On-device applications |\n",
    "| **SmolLM3** | 3B | ~6-8GB | Slow | High-performance scenarios |\n",
    "\n",
    "### ⚙️ Training Optimisation Features\n",
    "\n",
    "- **📦 Dynamic Batch Sizing**: Automatic adjustment based on GPU memory\n",
    "- **🔄 Gradient Accumulation**: Simulate larger batches on limited hardware\n",
    "- **⚡ Mixed Precision**: FP16 training for supported hardware\n",
    "- **🧹 Memory Management**: Optimised settings for different platforms\n",
    "- **📊 Early Stopping**: Prevent overfitting with configurable patience\n",
    "\n",
    "### 🖥️ Hardware Support Matrix\n",
    "\n",
    "| Platform | Optimisation | Batch Size | Mixed Precision | Notes |\n",
    "|----------|--------------|------------|----------------|--------|\n",
    "| **NVIDIA GPU (>8GB)** | Full | 8+ | ✅ FP16 | Optimal performance |\n",
    "| **NVIDIA GPU (<8GB)** | Memory-aware | 2-4 | ✅ FP16 | Reduced batch size |\n",
    "| **Apple Silicon (MPS)** | MPS-optimised | 4 | ❌ Disabled | Native acceleration |\n",
    "| **CPU** | Multi-core | 4 | ❌ N/A | Fallback mode |\n",
    "\n",
    "### 📁 Output Structure\n",
    "\n",
    "```\n",
    "models/\n",
    "├── {model_name}/\n",
    "│   ├── config.json\n",
    "│   ├── model.safetensors\n",
    "│   ├── tokenizer_config.json\n",
    "│   ├── tokenizer.json\n",
    "│   ├── label_encoder.pkl\n",
    "│   └── logs/\n",
    "└── training_report.json\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites**: Complete data processing via `1_data_processing_generalised.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 14:33:45,038 - pipeline.training - INFO - 🏋️ Starting Model Training - Generalized Pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Configuration loaded from ../config/pipeline_config.json\n"
     ]
    }
   ],
   "source": [
    "# Import configuration system and training utilities\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.pipeline_utils import ConfigManager, StateManager, LoggingManager\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialise managers\n",
    "config = ConfigManager('../config/pipeline_config.json')\n",
    "state = StateManager('../config/pipeline_state.json')\n",
    "logger_manager = LoggingManager(config, 'training')\n",
    "logger = logger_manager.get_logger()\n",
    "\n",
    "logger.info(\"🏋️ Starting Model Training - Generalised Pipeline\")\n",
    "print(\"📋 Configuration loaded from ../config/pipeline_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 14:33:45,117 - pipeline.training - INFO - 🔍 Checking training prerequisites...\n",
      "2025-08-18 14:33:45,153 - pipeline.training - INFO - Successfully loaded training data\n",
      "2025-08-18 14:33:45,153 - pipeline.training - INFO - Successfully loaded training data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data processing verification passed\n",
      "🏋️ Training Configuration:\n",
      "   📊 Batch size: 16\n",
      "   🔄 Epochs: 3\n",
      "   📈 Learning rate: 2e-05\n",
      "   🤖 Models to train: 1\n",
      "\n",
      "📋 Configured Models:\n",
      "   ✅ smollm3-financial -> HuggingFaceTB/SmolLM3-3B 🔥 NEW!\n",
      "\n",
      "🚀 SmolLM3 Models Detected:\n",
      "   🔥 smollm3-financial (HuggingFaceTB/SmolLM3-3B)\n",
      "      🏷️ Labels: 3\n",
      "      ✅ Enabled: True\n",
      "\n",
      "📂 Loading Processed Datasets:\n",
      "   ✅ Training data loaded:\n",
      "      📊 Train: 4361 samples\n",
      "      📊 Validation: 485 samples\n",
      "      🏷️ Labels: ['negative', 'neutral', 'positive']\n"
     ]
    }
   ],
   "source": [
    "# Verify prerequisites and load processed data\n",
    "logger.info(\"🔍 Checking training prerequisites...\")\n",
    "\n",
    "# Verify data processing was completed\n",
    "if not state.is_step_complete('data_processing_completed'):\n",
    "    logger.error(\"Data processing step not completed. Please run 1_data_processing_generalised.ipynb first.\")\n",
    "    raise RuntimeError(\"Data processing required. Run 1_data_processing_generalised.ipynb first.\")\n",
    "\n",
    "print(\"✅ Data processing verification passed\")\n",
    "\n",
    "# Load training configuration\n",
    "training_config = config.get('training', {})\n",
    "models_config = config.get('models', {})\n",
    "data_config = config.get('data', {})\n",
    "\n",
    "print(f\"🏋️ Training Configuration:\")\n",
    "print(f\"   📊 Batch size: {training_config.get('batch_size', 16)}\")\n",
    "print(f\"   🔄 Epochs: {training_config.get('num_epochs', 3)}\")\n",
    "print(f\"   📈 Learning rate: {training_config.get('learning_rate', 2e-5)}\")\n",
    "\n",
    "# Get base models from configuration\n",
    "base_models = models_config.get('base_models', [])\n",
    "enabled_models = [m for m in base_models if m.get('enabled', True)]\n",
    "print(f\"   🤖 Models to train: {len(enabled_models)}\")\n",
    "\n",
    "# Show all configured models\n",
    "print(f\"\\n📋 Configured Models:\")\n",
    "for model in enabled_models:\n",
    "    print(f\"   ✅ {model['name']} -> {model['model_id']}\")\n",
    "    print(f\"      🏷️ Labels: {model['num_labels']}\")\n",
    "    print(f\"      ✅ Enabled: {model.get('enabled', True)}\")\n",
    "\n",
    "# Load processed datasets\n",
    "print(f\"\\n📂 Loading Processed Datasets:\")\n",
    "processed_data_dir = data_config.get('processed_data_dir', 'data/processed')\n",
    "train_path = Path(f\"../{processed_data_dir}/train.csv\")\n",
    "val_path = Path(f\"../{processed_data_dir}/validation.csv\")\n",
    "\n",
    "if train_path.exists() and val_path.exists():\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    \n",
    "    print(f\"   ✅ Training data loaded:\")\n",
    "    print(f\"      📊 Train: {len(train_df)} samples\")\n",
    "    print(f\"      📊 Validation: {len(val_df)} samples\")\n",
    "    print(f\"      🏷️ Labels: {sorted(train_df['label'].unique())}\")\n",
    "    \n",
    "    logger.info(f\"Successfully loaded training data\")\n",
    "else:\n",
    "    logger.error(f\"Processed data not found at {train_path} or {val_path}\")\n",
    "    raise FileNotFoundError(\"Processed data not found. Please run 1_data_processing_generalised.ipynb first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 14:33:45,177 - pipeline.training - INFO - ⚡ Applying training optimizations...\n",
      "2025-08-18 14:33:45,253 - pipeline.training - INFO - Training optimizations completed\n",
      "2025-08-18 14:33:45,253 - pipeline.training - INFO - Training optimizations completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Training Optimizations:\n",
      "   🍎 Apple Silicon MPS detected: batch size set to 1\n",
      "   ⚠️ Using aggressive memory conservation for MPS\n",
      "   🔄 Gradient accumulation steps: 8\n",
      "   📏 Max sequence length: 64\n",
      "   🍎 Mixed precision disabled for MPS compatibility\n",
      "\n",
      "🔥 SmolLM3 Memory Optimizations:\n",
      "   📦 SmolLM3 batch size: 1\n",
      "   🔄 SmolLM3 gradient accumulation: 8\n",
      "   📏 SmolLM3 max length: 64\n",
      "   💾 SmolLM3 requires ~6-8GB RAM for training\n",
      "   🔄 Reduced to 1 epoch for faster training (increase later if needed)\n",
      "   🧹 Memory management optimizations for MPS enabled\n",
      "   ✅ Optimizations applied\n"
     ]
    }
   ],
   "source": [
    "# Dynamic training optimisations based on hardware and model requirements\n",
    "logger.info(\"⚡ Applying dynamic training optimisations...\")\n",
    "\n",
    "# Detect device capabilities and optimise settings\n",
    "device_info = torch.cuda.get_device_properties(0) if torch.cuda.is_available() else None\n",
    "is_mps = torch.backends.mps.is_available() if hasattr(torch.backends, 'mps') else False\n",
    "\n",
    "print(\"⚡ Training Optimisations:\")\n",
    "\n",
    "# Hardware-based optimisation\n",
    "if device_info:\n",
    "    gpu_memory_gb = device_info.total_memory / 1e9\n",
    "    print(f\"   💾 GPU Memory: {gpu_memory_gb:.1f} GB\")\n",
    "    \n",
    "    # Adjust batch size based on GPU memory\n",
    "    if gpu_memory_gb < 8:\n",
    "        training_config['batch_size'] = 2\n",
    "        print(f\"   📊 Batch size optimised to 2 (limited GPU memory)\")\n",
    "    elif gpu_memory_gb < 16:\n",
    "        training_config['batch_size'] = 4\n",
    "        print(f\"   📊 Batch size optimised to 4 (moderate GPU memory)\")\n",
    "    else:\n",
    "        training_config['batch_size'] = 8\n",
    "        print(f\"   📊 Batch size optimised to 8 (high GPU memory)\")\n",
    "elif is_mps:\n",
    "    # Apple Silicon MPS optimisation\n",
    "    training_config['batch_size'] = 4\n",
    "    print(f\"   🍎 Apple Silicon MPS detected: batch size set to 4\")\n",
    "    print(f\"   ⚡ Optimised for efficient local training\")\n",
    "else:\n",
    "    # CPU training - moderate batch size\n",
    "    training_config['batch_size'] = 4\n",
    "    print(f\"   💻 CPU training: batch size set to 4\")\n",
    "\n",
    "# Apply model-specific optimisations from configuration\n",
    "model_specific_configs = training_config.get('model_specific_configs', {})\n",
    "print(f\"\\n🤖 Model-Specific Optimisations:\")\n",
    "\n",
    "for model in enabled_models:\n",
    "    model_name = model['name']\n",
    "    model_id = model['model_id']\n",
    "    \n",
    "    # Categorise model by type or size hints in the model ID\n",
    "    model_category = None\n",
    "    \n",
    "    # Check if model is explicitly listed in config categories\n",
    "    for category, config in model_specific_configs.items():\n",
    "        if model_name in config.get('models', []):\n",
    "            model_category = category\n",
    "            break\n",
    "    \n",
    "    # If not found, categorise by model ID patterns\n",
    "    if not model_category:\n",
    "        model_id_lower = model_id.lower()\n",
    "        if any(pattern in model_id_lower for pattern in ['tiny', '135m', 'small']):\n",
    "            model_category = 'small_models'\n",
    "        elif any(pattern in model_id_lower for pattern in ['3b', '1b', 'large']):\n",
    "            model_category = 'large_models'\n",
    "        else:\n",
    "            model_category = 'medium_models'\n",
    "    \n",
    "    # Apply category-specific optimisations\n",
    "    if model_category in model_specific_configs:\n",
    "        category_config = model_specific_configs[model_category]\n",
    "        print(f\"   🔧 {model_name} -> {model_category}\")\n",
    "        print(f\"      📊 Recommended batch size: {category_config.get('batch_size', 'default')}\")\n",
    "        print(f\"      📈 Learning rate: {category_config.get('learning_rate', 'default')}\")\n",
    "        print(f\"      🔄 Epochs: {category_config.get('num_epochs', 'default')}\")\n",
    "        \n",
    "        if 'memory_requirements' in category_config:\n",
    "            print(f\"      💾 Memory needs: {category_config['memory_requirements']}\")\n",
    "        \n",
    "        # Store model-specific config for later use\n",
    "        training_config[f'{model_name}_config'] = category_config\n",
    "\n",
    "# Enable gradient accumulation for effective larger batch size\n",
    "if training_config['batch_size'] < 8:\n",
    "    training_config['gradient_accumulation_steps'] = max(1, 8 // training_config['batch_size'])\n",
    "    print(f\"\\n🔄 Gradient accumulation steps: {training_config['gradient_accumulation_steps']}\")\n",
    "\n",
    "# Sequence length from configuration with fallback\n",
    "training_config['max_length'] = data_config.get('max_sequence_length', 128)\n",
    "print(f\"📏 Max sequence length: {training_config['max_length']}\")\n",
    "\n",
    "# Enable mixed precision if available (but not on MPS - can cause issues)\n",
    "if torch.cuda.is_available() and hasattr(torch.cuda, 'amp'):\n",
    "    training_config['fp16'] = True\n",
    "    print(f\"⚡ Mixed precision (FP16) enabled\")\n",
    "elif is_mps:\n",
    "    training_config['fp16'] = False\n",
    "    print(f\"🍎 Mixed precision disabled for MPS compatibility\")\n",
    "\n",
    "# Memory management settings for MPS\n",
    "if is_mps:\n",
    "    training_config['dataloader_pin_memory'] = False\n",
    "    training_config['dataloader_num_workers'] = 0\n",
    "    training_config['save_total_limit'] = 2\n",
    "    print(f\"🧹 Memory management optimisations for MPS enabled\")\n",
    "\n",
    "# Show final training configuration\n",
    "print(f\"\\n📋 Final Training Configuration:\")\n",
    "print(f\"   📊 Base batch size: {training_config.get('batch_size', 4)}\")\n",
    "print(f\"   🔄 Gradient accumulation: {training_config.get('gradient_accumulation_steps', 1)}\")\n",
    "print(f\"   📏 Max sequence length: {training_config.get('max_length', 128)}\")\n",
    "print(f\"   🔄 Epochs: {training_config.get('num_epochs', 3)}\")\n",
    "print(f\"   📈 Learning rate: {training_config.get('learning_rate', 2e-5)}\")\n",
    "print(f\"   ⚡ Mixed precision: {training_config.get('fp16', False)}\")\n",
    "\n",
    "print(f\"   ✅ Dynamic optimisations applied\")\n",
    "logger.info(\"Training optimisations completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 14:33:45,291 - pipeline.training - INFO - 🚀 Starting model training loop...\n",
      "2025-08-18 14:33:45,295 - pipeline.training - INFO - Starting training for smollm3-financial\n",
      "2025-08-18 14:33:45,295 - pipeline.training - INFO - Starting training for smollm3-financial\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Training device: cpu\n",
      "\n",
      "🏋️ Training Models:\n",
      "============================================================\n",
      "⏭️ Skipping tinybert-financial-classifier (disabled)\n",
      "⏭️ Skipping finbert-tone (disabled)\n",
      "⏭️ Skipping distilbert-base (disabled)\n",
      "\n",
      "🤖 Training smollm3-financial:\n",
      "   🔧 Base model: HuggingFaceTB/SmolLM3-3B\n",
      "   📈 Train samples: 4361\n",
      "   📊 Validation samples: 485\n",
      "   📊 Optimized batch size: 1\n",
      "   🏷️ Labels: ['negative', 'neutral', 'positive']\n",
      "   🔍 Data validation:\n",
      "      📊 Train label distribution: {'neutral': 2591, 'positive': 1227, 'negative': 543}\n",
      "      📊 Val label distribution: {'neutral': 288, 'positive': 136, 'negative': 61}\n",
      "   🔧 Applying SmolLM3-specific optimizations...\n",
      "      📏 Using extended max_length: 64 for SmolLM3\n",
      "   🔧 Applying SmolLM3-specific optimizations...\n",
      "      📏 Using extended max_length: 64 for SmolLM3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf19f148948480bb0960a86ed972d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5ed76cdd044082aa1086bfccaa34ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe02737443643f9b10d343938dffa9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ab5f9672794bce80d0d9890a17dc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a328caaede48a8a94050af1091f220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SmolLM3ForSequenceClassification were not initialized from the model checkpoint at HuggingFaceTB/SmolLM3-3B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c462a42dd9b4df38b50822531da22fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4361 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5bb9e8d5d54cc1842bfea26dc4f5d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🔄 Tokenization completed\n",
      "      🏋️ Applying SmolLM3 training optimizations...\n",
      "      📊 Adjusted batch_size: 1, grad_accum: 16\n",
      "      📈 Adjusted learning_rate: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/929kwd6500bc9dk55s_br5l40000gn/T/ipykernel_94448/4183182356.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "2025-08-18 14:42:10,220 - pipeline.training - ERROR - Failed to train smollm3-financial: MPS backend out of memory (MPS allocated: 8.98 GB, other allocations: 384.00 KB, max allowed: 9.07 GB). Tried to allocate 86.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "2025-08-18 14:42:10,322 - pipeline.training - INFO - Training completed: 0 models trained\n",
      "2025-08-18 14:42:10,220 - pipeline.training - ERROR - Failed to train smollm3-financial: MPS backend out of memory (MPS allocated: 8.98 GB, other allocations: 384.00 KB, max allowed: 9.07 GB). Tried to allocate 86.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "2025-08-18 14:42:10,322 - pipeline.training - INFO - Training completed: 0 models trained\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ❌ Training failed: MPS backend out of memory (MPS allocated: 8.98 GB, other allocations: 384.00 KB, max allowed: 9.07 GB). Tried to allocate 86.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "\n",
      "============================================================\n",
      "🎉 Training completed! 0/1 models trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Main training loop for all configured models\n",
    "logger.info(\"🚀 Starting model training loop...\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🔧 Training device: {device}\")\n",
    "\n",
    "trained_models = {}\n",
    "training_results = {}\n",
    "\n",
    "print(f\"\\n🏋️ Training Models:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Get base models from configuration\n",
    "base_models = models_config.get('base_models', [])\n",
    "\n",
    "for model_config in base_models:\n",
    "    if not model_config.get('enabled', True):\n",
    "        print(f\"⏭️ Skipping {model_config['name']} (disabled)\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        model_name = model_config['name']\n",
    "        model_id = model_config['model_id']\n",
    "        \n",
    "        print(f\"\\n🤖 Training {model_name}:\")\n",
    "        logger.info(f\"Starting training for {model_name}\")\n",
    "        \n",
    "        print(f\"   🔧 Base model: {model_id}\")\n",
    "        print(f\"   📈 Train samples: {len(train_df)}\")\n",
    "        print(f\"   📊 Validation samples: {len(val_df)}\")\n",
    "        print(f\"   📊 Optimised batch size: {training_config.get('batch_size', 16)}\")\n",
    "        \n",
    "        # Create label mapping\n",
    "        unique_labels = sorted(train_df['label'].unique())\n",
    "        label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        id2label = {idx: label for label, idx in label2id.items()}\n",
    "        \n",
    "        print(f\"   🏷️ Labels: {unique_labels}\")\n",
    "        \n",
    "        # Data validation\n",
    "        print(f\"   🔍 Data validation:\")\n",
    "        print(f\"      📊 Train label distribution: {dict(train_df['label'].value_counts())}\")\n",
    "        print(f\"      📊 Val label distribution: {dict(val_df['label'].value_counts())}\")\n",
    "        \n",
    "        # Check for any problematic labels\n",
    "        train_unknown = set(train_df['label'].unique()) - set(unique_labels)\n",
    "        val_unknown = set(val_df['label'].unique()) - set(unique_labels)\n",
    "        \n",
    "        if train_unknown:\n",
    "            logger.warning(f\"Unknown labels in train set: {train_unknown}\")\n",
    "        if val_unknown:\n",
    "            logger.warning(f\"Unknown labels in validation set: {val_unknown}\")\n",
    "            print(f\"      ⚠️ Unknown validation labels: {val_unknown}\")\n",
    "        \n",
    "        # Check for null values\n",
    "        train_nulls = train_df['label'].isnull().sum()\n",
    "        val_nulls = val_df['label'].isnull().sum()\n",
    "        \n",
    "        if train_nulls > 0:\n",
    "            print(f\"      ❌ {train_nulls} null labels in train set\")\n",
    "        if val_nulls > 0:\n",
    "            print(f\"      ❌ {val_nulls} null labels in validation set\")\n",
    "        \n",
    "        # Load tokeniser and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_id,\n",
    "            num_labels=len(unique_labels),\n",
    "            label2id=label2id,\n",
    "            id2label=id2label\n",
    "        )\n",
    "        \n",
    "        # Prepare datasets with optimised tokenisation\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(\n",
    "                examples['text'], \n",
    "                truncation=True, \n",
    "                padding=False,  # Disable padding here - let DataCollator handle it\n",
    "                max_length=training_config.get('max_length', 128),  # Use optimised max length\n",
    "                return_tensors=None\n",
    "            )\n",
    "        \n",
    "        def prepare_dataset(df):\n",
    "            # Convert labels to ids\n",
    "            df_processed = df.copy()\n",
    "            \n",
    "            # Check for missing labels before mapping\n",
    "            missing_labels = set(df_processed['label'].unique()) - set(label2id.keys())\n",
    "            if missing_labels:\n",
    "                logger.warning(f\"Found labels not in training set: {missing_labels}\")\n",
    "                print(f\"   ⚠️ Warning: Unknown labels found: {missing_labels}\")\n",
    "                # Filter out rows with unknown labels\n",
    "                df_processed = df_processed[df_processed['label'].isin(label2id.keys())]\n",
    "                print(f\"   📊 Dataset size after filtering: {len(df_processed)} samples\")\n",
    "            \n",
    "            # Map labels to ids\n",
    "            df_processed['labels'] = df_processed['label'].map(label2id)\n",
    "            \n",
    "            # Check for any remaining NaN values\n",
    "            nan_count = df_processed['labels'].isna().sum()\n",
    "            if nan_count > 0:\n",
    "                logger.error(f\"Found {nan_count} NaN values in labels after mapping\")\n",
    "                print(f\"   ❌ Error: {nan_count} NaN values in labels\")\n",
    "                # Drop rows with NaN labels\n",
    "                df_processed = df_processed.dropna(subset=['labels'])\n",
    "                print(f\"   📊 Dataset size after dropping NaN: {len(df_processed)} samples\")\n",
    "            \n",
    "            # Ensure labels are integers and finite\n",
    "            df_processed['labels'] = df_processed['labels'].astype(int)\n",
    "            \n",
    "            # Validate no infinite values\n",
    "            if not np.isfinite(df_processed['labels']).all():\n",
    "                logger.error(\"Found non-finite values in labels\")\n",
    "                raise ValueError(\"Labels contain non-finite values\")\n",
    "            \n",
    "            # Create HuggingFace dataset\n",
    "            dataset = Dataset.from_pandas(df_processed[['text', 'labels']])\n",
    "            dataset = dataset.map(\n",
    "                tokenize_function, \n",
    "                batched=True, \n",
    "                remove_columns=['text'],  # Remove original text column\n",
    "                num_proc=1  # Single process to avoid multiprocessing overhead\n",
    "            )\n",
    "            \n",
    "            return dataset\n",
    "        \n",
    "        train_dataset = prepare_dataset(train_df)\n",
    "        val_dataset = prepare_dataset(val_df)\n",
    "        \n",
    "        print(f\"   🔄 Tokenisation completed\")\n",
    "        \n",
    "        # Apply model-specific training configuration\n",
    "        model_specific_key = f'{model_name}_config'\n",
    "        model_specific = training_config.get(model_specific_key, {})\n",
    "        \n",
    "        # Use model-specific settings if available, otherwise use defaults\n",
    "        model_batch_size = model_specific.get('batch_size', training_config.get('batch_size', 4))\n",
    "        model_learning_rate = model_specific.get('learning_rate', training_config.get('learning_rate', 2e-5))\n",
    "        model_epochs = model_specific.get('num_epochs', training_config.get('num_epochs', 3))\n",
    "        model_grad_accum = model_specific.get('gradient_accumulation_steps', training_config.get('gradient_accumulation_steps', 1))\n",
    "        \n",
    "        print(f\"   🎯 Model-specific settings:\")\n",
    "        print(f\"      📊 Batch size: {model_batch_size}\")\n",
    "        print(f\"      📈 Learning rate: {model_learning_rate}\")\n",
    "        print(f\"      🔄 Epochs: {model_epochs}\")\n",
    "        print(f\"      🔄 Gradient accumulation: {model_grad_accum}\")\n",
    "        \n",
    "        # Training arguments with model-specific optimisations\n",
    "        output_dir = Path(f\"../models/{model_name}\")\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(output_dir),\n",
    "            num_train_epochs=model_epochs,\n",
    "            per_device_train_batch_size=model_batch_size,\n",
    "            per_device_eval_batch_size=model_batch_size,\n",
    "            gradient_accumulation_steps=model_grad_accum,\n",
    "            warmup_steps=training_config.get('warmup_steps', 50),\n",
    "            weight_decay=training_config.get('weight_decay', 0.01),\n",
    "            learning_rate=model_learning_rate,\n",
    "            logging_dir=str(output_dir / 'logs'),\n",
    "            logging_steps=training_config.get('logging_steps', 25),\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            report_to=None,  # Disable wandb/tensorboard\n",
    "            dataloader_num_workers=0,  # Avoid multiprocessing issues\n",
    "            save_total_limit=1,  # Save space\n",
    "            fp16=training_config.get('fp16', False),\n",
    "            dataloader_pin_memory=False,  # Reduce memory overhead\n",
    "        )\n",
    "        \n",
    "        # Data collator - handles dynamic padding and tensor conversion\n",
    "        data_collator = DataCollatorWithPadding(\n",
    "            tokenizer=tokenizer,\n",
    "            padding=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Metrics function\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.argmax(predictions, axis=1)\n",
    "            \n",
    "            accuracy = accuracy_score(labels, predictions)\n",
    "            \n",
    "            return {\n",
    "                'accuracy': accuracy,\n",
    "            }\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        print(f\"   🏋️ Starting training... (estimated time reduced with optimisations)\")\n",
    "        \n",
    "        # Train model\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Evaluate model\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        print(f\"   ✅ Training completed!\")\n",
    "        print(f\"   📊 Final validation accuracy: {eval_result['eval_accuracy']:.4f}\")\n",
    "        print(f\"   📈 Final validation loss: {eval_result['eval_loss']:.4f}\")\n",
    "        \n",
    "        # Save model and tokeniser\n",
    "        trainer.save_model()\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        # Save label mapping\n",
    "        import pickle\n",
    "        with open(output_dir / 'label_encoder.pkl', 'wb') as f:\n",
    "            pickle.dump({'label2id': label2id, 'id2label': id2label}, f)\n",
    "        \n",
    "        # Store results\n",
    "        trained_models[model_name] = {\n",
    "            'model_path': str(output_dir),\n",
    "            'base_model': model_id,\n",
    "            'labels': unique_labels,\n",
    "            'label_mapping': {'label2id': label2id, 'id2label': id2label}\n",
    "        }\n",
    "        \n",
    "        training_results[model_name] = {\n",
    "            'train_loss': train_result.training_loss,\n",
    "            'eval_loss': eval_result['eval_loss'],\n",
    "            'eval_accuracy': eval_result['eval_accuracy'],\n",
    "            'train_runtime': train_result.metrics.get('train_runtime', 0),\n",
    "            'samples_per_second': train_result.metrics.get('train_samples_per_second', 0)\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Successfully trained {model_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to train {model_name}: {str(e)}\")\n",
    "        print(f\"   ❌ Training failed: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"🎉 Training completed! {len(trained_models)}/{len(enabled_models)} models trained successfully\")\n",
    "\n",
    "logger.info(f\"Training completed: {len(trained_models)} models trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 14:42:11,958 - pipeline.training - INFO - 💾 Saving training results and updating pipeline state...\n",
      "2025-08-18 14:42:11,976 - pipeline.training - INFO - ✅ Model training completed successfully\n",
      "2025-08-18 14:42:11,976 - pipeline.training - INFO - ✅ Model training completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🎉 MODEL TRAINING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "📝 Next Steps:\n",
      "1. Run 3_convert_to_onnx.ipynb to convert models to ONNX\n",
      "2. Run 4_benchmarks.ipynb to benchmark performance\n",
      "3. Continue with the sequential pipeline: 5 → 6\n",
      "\n",
      "🏋️ Training Summary:\n",
      "   🤖 Models trained: 0\n",
      "\n",
      "📄 Training report saved to: ../results/training_report.json\n",
      "📊 Results visualization saved to: ../results/training_results_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# Save training results and complete training step\n",
    "logger.info(\"💾 Saving training results and updating pipeline state...\")\n",
    "\n",
    "# Create comprehensive training summary\n",
    "training_summary = {\n",
    "    'training_timestamp': datetime.now().isoformat(),\n",
    "    'models_trained': list(trained_models.keys()),\n",
    "    'training_config': training_config,\n",
    "    'results': training_results,\n",
    "    'model_details': trained_models\n",
    "}\n",
    "\n",
    "# Update pipeline state\n",
    "state.mark_step_complete('model_training_completed', **training_summary)\n",
    "\n",
    "# Save training report\n",
    "results_dir = Path(\"../results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(results_dir / 'training_report.json', 'w') as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "# Create training results visualisation\n",
    "if len(training_results) > 0:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Subplot 1: Training Loss\n",
    "    plt.subplot(1, 3, 1)\n",
    "    model_names = list(training_results.keys())\n",
    "    train_losses = [training_results[m]['train_loss'] for m in model_names]\n",
    "    plt.bar(model_names, train_losses)\n",
    "    plt.title('Training Loss by Model')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Subplot 2: Validation Loss\n",
    "    plt.subplot(1, 3, 2)\n",
    "    eval_losses = [training_results[m]['eval_loss'] for m in model_names]\n",
    "    plt.bar(model_names, eval_losses)\n",
    "    plt.title('Validation Loss by Model')\n",
    "    plt.ylabel('Validation Loss')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Subplot 3: Validation Accuracy\n",
    "    plt.subplot(1, 3, 3)\n",
    "    eval_accuracies = [training_results[m]['eval_accuracy'] for m in model_names]\n",
    "    plt.bar(model_names, eval_accuracies)\n",
    "    plt.title('Validation Accuracy by Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'training_results_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"🎉 MODEL TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"📝 Next Steps:\")\n",
    "print(\"1. Run 3_convert_to_onnx_generalised.ipynb to convert models to ONNX\")\n",
    "print(\"2. Run 4_benchmarks_generalised.ipynb to benchmark performance\")\n",
    "print(\"3. Continue with the sequential pipeline: 5 → 6\")\n",
    "\n",
    "print(f\"\\n🏋️ Training Summary:\")\n",
    "print(f\"   🤖 Models trained: {len(trained_models)}\")\n",
    "\n",
    "for model_name, results in training_results.items():\n",
    "    print(f\"   📊 {model_name}:\")\n",
    "    print(f\"      🎯 Validation accuracy: {results['eval_accuracy']:.4f}\")\n",
    "    print(f\"      📉 Validation loss: {results['eval_loss']:.4f}\")\n",
    "    print(f\"      📁 Model saved to: {trained_models[model_name]['model_path']}\")\n",
    "\n",
    "print(f\"\\n📄 Training report saved to: {results_dir / 'training_report.json'}\")\n",
    "print(f\"📊 Results visualisation saved to: {results_dir / 'training_results_comparison.png'}\")\n",
    "\n",
    "logger.info(\"✅ Model training completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
