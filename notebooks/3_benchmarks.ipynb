{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMillward2012/deepmind_internship/blob/main/notebooks/7_benchmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/matthew/Documents/deepmind_internship\n",
            "README.md        \u001b[34mmodels\u001b[m\u001b[m           \u001b[34mresults\u001b[m\u001b[m\n",
            "\u001b[34mdata\u001b[m\u001b[m             \u001b[34mnotebooks\u001b[m\u001b[m        \u001b[34msrc\u001b[m\u001b[m\n",
            "\u001b[34mfigures\u001b[m\u001b[m          requirements.txt \u001b[34mvenv-py311\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "RTwrJ4dSirfW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
        "from transformers.onnx import export\n",
        "from transformers.onnx.features import FeaturesManager\n",
        "import onnxruntime as ort\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\"models\")\n",
        "ONNX_OPSET = 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_valid_model_dir(d):\n",
        "    return (d / \"config.json\").exists() and ((d / \"pytorch_model.bin\").exists() or (d / \"model.safetensors\").exists())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found valid models: ['all-MiniLM-L6-v2-financial-sentiment', 'distilbert-financial-sentiment', 'finbert-tone-financial-sentiment', 'SmolLM2-360M-Instruct-financial-sentiment', 'tinybert-financial-classifier', 'mobilebert-uncased-financial-sentiment']\n"
          ]
        }
      ],
      "source": [
        "model_dirs = [d for d in BASE_DIR.iterdir() if d.is_dir() and is_valid_model_dir(d)]\n",
        "print(\"Found valid models:\", [m.name for m in model_dirs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ONNXExportWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Call model with return_dict=False to get a tuple output\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
        "        # Return only the logits tensor (usually first element)\n",
        "        return outputs[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_to_onnx(model_dir, onnx_path):\n",
        "    print(\"ðŸ” Loading model and tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    model.eval()\n",
        "\n",
        "    wrapped_model = ONNXExportWrapper(model)  # Wrap the model here\n",
        "\n",
        "    dummy_input = tokenizer(\"This company is doing great!\", return_tensors=\"pt\")\n",
        "\n",
        "    print(\"ðŸš€ Exporting to ONNX...\")\n",
        "    torch.onnx.export(\n",
        "        wrapped_model,\n",
        "        (dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
        "        str(onnx_path),\n",
        "        input_names=[\"input_ids\", \"attention_mask\"],\n",
        "        output_names=[\"output\"],\n",
        "        dynamic_axes={\n",
        "            \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "            \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "            \"output\": {0: \"batch_size\"},\n",
        "        },\n",
        "        opset_version=17,  # Use >=14 due to scaled_dot_product_attention operator support\n",
        "        do_constant_folding=True,\n",
        "    )\n",
        "    print(f\"âœ… Exported to {onnx_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "â³ Processing all-MiniLM-L6-v2-financial-sentiment...\n",
            "âœ… ONNX already exists.\n",
            "\n",
            "â³ Processing distilbert-financial-sentiment...\n",
            "âœ… ONNX already exists.\n",
            "\n",
            "â³ Processing finbert-tone-financial-sentiment...\n",
            "ðŸ“¦ Exporting to ONNX...\n",
            "ðŸ” Loading model and tokenizer...\n",
            "ðŸš€ Exporting to ONNX...\n",
            "âœ… Exported to models/finbert-tone-financial-sentiment/onnx/model.onnx\n",
            "\n",
            "â³ Processing SmolLM2-360M-Instruct-financial-sentiment...\n",
            "âœ… ONNX already exists.\n",
            "\n",
            "â³ Processing tinybert-financial-classifier...\n",
            "âœ… ONNX already exists.\n",
            "\n",
            "â³ Processing mobilebert-uncased-financial-sentiment...\n",
            "âœ… ONNX already exists.\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for model_dir in model_dirs:\n",
        "    print(f\"\\nâ³ Processing {model_dir.name}...\")\n",
        "    \n",
        "    onnx_dir = model_dir / \"onnx\"\n",
        "    onnx_dir.mkdir(exist_ok=True)\n",
        "    onnx_model_path = onnx_dir / \"model.onnx\"\n",
        "    quantised_model_path = onnx_dir / \"model-int8.onnx\"\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "    # Export ONNX if not already done\n",
        "    if not onnx_model_path.exists():\n",
        "        print(\"ðŸ“¦ Exporting to ONNX...\")\n",
        "        export_to_onnx(model_dir, onnx_model_path)\n",
        "    else:\n",
        "        print(\"âœ… ONNX already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SKIP] No ONNX model found for .DS_Store at expected path: models/.DS_Store/onnx/model.onnx\n",
            "[PROCESSING] Quantizing model 'all-MiniLM-L6-v2-financial-sentiment'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] Saved quantized model: models/all-MiniLM-L6-v2-financial-sentiment/onnx/model_quantized.onnx\n",
            "[PROCESSING] Quantizing model 'distilbert-financial-sentiment'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] Saved quantized model: models/distilbert-financial-sentiment/onnx/model_quantized.onnx\n",
            "[PROCESSING] Quantizing model 'finbert-tone-financial-sentiment'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] Saved quantized model: models/finbert-tone-financial-sentiment/onnx/model_quantized.onnx\n",
            "[SKIP] No ONNX model found for .gitkeep at expected path: models/.gitkeep/onnx/model.onnx\n",
            "[PROCESSING] Quantizing model 'SmolLM2-360M-Instruct-financial-sentiment'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] Saved quantized model: models/SmolLM2-360M-Instruct-financial-sentiment/onnx/model_quantized.onnx\n",
            "[PROCESSING] Quantizing model 'tinybert-financial-classifier'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] Saved quantized model: models/tinybert-financial-classifier/onnx/model_quantized.onnx\n",
            "[PROCESSING] Quantizing model 'mobilebert-uncased-financial-sentiment'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/model/mobilebert/embeddings/Slice_2_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/model/mobilebert/embeddings/Slice_4_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] Saved quantized model: models/mobilebert-uncased-financial-sentiment/onnx/model_quantized.onnx\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "models_dir = \"models\"  # root directory containing model subfolders\n",
        "\n",
        "def quantize_all_models(models_root):\n",
        "    for model_name in os.listdir(models_root):\n",
        "        model_path = os.path.join(models_root, model_name, \"onnx\", \"model.onnx\")\n",
        "        \n",
        "        if not os.path.isfile(model_path):\n",
        "            print(f\"[SKIP] No ONNX model found for {model_name} at expected path: {model_path}\")\n",
        "            continue\n",
        "        \n",
        "        quantized_model_path = os.path.join(models_root, model_name, \"onnx\", \"model_quantized.onnx\")\n",
        "        print(f\"[PROCESSING] Quantizing model '{model_name}'\")\n",
        "        \n",
        "        try:\n",
        "            quantize_dynamic(\n",
        "                model_input=model_path,\n",
        "                model_output=quantized_model_path,\n",
        "                weight_type=QuantType.QInt8\n",
        "            )\n",
        "            print(f\"[SUCCESS] Saved quantized model: {quantized_model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Failed to quantize {model_name}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    quantize_all_models(models_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import onnxruntime as ort\n",
        "import psutil\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "EXAMPLE_INPUT = \"Stocks surged after the company reported record earnings.\"\n",
        "MAX_LENGTH = 128\n",
        "BENCHMARK_ITERATIONS = 100\n",
        "\n",
        "def benchmark_onnx_model(onnx_path, tokenizer):\n",
        "    # Load ONNX model session\n",
        "    sess = ort.InferenceSession(str(onnx_path), providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "    # Measure memory usage after session creation\n",
        "    process = psutil.Process()\n",
        "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "    # Prepare input tokens\n",
        "    inputs = tokenizer(EXAMPLE_INPUT, return_tensors=\"np\", max_length=MAX_LENGTH, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # Warm-up\n",
        "    for _ in range(10):\n",
        "        sess.run(None, {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]})\n",
        "\n",
        "    # Measure latency over multiple iterations\n",
        "    times = []\n",
        "    for _ in range(BENCHMARK_ITERATIONS):\n",
        "        start = time.time()\n",
        "        sess.run(None, {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]})\n",
        "        times.append((time.time() - start) * 1000)  # milliseconds\n",
        "\n",
        "    avg_latency = sum(times) / len(times)\n",
        "    p99_latency = sorted(times)[int(len(times) * 0.99) - 1]\n",
        "\n",
        "    # Calculate throughput: predictions per second (using avg latency)\n",
        "    throughput = 1000 / avg_latency\n",
        "\n",
        "    # Model size in MB\n",
        "    model_size_mb = onnx_path.stat().st_size / (1024 * 1024)\n",
        "\n",
        "    return {\n",
        "        \"avg_latency_ms\": avg_latency,\n",
        "        \"p99_latency_ms\": p99_latency,\n",
        "        \"memory_mb\": memory_mb,\n",
        "        \"model_size_mb\": model_size_mb,\n",
        "        \"throughput_preds_per_sec\": throughput\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benchmarking model: all-MiniLM-L6-v2-financial-sentiment\n",
            "Benchmarking model: distilbert-financial-sentiment\n",
            "Benchmarking model: finbert-tone-financial-sentiment\n",
            "Benchmarking model: SmolLM2-360M-Instruct-financial-sentiment\n",
            "Benchmarking model: tinybert-financial-classifier\n",
            "Benchmarking model: mobilebert-uncased-financial-sentiment\n",
            "   avg_latency_ms  p99_latency_ms    memory_mb  model_size_mb  \\\n",
            "0        9.539635       14.684200   929.234375      13.909289   \n",
            "1       18.591957       26.837826   684.359375      21.980877   \n",
            "2       55.933518       66.138983   930.906250      25.459671   \n",
            "3       60.804486       65.684080   710.593750      64.228925   \n",
            "4      121.273766      125.671864   737.656250     105.492896   \n",
            "5      446.809196      501.713276  1150.609375     347.381740   \n",
            "\n",
            "   throughput_preds_per_sec                                      model  \n",
            "0                104.825812              tinybert-financial-classifier  \n",
            "1                 53.786699       all-MiniLM-L6-v2-financial-sentiment  \n",
            "2                 17.878368     mobilebert-uncased-financial-sentiment  \n",
            "3                 16.446155             distilbert-financial-sentiment  \n",
            "4                  8.245806           finbert-tone-financial-sentiment  \n",
            "5                  2.238092  SmolLM2-360M-Instruct-financial-sentiment  \n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for model_dir in BASE_DIR.iterdir():\n",
        "    if not model_dir.is_dir() or model_dir.name == \".gitkeep\":\n",
        "        continue\n",
        "\n",
        "    onnx_path = model_dir / \"onnx\" / \"model_quantized.onnx\"\n",
        "    if onnx_path.exists():\n",
        "        print(f\"Benchmarking model: {model_dir.name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "        result = benchmark_onnx_model(onnx_path, tokenizer)\n",
        "        result[\"model\"] = model_dir.name\n",
        "        results.append(result)\n",
        "    else:\n",
        "        print(f\"ONNX model not found for {model_dir.name}\")\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df = df.sort_values(\"avg_latency_ms\").reset_index(drop=True)\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import onnxruntime as ort\n",
        "import psutil\n",
        "from transformers import AutoTokenizer\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "EXAMPLE_INPUT = \"Stocks surged after the company reported record earnings.\"\n",
        "MAX_LENGTH = 128\n",
        "BENCHMARK_ITERATIONS = 100\n",
        "WARMUP_ITERATIONS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available ONNX Runtime Providers: ['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n",
            "Primary provider: CoreMLExecutionProvider\n",
            "System Memory: 8.0 GB\n",
            "CPU Count: 8\n",
            "Note: Using reduced logging to minimize output noise\n",
            "\n",
            "ðŸ” Benchmarking all-MiniLM-L6-v2-financial-sentiment...\n",
            "  Batch size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 58.9ms avg latency (CoreMLExecutionProvider)\n",
            "  Batch size: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;31m2025-07-23 15:08:56.509337 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running 6255025888846516209_CoreML_6255025888846516209_4 node. Name:'CoreMLExecutionProvider_6255025888846516209_CoreML_6255025888846516209_4_4' Status Message: Error executing model: Unable to compute the prediction using a neural network model. It can be an invalid input data or broken/unsupported model (error code: -1).\u001b[m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    âš ï¸  CoreML runtime error, retrying with CPU...\n",
            "    ðŸ”„ Retry 1 with CPUExecutionProvider...\n",
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 49.1ms avg latency (CPUExecutionProvider)\n",
            "  Batch size: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;31m2025-07-23 15:09:03.860607 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running 6255025888846516209_CoreML_6255025888846516209_4 node. Name:'CoreMLExecutionProvider_6255025888846516209_CoreML_6255025888846516209_4_4' Status Message: Error executing model: Unable to compute the prediction using a neural network model. It can be an invalid input data or broken/unsupported model (error code: -1).\u001b[m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    âš ï¸  CoreML runtime error, retrying with CPU...\n",
            "    ðŸ”„ Retry 1 with CPUExecutionProvider...\n",
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 103.3ms avg latency (CPUExecutionProvider)\n",
            "âœ… Completed all-MiniLM-L6-v2-financial-sentiment\n",
            "\n",
            "ðŸ” Benchmarking distilbert-financial-sentiment...\n",
            "  Batch size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 144.7ms avg latency (CoreMLExecutionProvider)\n",
            "  Batch size: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;31m2025-07-23 15:09:37.744211 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running 10752758088624767045_CoreML_10752758088624767045_5 node. Name:'CoreMLExecutionProvider_10752758088624767045_CoreML_10752758088624767045_5_5' Status Message: Error executing model: Unable to compute the prediction using a neural network model. It can be an invalid input data or broken/unsupported model (error code: -1).\u001b[m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    âš ï¸  CoreML runtime error, retrying with CPU...\n",
            "    ðŸ”„ Retry 1 with CPUExecutionProvider...\n",
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 184.4ms avg latency (CPUExecutionProvider)\n",
            "  Batch size: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;31m2025-07-23 15:10:02.399882 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running 10752758088624767045_CoreML_10752758088624767045_5 node. Name:'CoreMLExecutionProvider_10752758088624767045_CoreML_10752758088624767045_5_5' Status Message: Error executing model: Unable to compute the prediction using a neural network model. It can be an invalid input data or broken/unsupported model (error code: -1).\u001b[m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    âš ï¸  CoreML runtime error, retrying with CPU...\n",
            "    ðŸ”„ Retry 1 with CPUExecutionProvider...\n",
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 375.0ms avg latency (CPUExecutionProvider)\n",
            "âœ… Completed distilbert-financial-sentiment\n",
            "\n",
            "ðŸ” Benchmarking finbert-tone-financial-sentiment...\n",
            "  Batch size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 293.4ms avg latency (CoreMLExecutionProvider)\n",
            "  Batch size: 2\n",
            "    âš ï¸  CoreML runtime error, retrying with CPU...\n",
            "    ðŸ”„ Retry 1 with CPUExecutionProvider...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;31m2025-07-23 15:11:30.950055 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running 17604349712411484192_CoreML_17604349712411484192_4 node. Name:'CoreMLExecutionProvider_17604349712411484192_CoreML_17604349712411484192_4_4' Status Message: Error executing model: Unable to compute the prediction using a neural network model. It can be an invalid input data or broken/unsupported model (error code: -1).\u001b[m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 371.5ms avg latency (CPUExecutionProvider)\n",
            "  Batch size: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;31m2025-07-23 15:12:19.440777 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running 17604349712411484192_CoreML_17604349712411484192_4 node. Name:'CoreMLExecutionProvider_17604349712411484192_CoreML_17604349712411484192_4_4' Status Message: Error executing model: Unable to compute the prediction using a neural network model. It can be an invalid input data or broken/unsupported model (error code: -1).\u001b[m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    âš ï¸  CoreML runtime error, retrying with CPU...\n",
            "    ðŸ”„ Retry 1 with CPUExecutionProvider...\n",
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 754.2ms avg latency (CPUExecutionProvider)\n",
            "âœ… Completed finbert-tone-financial-sentiment\n",
            "\n",
            "ðŸ” Benchmarking SmolLM2-360M-Instruct-financial-sentiment...\n",
            "  Batch size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 1876.3ms avg latency (CoreMLExecutionProvider)\n",
            "  Batch size: 2\n",
            "    âš ï¸  CoreML runtime error, retrying with CPU...\n",
            "    ðŸ”„ Retry 1 with CPUExecutionProvider...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;31m2025-07-23 15:17:55.187716 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running 9273022787927528322_CoreML_9273022787927528322_4 node. Name:'CoreMLExecutionProvider_9273022787927528322_CoreML_9273022787927528322_4_4' Status Message: Error executing model: Unable to compute the prediction using a neural network model. It can be an invalid input data or broken/unsupported model (error code: -1).\u001b[m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 1521.9ms avg latency (CPUExecutionProvider)\n",
            "  Batch size: 4\n",
            "    âš ï¸  CoreML runtime error, retrying with CPU...\n",
            "    ðŸ”„ Retry 1 with CPUExecutionProvider...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;31m2025-07-23 15:21:11.138469 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running 9273022787927528322_CoreML_9273022787927528322_4 node. Name:'CoreMLExecutionProvider_9273022787927528322_CoreML_9273022787927528322_4_4' Status Message: Error executing model: Unable to compute the prediction using a neural network model. It can be an invalid input data or broken/unsupported model (error code: -1).\u001b[m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 2934.4ms avg latency (CPUExecutionProvider)\n",
            "âœ… Completed SmolLM2-360M-Instruct-financial-sentiment\n",
            "\n",
            "ðŸ” Benchmarking tinybert-financial-classifier...\n",
            "  Batch size: 1\n",
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 41.9ms avg latency (CoreMLExecutionProvider)\n",
            "  Batch size: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;31m2025-07-23 15:27:26.859409 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running 14308768299282785986_CoreML_14308768299282785986_4 node. Name:'CoreMLExecutionProvider_14308768299282785986_CoreML_14308768299282785986_4_4' Status Message: Error executing model: Unable to compute the prediction using a neural network model. It can be an invalid input data or broken/unsupported model (error code: -1).\u001b[m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    âš ï¸  CoreML runtime error, retrying with CPU...\n",
            "    ðŸ”„ Retry 1 with CPUExecutionProvider...\n",
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 22.9ms avg latency (CPUExecutionProvider)\n",
            "  Batch size: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;31m2025-07-23 15:27:30.786944 [E:onnxruntime:, sequential_executor.cc:572 ExecuteKernel] Non-zero status code returned while running 14308768299282785986_CoreML_14308768299282785986_4 node. Name:'CoreMLExecutionProvider_14308768299282785986_CoreML_14308768299282785986_4_4' Status Message: Error executing model: Unable to compute the prediction using a neural network model. It can be an invalid input data or broken/unsupported model (error code: -1).\u001b[m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    âš ï¸  CoreML runtime error, retrying with CPU...\n",
            "    ðŸ”„ Retry 1 with CPUExecutionProvider...\n",
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 45.4ms avg latency (CPUExecutionProvider)\n",
            "âœ… Completed tinybert-financial-classifier\n",
            "\n",
            "ðŸ” Benchmarking mobilebert-uncased-financial-sentiment...\n",
            "  Batch size: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 192.9ms avg latency (CoreMLExecutionProvider)\n",
            "  Batch size: 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 299.0ms avg latency (CoreMLExecutionProvider)\n",
            "  Batch size: 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n",
            "Context leak detected, msgtracer returned -1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Warming up for 20 iterations...\n",
            "  Running 100 benchmark iterations...\n",
            "    âœ… 430.0ms avg latency (CoreMLExecutionProvider)\n",
            "âœ… Completed mobilebert-uncased-financial-sentiment\n",
            "\n",
            "\n",
            "================================================================================\n",
            "BENCHMARK SUMMARY\n",
            "================================================================================\n",
            "                                    model  batch_size  avg_latency_ms  p99_latency_ms  throughput_samples_per_sec  memory_delta_mb                provider\n",
            "SmolLM2-360M-Instruct-financial-sentiment           1         1876.28         3174.23                        0.53           229.77 CoreMLExecutionProvider\n",
            "SmolLM2-360M-Instruct-financial-sentiment           2         1521.88         3976.48                        1.31          -811.50    CPUExecutionProvider\n",
            "SmolLM2-360M-Instruct-financial-sentiment           4         2934.42         3952.32                        1.36           691.36    CPUExecutionProvider\n",
            "     all-MiniLM-L6-v2-financial-sentiment           1           58.85          168.29                       16.99            98.23 CoreMLExecutionProvider\n",
            "     all-MiniLM-L6-v2-financial-sentiment           2           49.11           61.52                       40.73          -121.41    CPUExecutionProvider\n",
            "     all-MiniLM-L6-v2-financial-sentiment           4          103.33          136.76                       38.71          -151.31    CPUExecutionProvider\n",
            "           distilbert-financial-sentiment           1          144.70          178.86                        6.91           -92.14 CoreMLExecutionProvider\n",
            "           distilbert-financial-sentiment           2          184.36          240.59                       10.85          -114.92    CPUExecutionProvider\n",
            "           distilbert-financial-sentiment           4          375.04          500.44                       10.67          -378.70    CPUExecutionProvider\n",
            "         finbert-tone-financial-sentiment           1          293.40          433.77                        3.41           179.55 CoreMLExecutionProvider\n",
            "         finbert-tone-financial-sentiment           2          371.52          455.52                        5.38           183.17    CPUExecutionProvider\n",
            "         finbert-tone-financial-sentiment           4          754.17         1128.99                        5.30          -349.77    CPUExecutionProvider\n",
            "   mobilebert-uncased-financial-sentiment           1          192.86          314.68                        5.19          -162.52 CoreMLExecutionProvider\n",
            "   mobilebert-uncased-financial-sentiment           2          298.95          565.73                        6.69           -51.83 CoreMLExecutionProvider\n",
            "   mobilebert-uncased-financial-sentiment           4          430.02          496.15                        9.30           875.42 CoreMLExecutionProvider\n",
            "            tinybert-financial-classifier           1           41.91           66.61                       23.86           179.55 CoreMLExecutionProvider\n",
            "            tinybert-financial-classifier           2           22.94           37.27                       87.20             4.55    CPUExecutionProvider\n",
            "            tinybert-financial-classifier           4           45.39           55.63                       88.13             4.36    CPUExecutionProvider\n",
            "\n",
            "================================================================================\n",
            "TOP PERFORMERS\n",
            "================================================================================\n",
            "\n",
            "Batch Size 1:\n",
            "  Fastest: tinybert-financial-classifier (41.91ms)\n",
            "  Highest Throughput: tinybert-financial-classifier (23.86 samples/sec)\n",
            "\n",
            "Batch Size 2:\n",
            "  Fastest: tinybert-financial-classifier (22.94ms)\n",
            "  Highest Throughput: tinybert-financial-classifier (87.20 samples/sec)\n",
            "\n",
            "Batch Size 4:\n",
            "  Fastest: tinybert-financial-classifier (45.39ms)\n",
            "  Highest Throughput: tinybert-financial-classifier (88.13 samples/sec)\n",
            "\n",
            "ðŸ“Š Successfully benchmarked 6 models\n",
            "ðŸ“ Results: 18 total configurations tested\n",
            "ðŸ“ Detailed results saved to 'benchmark_results/' directory\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import gc\n",
        "import statistics\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "import onnxruntime as ort\n",
        "import psutil\n",
        "from transformers import AutoTokenizer\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Configuration\n",
        "EXAMPLE_INPUTS = [\n",
        "    \"Stocks surged after the company reported record earnings.\",\n",
        "    \"The weather forecast predicts heavy rain throughout the weekend.\",\n",
        "    \"Scientists have discovered a new species of deep-sea creature.\",\n",
        "    \"Technology companies are investing heavily in artificial intelligence research.\",\n",
        "    \"The local community center will host a charity fundraising event next month.\"\n",
        "]\n",
        "MAX_LENGTH = 128\n",
        "BENCHMARK_ITERATIONS = 100\n",
        "WARMUP_ITERATIONS = 20\n",
        "BATCH_SIZES = [1, 2, 4]  # Reduced batch sizes for better compatibility\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkResult:\n",
        "    model: str\n",
        "    batch_size: int\n",
        "    avg_latency_ms: float\n",
        "    p50_latency_ms: float\n",
        "    p95_latency_ms: float\n",
        "    p99_latency_ms: float\n",
        "    std_latency_ms: float\n",
        "    min_latency_ms: float\n",
        "    max_latency_ms: float\n",
        "    memory_delta_mb: float\n",
        "    peak_memory_mb: float\n",
        "    model_size_mb: float\n",
        "    throughput_samples_per_sec: float\n",
        "    tokens_per_sec: float\n",
        "    cpu_utilization_avg: float\n",
        "    gpu_available: bool\n",
        "    provider: str\n",
        "    session_creation_time_ms: float\n",
        "\n",
        "@contextmanager\n",
        "def cpu_monitor():\n",
        "    \"\"\"Monitor CPU usage during execution\"\"\"\n",
        "    process = psutil.Process()\n",
        "    cpu_percentages = []\n",
        "    \n",
        "    def sample_cpu():\n",
        "        cpu_percentages.append(process.cpu_percent())\n",
        "    \n",
        "    # Initial sample\n",
        "    sample_cpu()\n",
        "    yield cpu_percentages\n",
        "    # Final sample\n",
        "    sample_cpu()\n",
        "\n",
        "import platform\n",
        "import onnxruntime as ort\n",
        "\n",
        "def get_optimal_providers(fallback_to_cpu=False):\n",
        "    available_providers = ort.get_available_providers()\n",
        "\n",
        "    if fallback_to_cpu:\n",
        "        return [\"CPUExecutionProvider\"]\n",
        "\n",
        "    system = platform.system()\n",
        "    if system == \"Darwin\":\n",
        "        # For M1 Mac, prefer CPU, then CoreML if available\n",
        "        preferred_providers = [\n",
        "            \"CPUExecutionProvider\",\n",
        "            \"CoreMLExecutionProvider\"\n",
        "        ]\n",
        "    else:\n",
        "        # For Windows/Linux, prefer GPU providers first\n",
        "        preferred_providers = [\n",
        "            \"CUDAExecutionProvider\",\n",
        "            \"ROCMExecutionProvider\",\n",
        "            \"OpenVINOExecutionProvider\",\n",
        "            \"CPUExecutionProvider\"\n",
        "        ]\n",
        "        if \"CoreMLExecutionProvider\" in available_providers:\n",
        "            preferred_providers.insert(-1, \"CoreMLExecutionProvider\")\n",
        "\n",
        "    # Filter to only those available\n",
        "    final_providers = [p for p in preferred_providers if p in available_providers]\n",
        "\n",
        "    if final_providers:\n",
        "        return final_providers\n",
        "\n",
        "    return [\"CPUExecutionProvider\"]\n",
        "\n",
        "def load_onnx_session(onnx_path: Path, providers: Optional[List[str]] = None, enable_logging: bool = False) -> Tuple[ort.InferenceSession, float]:\n",
        "    \"\"\"Load ONNX session with timing and error handling\"\"\"\n",
        "    if providers is None:\n",
        "        providers = get_optimal_providers()\n",
        "    \n",
        "    start_time = time.perf_counter()\n",
        "    \n",
        "    session_options = ort.SessionOptions()\n",
        "    session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "    session_options.enable_mem_pattern = True\n",
        "    session_options.enable_cpu_mem_arena = True\n",
        "    \n",
        "    # Reduce verbose logging\n",
        "    if not enable_logging:\n",
        "        session_options.log_severity_level = 3  # Only show errors\n",
        "    \n",
        "    try:\n",
        "        session = ort.InferenceSession(\n",
        "            str(onnx_path), \n",
        "            providers=providers,\n",
        "            sess_options=session_options\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # If CoreML fails, fallback to CPU\n",
        "        if \"CoreML\" in str(providers[0]):\n",
        "            print(f\"    âš ï¸  CoreML failed, falling back to CPU: {str(e)[:100]}...\")\n",
        "            providers = [\"CPUExecutionProvider\"]\n",
        "            session = ort.InferenceSession(\n",
        "                str(onnx_path), \n",
        "                providers=providers,\n",
        "                sess_options=session_options\n",
        "            )\n",
        "        else:\n",
        "            raise e\n",
        "    \n",
        "    creation_time = (time.perf_counter() - start_time) * 1000\n",
        "    return session, creation_time\n",
        "\n",
        "def measure_memory_usage() -> float:\n",
        "    \"\"\"Get current memory usage in MB\"\"\"\n",
        "    process = psutil.Process()\n",
        "    return process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "def prepare_batch_inputs(tokenizer, texts: List[str], max_length: int = MAX_LENGTH) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Prepare batched inputs for inference\"\"\"\n",
        "    encoded = tokenizer(\n",
        "        texts, \n",
        "        return_tensors=\"np\", \n",
        "        max_length=max_length, \n",
        "        padding=\"max_length\", \n",
        "        truncation=True\n",
        "    )\n",
        "    return {\n",
        "        \"input_ids\": encoded[\"input_ids\"].astype(np.int64),\n",
        "        \"attention_mask\": encoded[\"attention_mask\"].astype(np.int64)\n",
        "    }\n",
        "\n",
        "def warmup_session(session: ort.InferenceSession, inputs: Dict[str, np.ndarray], iterations: int = WARMUP_ITERATIONS):\n",
        "    \"\"\"Warmup the session with multiple iterations\"\"\"\n",
        "    print(f\"  Warming up for {iterations} iterations...\")\n",
        "    for i in range(iterations):\n",
        "        session.run(None, inputs)\n",
        "        if i % 5 == 0:  # Occasional garbage collection during warmup\n",
        "            gc.collect()\n",
        "\n",
        "def measure_latency_detailed(session: ort.InferenceSession, inputs: Dict[str, np.ndarray], \n",
        "                           iterations: int = BENCHMARK_ITERATIONS) -> Tuple[List[float], float]:\n",
        "    \"\"\"Measure latency with detailed statistics and CPU monitoring\"\"\"\n",
        "    times = []\n",
        "    \n",
        "    # Force garbage collection before benchmark\n",
        "    gc.collect()\n",
        "    \n",
        "    with cpu_monitor() as cpu_samples:\n",
        "        for i in range(iterations):\n",
        "            # Periodic garbage collection to avoid memory buildup\n",
        "            if i % 25 == 0 and i > 0:\n",
        "                gc.collect()\n",
        "            \n",
        "            start = time.perf_counter()\n",
        "            session.run(None, inputs)\n",
        "            end = time.perf_counter()\n",
        "            \n",
        "            times.append((end - start) * 1000)  # Convert to milliseconds\n",
        "            \n",
        "            # Sample CPU every 10 iterations\n",
        "            if i % 10 == 0:\n",
        "                cpu_samples.append(psutil.Process().cpu_percent())\n",
        "    \n",
        "    avg_cpu = statistics.mean(cpu_samples) if cpu_samples else 0.0\n",
        "    return times, avg_cpu\n",
        "\n",
        "def calculate_detailed_stats(times: List[float], batch_size: int, num_tokens: int) -> Dict[str, float]:\n",
        "    \"\"\"Calculate comprehensive latency statistics\"\"\"\n",
        "    times_sorted = sorted(times)\n",
        "    n = len(times)\n",
        "    \n",
        "    return {\n",
        "        'avg_latency_ms': statistics.mean(times),\n",
        "        'p50_latency_ms': times_sorted[n // 2],\n",
        "        'p95_latency_ms': times_sorted[int(n * 0.95)],\n",
        "        'p99_latency_ms': times_sorted[int(n * 0.99)] if n >= 100 else times_sorted[-1],\n",
        "        'std_latency_ms': statistics.stdev(times) if len(times) > 1 else 0.0,\n",
        "        'min_latency_ms': min(times),\n",
        "        'max_latency_ms': max(times),\n",
        "        'throughput_samples_per_sec': (1000 * batch_size) / statistics.mean(times),\n",
        "        'tokens_per_sec': (1000 * num_tokens) / statistics.mean(times)\n",
        "    }\n",
        "\n",
        "def get_model_size_mb(onnx_path: Path) -> float:\n",
        "    \"\"\"Get model size in megabytes\"\"\"\n",
        "    return onnx_path.stat().st_size / (1024 * 1024)\n",
        "\n",
        "def benchmark_onnx_model(onnx_path: Path, tokenizer, batch_size: int = 1, max_retries: int = 2) -> Optional[BenchmarkResult]:\n",
        "    \"\"\"Comprehensive benchmark of ONNX model with error handling\"\"\"\n",
        "    \n",
        "    for attempt in range(max_retries + 1):\n",
        "        try:\n",
        "            # Try different providers based on attempt\n",
        "            if attempt == 0:\n",
        "                providers = get_optimal_providers()\n",
        "            elif attempt == 1:\n",
        "                providers = [\"CPUExecutionProvider\"]  # Fallback to CPU\n",
        "            else:\n",
        "                print(f\"    âŒ All attempts failed for batch size {batch_size}\")\n",
        "                return None\n",
        "            \n",
        "            if attempt > 0:\n",
        "                print(f\"    ðŸ”„ Retry {attempt} with {providers[0]}...\")\n",
        "            \n",
        "            session, creation_time = load_onnx_session(onnx_path, providers)\n",
        "            \n",
        "            # Prepare batch inputs\n",
        "            test_texts = (EXAMPLE_INPUTS * ((batch_size // len(EXAMPLE_INPUTS)) + 1))[:batch_size]\n",
        "            inputs = prepare_batch_inputs(tokenizer, test_texts)\n",
        "            \n",
        "            # Test single inference first to catch runtime errors early\n",
        "            try:\n",
        "                session.run(None, inputs)\n",
        "            except Exception as e:\n",
        "                if \"CoreML\" in str(providers[0]) and attempt == 0:\n",
        "                    print(f\"    âš ï¸  CoreML runtime error, retrying with CPU...\")\n",
        "                    continue\n",
        "                else:\n",
        "                    raise e\n",
        "            \n",
        "            # Calculate total tokens for throughput calculation\n",
        "            total_tokens = inputs[\"input_ids\"].size\n",
        "            \n",
        "            # Memory measurement\n",
        "            memory_before = measure_memory_usage()\n",
        "            peak_memory = memory_before\n",
        "            \n",
        "            # Warmup\n",
        "            warmup_session(session, inputs)\n",
        "            \n",
        "            # Actual benchmarking\n",
        "            print(f\"  Running {BENCHMARK_ITERATIONS} benchmark iterations...\")\n",
        "            times, avg_cpu = measure_latency_detailed(session, inputs)\n",
        "            \n",
        "            memory_after = measure_memory_usage()\n",
        "            peak_memory = max(peak_memory, memory_after)\n",
        "            \n",
        "            # Calculate statistics\n",
        "            stats = calculate_detailed_stats(times, batch_size, total_tokens)\n",
        "            model_size_mb = get_model_size_mb(onnx_path)\n",
        "            \n",
        "            return BenchmarkResult(\n",
        "                model=onnx_path.parent.parent.name,\n",
        "                batch_size=batch_size,\n",
        "                memory_delta_mb=memory_after - memory_before,\n",
        "                peak_memory_mb=peak_memory,\n",
        "                model_size_mb=model_size_mb,\n",
        "                cpu_utilization_avg=avg_cpu,\n",
        "                gpu_available=\"CUDA\" in providers[0] or \"ROCM\" in providers[0],\n",
        "                provider=providers[0],\n",
        "                session_creation_time_ms=creation_time,\n",
        "                **stats\n",
        "            )\n",
        "            \n",
        "        except Exception as e:\n",
        "            if attempt < max_retries:\n",
        "                print(f\"    âš ï¸  Attempt {attempt + 1} failed: {str(e)[:100]}...\")\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"    âŒ Final attempt failed: {str(e)[:100]}...\")\n",
        "                return None\n",
        "    \n",
        "    return None\n",
        "\n",
        "def save_detailed_results(results: List[BenchmarkResult], output_dir: Path = Path(\"benchmark_results\")):\n",
        "    \"\"\"Save results with multiple formats and visualizations\"\"\"\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame([result.__dict__ for result in results])\n",
        "    \n",
        "    # Save raw results\n",
        "    df.to_csv(output_dir / \"benchmark_results.csv\", index=False)\n",
        "    df.to_json(output_dir / \"benchmark_results.json\", indent=2)\n",
        "    \n",
        "    # Create summary report\n",
        "    summary_cols = ['model', 'batch_size', 'avg_latency_ms', 'p99_latency_ms', \n",
        "                   'throughput_samples_per_sec', 'memory_delta_mb', 'provider']\n",
        "    summary_df = df[summary_cols].sort_values(['model', 'batch_size'])\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BENCHMARK SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(summary_df.to_string(index=False, float_format='%.2f'))\n",
        "    \n",
        "    # Best performer analysis\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"TOP PERFORMERS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for batch_size in df['batch_size'].unique():\n",
        "        batch_df = df[df['batch_size'] == batch_size]\n",
        "        fastest = batch_df.loc[batch_df['avg_latency_ms'].idxmin()]\n",
        "        highest_throughput = batch_df.loc[batch_df['throughput_samples_per_sec'].idxmax()]\n",
        "        \n",
        "        print(f\"\\nBatch Size {batch_size}:\")\n",
        "        print(f\"  Fastest: {fastest['model']} ({fastest['avg_latency_ms']:.2f}ms)\")\n",
        "        print(f\"  Highest Throughput: {highest_throughput['model']} ({highest_throughput['throughput_samples_per_sec']:.2f} samples/sec)\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main benchmarking function\"\"\"\n",
        "    base_dir = Path(\"models/\")  # Change this to your actual models folder\n",
        "    \n",
        "    if not base_dir.exists():\n",
        "        print(f\"Models directory '{base_dir}' not found!\")\n",
        "        return\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    print(\"Available ONNX Runtime Providers:\", ort.get_available_providers())\n",
        "    print(\"Primary provider:\", get_optimal_providers()[0])\n",
        "    print(f\"System Memory: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
        "    print(f\"CPU Count: {psutil.cpu_count()}\")\n",
        "    print(\"Note: Using reduced logging to minimize output noise\")\n",
        "    print()\n",
        "    \n",
        "    model_dirs = [d for d in base_dir.iterdir() if d.is_dir()]\n",
        "    \n",
        "    for model_dir in model_dirs:\n",
        "        onnx_path = model_dir / \"onnx\" / \"model.onnx\"\n",
        "        \n",
        "        if not onnx_path.exists():\n",
        "            print(f\"âš ï¸  ONNX model not found in {model_dir}\")\n",
        "            continue\n",
        "            \n",
        "        print(f\"ðŸ” Benchmarking {model_dir.name}...\")\n",
        "        \n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "            \n",
        "            # Test multiple batch sizes\n",
        "            for batch_size in BATCH_SIZES:\n",
        "                print(f\"  Batch size: {batch_size}\")\n",
        "                result = benchmark_onnx_model(onnx_path, tokenizer, batch_size)\n",
        "                \n",
        "                if result is not None:\n",
        "                    results.append(result)\n",
        "                    print(f\"    âœ… {result.avg_latency_ms:.1f}ms avg latency ({result.provider})\")\n",
        "                else:\n",
        "                    print(f\"    âŒ Failed all attempts for batch size {batch_size}\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error with model {model_dir.name}: {e}\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"âœ… Completed {model_dir.name}\\n\")\n",
        "    \n",
        "    if results:\n",
        "        save_detailed_results(results)\n",
        "        print(f\"\\nðŸ“Š Successfully benchmarked {len(set(r.model for r in results))} models\")\n",
        "        print(f\"ðŸ“ Results: {len(results)} total configurations tested\")\n",
        "        print(\"ðŸ“ Detailed results saved to 'benchmark_results/' directory\")\n",
        "    else:\n",
        "        print(\"âŒ No models were successfully benchmarked!\")\n",
        "        print(\"ðŸ’¡ Try running with CPU-only by modifying get_optimal_providers() to return ['CPUExecutionProvider']\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPEaWosSVJk00KG/SJV/J7B",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
