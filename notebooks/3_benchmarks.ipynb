{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMillward2012/deepmind_internship/blob/main/notebooks/7_benchmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/matthew/Documents/deepmind_internship\n",
            "README.md         \u001b[34mfigures\u001b[m\u001b[m           requirements.txt  \u001b[34mvenv-py311\u001b[m\u001b[m\n",
            "\u001b[34mbenchmark_results\u001b[m\u001b[m \u001b[34mmodels\u001b[m\u001b[m            \u001b[34mresults\u001b[m\u001b[m\n",
            "\u001b[34mdata\u001b[m\u001b[m              \u001b[34mnotebooks\u001b[m\u001b[m         \u001b[34msrc\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RTwrJ4dSirfW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
        "from transformers.onnx import export\n",
        "from transformers.onnx.features import FeaturesManager\n",
        "import onnxruntime as ort\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\"models\")\n",
        "ONNX_OPSET = 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_valid_model_dir(d):\n",
        "    return (d / \"config.json\").exists() and ((d / \"pytorch_model.bin\").exists() or (d / \"model.safetensors\").exists())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found valid models: ['all-MiniLM-L6-v2-financial-sentiment', 'distilbert-financial-sentiment', 'finbert-tone-financial-sentiment', 'SmolLM2-360M-Instruct-financial-sentiment', 'tinybert-financial-classifier', 'mobilebert-uncased-financial-sentiment']\n"
          ]
        }
      ],
      "source": [
        "model_dirs = [d for d in BASE_DIR.iterdir() if d.is_dir() and is_valid_model_dir(d)]\n",
        "print(\"Found valid models:\", [m.name for m in model_dirs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ONNXExportWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Call model with return_dict=False to get a tuple output\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
        "        # Return only the logits tensor (usually first element)\n",
        "        return outputs[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_to_onnx(model_dir, onnx_path):\n",
        "    print(\"ðŸ” Loading model and tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    model.eval()\n",
        "\n",
        "    wrapped_model = ONNXExportWrapper(model)  # Wrap the model here\n",
        "\n",
        "    dummy_input = tokenizer(\"This company is doing great!\", return_tensors=\"pt\")\n",
        "\n",
        "    print(\"ðŸš€ Exporting to ONNX...\")\n",
        "    torch.onnx.export(\n",
        "        wrapped_model,\n",
        "        (dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
        "        str(onnx_path),\n",
        "        input_names=[\"input_ids\", \"attention_mask\"],\n",
        "        output_names=[\"output\"],\n",
        "        dynamic_axes={\n",
        "            \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "            \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "            \"output\": {0: \"batch_size\"},\n",
        "        },\n",
        "        opset_version=17,  # Use >=14 due to scaled_dot_product_attention operator support\n",
        "        do_constant_folding=True,\n",
        "    )\n",
        "    print(f\"âœ… Exported to {onnx_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "â³ Processing all-MiniLM-L6-v2-financial-sentiment...\n",
            "âœ… ONNX already exists.\n",
            "\n",
            "â³ Processing distilbert-financial-sentiment...\n",
            "âœ… ONNX already exists.\n",
            "\n",
            "â³ Processing finbert-tone-financial-sentiment...\n",
            "âœ… ONNX already exists.\n",
            "\n",
            "â³ Processing SmolLM2-360M-Instruct-financial-sentiment...\n",
            "âœ… ONNX already exists.\n",
            "\n",
            "â³ Processing tinybert-financial-classifier...\n",
            "âœ… ONNX already exists.\n",
            "\n",
            "â³ Processing mobilebert-uncased-financial-sentiment...\n",
            "âœ… ONNX already exists.\n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for model_dir in model_dirs:\n",
        "    print(f\"\\nâ³ Processing {model_dir.name}...\")\n",
        "    \n",
        "    onnx_dir = model_dir / \"onnx\"\n",
        "    onnx_dir.mkdir(exist_ok=True)\n",
        "    onnx_model_path = onnx_dir / \"model.onnx\"\n",
        "    quantised_model_path = onnx_dir / \"model-int8.onnx\"\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "    # Export ONNX if not already done\n",
        "    if not onnx_model_path.exists():\n",
        "        print(\"ðŸ“¦ Exporting to ONNX...\")\n",
        "        export_to_onnx(model_dir, onnx_model_path)\n",
        "    else:\n",
        "        print(\"âœ… ONNX already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SKIP] No ONNX model found for .DS_Store at expected path: models/.DS_Store/onnx/model.onnx\n",
            "[PROCESSING] Quantizing model 'all-MiniLM-L6-v2-financial-sentiment'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] Saved quantized model: models/all-MiniLM-L6-v2-financial-sentiment/onnx/model_quantized.onnx\n",
            "[PROCESSING] Quantizing model 'distilbert-financial-sentiment'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] Saved quantized model: models/distilbert-financial-sentiment/onnx/model_quantized.onnx\n",
            "[PROCESSING] Quantizing model 'finbert-tone-financial-sentiment'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] Saved quantized model: models/finbert-tone-financial-sentiment/onnx/model_quantized.onnx\n",
            "[SKIP] No ONNX model found for .gitkeep at expected path: models/.gitkeep/onnx/model.onnx\n",
            "[PROCESSING] Quantizing model 'SmolLM2-360M-Instruct-financial-sentiment'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] Saved quantized model: models/SmolLM2-360M-Instruct-financial-sentiment/onnx/model_quantized.onnx\n",
            "[PROCESSING] Quantizing model 'tinybert-financial-classifier'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] Saved quantized model: models/tinybert-financial-classifier/onnx/model_quantized.onnx\n",
            "[PROCESSING] Quantizing model 'mobilebert-uncased-financial-sentiment'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/model/mobilebert/embeddings/Slice_2_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/model/mobilebert/embeddings/Slice_4_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SUCCESS] Saved quantized model: models/mobilebert-uncased-financial-sentiment/onnx/model_quantized.onnx\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "models_dir = \"models\"  # root directory containing model subfolders\n",
        "\n",
        "def quantize_all_models(models_root):\n",
        "    for model_name in os.listdir(models_root):\n",
        "        model_path = os.path.join(models_root, model_name, \"onnx\", \"model.onnx\")\n",
        "        \n",
        "        if not os.path.isfile(model_path):\n",
        "            print(f\"[SKIP] No ONNX model found for {model_name} at expected path: {model_path}\")\n",
        "            continue\n",
        "        \n",
        "        quantized_model_path = os.path.join(models_root, model_name, \"onnx\", \"model_quantized.onnx\")\n",
        "        print(f\"[PROCESSING] Quantizing model '{model_name}'\")\n",
        "        \n",
        "        try:\n",
        "            quantize_dynamic(\n",
        "                model_input=model_path,\n",
        "                model_output=quantized_model_path,\n",
        "                weight_type=QuantType.QInt8\n",
        "            )\n",
        "            print(f\"[SUCCESS] Saved quantized model: {quantized_model_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Failed to quantize {model_name}: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    quantize_all_models(models_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import onnxruntime as ort\n",
        "import psutil\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "EXAMPLE_INPUT = \"Stocks surged after the company reported record earnings.\"\n",
        "MAX_LENGTH = 128\n",
        "BENCHMARK_ITERATIONS = 100\n",
        "\n",
        "def benchmark_onnx_model(onnx_path, tokenizer):\n",
        "    # Load ONNX model session\n",
        "    sess = ort.InferenceSession(str(onnx_path), providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "    # Measure memory usage after session creation\n",
        "    process = psutil.Process()\n",
        "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "    # Prepare input tokens\n",
        "    inputs = tokenizer(EXAMPLE_INPUT, return_tensors=\"np\", max_length=MAX_LENGTH, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # Warm-up\n",
        "    for _ in range(10):\n",
        "        sess.run(None, {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]})\n",
        "\n",
        "    # Measure latency over multiple iterations\n",
        "    times = []\n",
        "    for _ in range(BENCHMARK_ITERATIONS):\n",
        "        start = time.time()\n",
        "        sess.run(None, {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]})\n",
        "        times.append((time.time() - start) * 1000)  # milliseconds\n",
        "\n",
        "    avg_latency = sum(times) / len(times)\n",
        "    p99_latency = sorted(times)[int(len(times) * 0.99) - 1]\n",
        "\n",
        "    # Calculate throughput: predictions per second (using avg latency)\n",
        "    throughput = 1000 / avg_latency\n",
        "\n",
        "    # Model size in MB\n",
        "    model_size_mb = onnx_path.stat().st_size / (1024 * 1024)\n",
        "\n",
        "    return {\n",
        "        \"avg_latency_ms\": avg_latency,\n",
        "        \"p99_latency_ms\": p99_latency,\n",
        "        \"memory_mb\": memory_mb,\n",
        "        \"model_size_mb\": model_size_mb,\n",
        "        \"throughput_preds_per_sec\": throughput\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benchmarking model: all-MiniLM-L6-v2-financial-sentiment\n",
            "Benchmarking model: distilbert-financial-sentiment\n",
            "Benchmarking model: finbert-tone-financial-sentiment\n",
            "Benchmarking model: SmolLM2-360M-Instruct-financial-sentiment\n",
            "Benchmarking model: tinybert-financial-classifier\n",
            "Benchmarking model: mobilebert-uncased-financial-sentiment\n",
            "   avg_latency_ms  p99_latency_ms    memory_mb  model_size_mb  \\\n",
            "0        9.503901       15.094757   930.500000      13.909289   \n",
            "1       27.855840       46.684980   520.015625      21.980877   \n",
            "2       55.561740       63.303947   941.390625      25.459671   \n",
            "3       73.846185       98.513842   593.937500      64.228925   \n",
            "4      159.092190      243.785858   686.375000     105.492896   \n",
            "5      446.434224      538.583994  1168.546875     347.381740   \n",
            "\n",
            "   throughput_preds_per_sec                                      model  \n",
            "0                105.219951              tinybert-financial-classifier  \n",
            "1                 35.899115       all-MiniLM-L6-v2-financial-sentiment  \n",
            "2                 17.997996     mobilebert-uncased-financial-sentiment  \n",
            "3                 13.541661             distilbert-financial-sentiment  \n",
            "4                  6.285664           finbert-tone-financial-sentiment  \n",
            "5                  2.239972  SmolLM2-360M-Instruct-financial-sentiment  \n"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for model_dir in BASE_DIR.iterdir():\n",
        "    if not model_dir.is_dir() or model_dir.name == \".gitkeep\":\n",
        "        continue\n",
        "\n",
        "    onnx_path = model_dir / \"onnx\" / \"model_quantized.onnx\"\n",
        "    if onnx_path.exists():\n",
        "        print(f\"Benchmarking model: {model_dir.name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "        result = benchmark_onnx_model(onnx_path, tokenizer)\n",
        "        result[\"model\"] = model_dir.name\n",
        "        results.append(result)\n",
        "    else:\n",
        "        print(f\"ONNX model not found for {model_dir.name}\")\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df = df.sort_values(\"avg_latency_ms\").reset_index(drop=True)\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Discovering models...\n",
            "  âœ“ all-MiniLM-L6-v2-financial-sentiment: models/all-MiniLM-L6-v2-financial-sentiment/onnx/model_quantized.onnx\n",
            "  âœ“ distilbert-financial-sentiment: models/distilbert-financial-sentiment/onnx/model_quantized.onnx\n",
            "  âœ“ finbert-tone-financial-sentiment: models/finbert-tone-financial-sentiment/onnx/model_quantized.onnx\n",
            "  âœ“ SmolLM2-360M-Instruct-financial-sentiment: models/SmolLM2-360M-Instruct-financial-sentiment/onnx/model_quantized.onnx\n",
            "  âœ“ tinybert-financial-classifier: models/tinybert-financial-classifier/onnx/model_quantized.onnx\n",
            "  âœ“ mobilebert-uncased-financial-sentiment: models/mobilebert-uncased-financial-sentiment/onnx/model_quantized.onnx\n",
            "\n",
            "Found 6 models. Running full benchmark...\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import gc\n",
        "import statistics\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "import onnxruntime as ort\n",
        "import psutil\n",
        "from transformers import AutoTokenizer\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import platform\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkConfig:\n",
        "    \"\"\"Configuration for benchmarking\"\"\"\n",
        "    max_length: int = 128\n",
        "    benchmark_iterations: int = 100\n",
        "    warmup_iterations: int = 20\n",
        "    batch_sizes: List[int] = None\n",
        "    test_csv_path: Optional[str] = None\n",
        "    device_mode: str = \"auto\"  # auto, cpu, gpu, coreml\n",
        "    enable_logging: bool = False\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        if self.batch_sizes is None:\n",
        "            self.batch_sizes = [1, 2, 4]\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkResult:\n",
        "    \"\"\"Results from benchmarking a model\"\"\"\n",
        "    model: str\n",
        "    batch_size: int\n",
        "    avg_latency_ms: float\n",
        "    p50_latency_ms: float\n",
        "    p95_latency_ms: float\n",
        "    p99_latency_ms: float\n",
        "    std_latency_ms: float\n",
        "    min_latency_ms: float\n",
        "    max_latency_ms: float\n",
        "    memory_delta_mb: float\n",
        "    peak_memory_mb: float\n",
        "    model_size_mb: float\n",
        "    throughput_samples_per_sec: float\n",
        "    tokens_per_sec: float\n",
        "    cpu_utilization_avg: float\n",
        "    gpu_available: bool\n",
        "    provider: str\n",
        "    session_creation_time_ms: float\n",
        "    accuracy: Optional[float] = None\n",
        "    f1_score: Optional[float] = None\n",
        "    \n",
        "    def to_dict(self) -> Dict:\n",
        "        return asdict(self)\n",
        "\n",
        "class ExecutionProviderManager:\n",
        "    \"\"\"Manages ONNX execution providers based on platform and preferences\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_available_providers() -> List[str]:\n",
        "        return ort.get_available_providers()\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_execution_providers(mode: str = \"auto\") -> List[str]:\n",
        "        available = ort.get_available_providers()\n",
        "        \n",
        "        if mode == \"cpu\":\n",
        "            return [\"CPUExecutionProvider\"]\n",
        "        elif mode == \"gpu\":\n",
        "            gpu_providers = [\"CUDAExecutionProvider\", \"ROCMExecutionProvider\"]\n",
        "            for provider in gpu_providers:\n",
        "                if provider in available:\n",
        "                    return [provider]\n",
        "            logger.warning(\"No GPU providers available, falling back to CPU\")\n",
        "            return [\"CPUExecutionProvider\"]\n",
        "        elif mode == \"coreml\":\n",
        "            if \"CoreMLExecutionProvider\" in available:\n",
        "                return [\"CoreMLExecutionProvider\"]\n",
        "            logger.warning(\"CoreML not available, falling back to CPU\")\n",
        "            return [\"CPUExecutionProvider\"]\n",
        "        else:  # auto mode\n",
        "            system = platform.system()\n",
        "            if system == \"Darwin\":\n",
        "                # For macOS, prioritize CPU over CoreML as you mentioned\n",
        "                return [\"CPUExecutionProvider\"]\n",
        "            else:\n",
        "                # For other systems, prefer GPU if available\n",
        "                preferences = [\n",
        "                    \"CUDAExecutionProvider\", \n",
        "                    \"ROCMExecutionProvider\", \n",
        "                    \"OpenVINOExecutionProvider\", \n",
        "                    \"CPUExecutionProvider\"\n",
        "                ]\n",
        "                return [p for p in preferences if p in available]\n",
        "\n",
        "class ModelLoader:\n",
        "    \"\"\"Handles ONNX model loading and session creation\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def load_onnx_session(\n",
        "        onnx_path: Path, \n",
        "        providers: List[str], \n",
        "        enable_logging: bool = False\n",
        "    ) -> Tuple[ort.InferenceSession, float]:\n",
        "        start = time.perf_counter()\n",
        "        \n",
        "        opts = ort.SessionOptions()\n",
        "        opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "        opts.enable_mem_pattern = True\n",
        "        opts.enable_cpu_mem_arena = True\n",
        "        \n",
        "        if not enable_logging:\n",
        "            opts.log_severity_level = 3\n",
        "        \n",
        "        try:\n",
        "            session = ort.InferenceSession(str(onnx_path), providers=providers, sess_options=opts)\n",
        "            creation_time = (time.perf_counter() - start) * 1000\n",
        "            \n",
        "            # Verify the session was created with expected provider\n",
        "            actual_providers = session.get_providers()\n",
        "            logger.info(f\"Session created with providers: {actual_providers}\")\n",
        "            \n",
        "            return session, creation_time\n",
        "            \n",
        "        except Exception as e:\n",
        "            # Fallback for provider errors\n",
        "            if any(\"CoreML\" in p for p in providers):\n",
        "                logger.warning(f\"CoreML fallback error, switching to CPU: {e}\")\n",
        "                providers = [\"CPUExecutionProvider\"]\n",
        "                session = ort.InferenceSession(str(onnx_path), providers=providers, sess_options=opts)\n",
        "                creation_time = (time.perf_counter() - start) * 1000\n",
        "                return session, creation_time\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "class DataProcessor:\n",
        "    \"\"\"Handles data preprocessing and batch preparation\"\"\"\n",
        "    \n",
        "    def __init__(self, tokenizer, max_length: int = 128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        # Example inputs for benchmarking\n",
        "        self.example_inputs = [\n",
        "            \"Stocks surged after the company reported record earnings.\",\n",
        "            \"The weather forecast predicts heavy rain throughout the weekend.\", \n",
        "            \"Scientists have discovered a new species of deep-sea creature.\",\n",
        "            \"Technology companies are investing heavily in artificial intelligence research.\",\n",
        "            \"The local community center will host a charity fundraising event next month.\"\n",
        "        ]\n",
        "    \n",
        "    def prepare_batch_inputs(self, texts: List[str]) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Prepare batch inputs for ONNX model\"\"\"\n",
        "        encoding = self.tokenizer(\n",
        "            texts,\n",
        "            return_tensors=\"np\",\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].astype(np.int64),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].astype(np.int64)\n",
        "        }\n",
        "    \n",
        "    def get_benchmark_texts(self, batch_size: int) -> List[str]:\n",
        "        \"\"\"Get texts for benchmarking with specified batch size\"\"\"\n",
        "        multiplier = (batch_size // len(self.example_inputs)) + 1\n",
        "        texts = (self.example_inputs * multiplier)[:batch_size]\n",
        "        return texts\n",
        "    \n",
        "    def load_test_dataset(self, csv_path: Path) -> Tuple[List[str], List[int]]:\n",
        "        \"\"\"Load test dataset for accuracy evaluation\"\"\"\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path, names=[\"label\", \"text\"], encoding=\"latin1\")\n",
        "            \n",
        "            # Handle different label formats\n",
        "            if df[\"label\"].dtype == \"object\":\n",
        "                # String labels - map to integers\n",
        "                unique_labels = df[\"label\"].str.strip().unique()\n",
        "                if set(unique_labels).issubset({\"positive\", \"neutral\", \"negative\"}):\n",
        "                    label_map = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n",
        "                    df[\"label\"] = df[\"label\"].str.strip().map(label_map)\n",
        "                else:\n",
        "                    # Create mapping for any string labels\n",
        "                    label_map = {label: idx for idx, label in enumerate(sorted(unique_labels))}\n",
        "                    df[\"label\"] = df[\"label\"].str.strip().map(label_map)\n",
        "            \n",
        "            # Drop any rows with missing labels\n",
        "            df = df.dropna(subset=[\"label\"])\n",
        "            \n",
        "            # Split dataset\n",
        "            _, test_df = train_test_split(\n",
        "                df, \n",
        "                test_size=0.25, \n",
        "                random_state=42, \n",
        "                stratify=df[\"label\"]\n",
        "            )\n",
        "            \n",
        "            return test_df[\"text\"].tolist(), test_df[\"label\"].astype(int).tolist()\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading test dataset: {e}\")\n",
        "            raise\n",
        "\n",
        "class PerformanceMonitor:\n",
        "    \"\"\"Monitors system performance during benchmarking\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    @contextmanager\n",
        "    def cpu_monitor():\n",
        "        \"\"\"Context manager for monitoring CPU usage\"\"\"\n",
        "        process = psutil.Process()\n",
        "        cpu_samples = []\n",
        "        # Initial call to initialize measurement\n",
        "        process.cpu_percent(interval=None)\n",
        "        try:\n",
        "            yield cpu_samples\n",
        "        finally:\n",
        "            # Take final sample\n",
        "            final_sample = process.cpu_percent(interval=None)\n",
        "            if final_sample > 0:  # Only add if we got a valid reading\n",
        "                cpu_samples.append(final_sample)\n",
        "    \n",
        "    @staticmethod\n",
        "    def measure_memory_usage() -> float:\n",
        "        \"\"\"Get current memory usage in MB\"\"\"\n",
        "        return psutil.Process().memory_info().rss / (1024**2)\n",
        "    \n",
        "    @staticmethod\n",
        "    def get_model_size_mb(onnx_path: Path) -> float:\n",
        "        \"\"\"Get model file size in MB\"\"\"\n",
        "        return onnx_path.stat().st_size / (1024**2)\n",
        "\n",
        "class AccuracyEvaluator:\n",
        "    \"\"\"Handles model accuracy evaluation\"\"\"\n",
        "    \n",
        "    def __init__(self, session: ort.InferenceSession, data_processor: DataProcessor):\n",
        "        self.session = session\n",
        "        self.data_processor = data_processor\n",
        "    \n",
        "    def can_evaluate_classification(self) -> bool:\n",
        "        \"\"\"Check if the model can be evaluated for classification\"\"\"\n",
        "        output_names = [output.name for output in self.session.get_outputs()]\n",
        "        output_shapes = [output.shape for output in self.session.get_outputs()]\n",
        "        \n",
        "        # Check if we have logits output or single output that could be logits\n",
        "        has_logits = \"logits\" in output_names\n",
        "        has_single_output = len(output_names) == 1\n",
        "        \n",
        "        # Check if output shape suggests classification (batch_size, num_classes)\n",
        "        has_classification_shape = any(\n",
        "            len(shape) == 2 and (shape[1] is None or shape[1] > 1) \n",
        "            for shape in output_shapes if shape is not None\n",
        "        )\n",
        "        \n",
        "        return has_logits or (has_single_output and has_classification_shape)\n",
        "    \n",
        "    def evaluate_classification(\n",
        "        self, \n",
        "        texts: List[str], \n",
        "        labels: List[int]\n",
        "    ) -> Tuple[float, float]:\n",
        "        \"\"\"Evaluate classification accuracy and F1 score\"\"\"\n",
        "        if not self.can_evaluate_classification():\n",
        "            raise ValueError(\"Model doesn't appear to be a classification model\")\n",
        "        \n",
        "        # Prepare inputs\n",
        "        inputs = self.data_processor.prepare_batch_inputs(texts)\n",
        "        \n",
        "        # Run inference\n",
        "        outputs = self.session.run(None, inputs)\n",
        "        \n",
        "        # Get logits (assume first output contains logits)\n",
        "        logits = outputs[0]\n",
        "        \n",
        "        # Handle different output shapes\n",
        "        if len(logits.shape) == 3:\n",
        "            # If 3D, take the first token (CLS token for BERT-like models)\n",
        "            logits = logits[:, 0, :]\n",
        "        elif len(logits.shape) == 1:\n",
        "            # If 1D, reshape to 2D\n",
        "            logits = logits.reshape(1, -1)\n",
        "        \n",
        "        # Get predictions\n",
        "        predictions = np.argmax(logits, axis=1)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        f1 = f1_score(labels, predictions, average=\"weighted\")\n",
        "        \n",
        "        return accuracy, f1\n",
        "\n",
        "class LatencyBenchmarker:\n",
        "    \"\"\"Handles latency benchmarking\"\"\"\n",
        "    \n",
        "    def __init__(self, config: BenchmarkConfig):\n",
        "        self.config = config\n",
        "    \n",
        "    def warmup_session(self, session: ort.InferenceSession, inputs: Dict[str, np.ndarray]):\n",
        "        \"\"\"Warm up the session\"\"\"\n",
        "        logger.info(f\"Warming up for {self.config.warmup_iterations} iterations...\")\n",
        "        for i in range(self.config.warmup_iterations):\n",
        "            session.run(None, inputs)\n",
        "            if i % 5 == 0:\n",
        "                gc.collect()\n",
        "    \n",
        "    def measure_latency_and_cpu(\n",
        "        self, \n",
        "        session: ort.InferenceSession, \n",
        "        inputs: Dict[str, np.ndarray]\n",
        "    ) -> Tuple[List[float], float]:\n",
        "        \"\"\"Measure latency and CPU usage\"\"\"\n",
        "        times = []\n",
        "        gc.collect()\n",
        "        \n",
        "        with PerformanceMonitor.cpu_monitor() as cpu_samples:\n",
        "            for i in range(self.config.benchmark_iterations):\n",
        "                if i and i % 25 == 0:\n",
        "                    gc.collect()\n",
        "                \n",
        "                start_time = time.perf_counter()\n",
        "                session.run(None, inputs)\n",
        "                end_time = time.perf_counter()\n",
        "                \n",
        "                times.append((end_time - start_time) * 1000)\n",
        "                \n",
        "                # Sample CPU usage periodically\n",
        "                if i and i % 10 == 0:\n",
        "                    cpu_sample = psutil.Process().cpu_percent(interval=None)\n",
        "                    if cpu_sample > 0:\n",
        "                        cpu_samples.append(cpu_sample)\n",
        "        \n",
        "        avg_cpu = statistics.mean(cpu_samples) if cpu_samples else 0.0\n",
        "        return times, avg_cpu\n",
        "    \n",
        "    def calculate_stats(self, times: List[float], batch_size: int, total_tokens: int) -> Dict[str, float]:\n",
        "        \"\"\"Calculate latency statistics\"\"\"\n",
        "        sorted_times = sorted(times)\n",
        "        n = len(times)\n",
        "        mean_time = statistics.mean(times)\n",
        "        \n",
        "        return {\n",
        "            \"avg_latency_ms\": mean_time,\n",
        "            \"p50_latency_ms\": sorted_times[n // 2],\n",
        "            \"p95_latency_ms\": sorted_times[min(int(n * 0.95), n-1)],\n",
        "            \"p99_latency_ms\": sorted_times[min(int(n * 0.99), n-1)],\n",
        "            \"std_latency_ms\": statistics.stdev(times) if n > 1 else 0.0,\n",
        "            \"min_latency_ms\": sorted_times[0],\n",
        "            \"max_latency_ms\": sorted_times[-1],\n",
        "            \"throughput_samples_per_sec\": (1000 * batch_size) / mean_time,\n",
        "            \"tokens_per_sec\": (1000 * total_tokens) / mean_time\n",
        "        }\n",
        "\n",
        "class ONNXModelBenchmarker:\n",
        "    \"\"\"Main benchmarking class that orchestrates all components\"\"\"\n",
        "    \n",
        "    def __init__(self, config: BenchmarkConfig, tokenizer):\n",
        "        self.config = config\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data_processor = DataProcessor(tokenizer, config.max_length)\n",
        "        self.latency_benchmarker = LatencyBenchmarker(config)\n",
        "        self.provider_manager = ExecutionProviderManager()\n",
        "        \n",
        "    def benchmark_model(self, onnx_path: Path, batch_size: int) -> Optional[BenchmarkResult]:\n",
        "        \"\"\"Benchmark a single model with given batch size\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Benchmarking {onnx_path.name} with batch size {batch_size}\")\n",
        "            \n",
        "            # Get execution providers\n",
        "            providers = self.provider_manager.get_execution_providers(self.config.device_mode)\n",
        "            logger.info(f\"Using providers: {providers}\")\n",
        "            \n",
        "            # Load model\n",
        "            session, creation_time = ModelLoader.load_onnx_session(\n",
        "                onnx_path, providers, self.config.enable_logging\n",
        "            )\n",
        "            \n",
        "            # Prepare benchmark data\n",
        "            texts = self.data_processor.get_benchmark_texts(batch_size)\n",
        "            inputs = self.data_processor.prepare_batch_inputs(texts)\n",
        "            \n",
        "            # Initial test run\n",
        "            session.run(None, inputs)\n",
        "            \n",
        "            # Measure memory before benchmarking\n",
        "            memory_before = PerformanceMonitor.measure_memory_usage()\n",
        "            \n",
        "            # Warm up\n",
        "            self.latency_benchmarker.warmup_session(session, inputs)\n",
        "            \n",
        "            # Run benchmark\n",
        "            logger.info(f\"Running {self.config.benchmark_iterations} benchmark iterations...\")\n",
        "            times, cpu_avg = self.latency_benchmarker.measure_latency_and_cpu(session, inputs)\n",
        "            \n",
        "            # Measure memory after benchmarking\n",
        "            memory_after = PerformanceMonitor.measure_memory_usage()\n",
        "            \n",
        "            # Calculate performance stats\n",
        "            stats = self.latency_benchmarker.calculate_stats(\n",
        "                times, batch_size, inputs[\"input_ids\"].size\n",
        "            )\n",
        "            \n",
        "            # Get model info\n",
        "            model_size = PerformanceMonitor.get_model_size_mb(onnx_path)\n",
        "            gpu_available = any(\n",
        "                gpu in providers for gpu in [\"CUDAExecutionProvider\", \"ROCMExecutionProvider\"]\n",
        "            )\n",
        "            \n",
        "            # Evaluate accuracy if possible and dataset provided\n",
        "            accuracy = f1 = None\n",
        "            if self.config.test_csv_path:\n",
        "                try:\n",
        "                    evaluator = AccuracyEvaluator(session, self.data_processor)\n",
        "                    if evaluator.can_evaluate_classification():\n",
        "                        test_texts, test_labels = self.data_processor.load_test_dataset(\n",
        "                            Path(self.config.test_csv_path)\n",
        "                        )\n",
        "                        # Use subset for evaluation to match batch size\n",
        "                        eval_texts = test_texts[:batch_size]\n",
        "                        eval_labels = test_labels[:batch_size]\n",
        "                        accuracy, f1 = evaluator.evaluate_classification(eval_texts, eval_labels)\n",
        "                        logger.info(f\"Accuracy: {accuracy:.2%}, F1 score: {f1:.2%}\")\n",
        "                    else:\n",
        "                        logger.warning(\"Model doesn't appear to support classification evaluation\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Accuracy evaluation failed: {e}\")\n",
        "            \n",
        "            # Create result\n",
        "            result = BenchmarkResult(\n",
        "                model=onnx_path.parent.parent.name,  # Get model name from parent.parent since structure is models/modelName/onnx/model_quantized.onnx\n",
        "                batch_size=batch_size,\n",
        "                memory_delta_mb=memory_after - memory_before,\n",
        "                peak_memory_mb=max(memory_before, memory_after),\n",
        "                model_size_mb=model_size,\n",
        "                cpu_utilization_avg=cpu_avg,\n",
        "                gpu_available=gpu_available,\n",
        "                provider=providers[0],\n",
        "                session_creation_time_ms=creation_time,\n",
        "                accuracy=accuracy,\n",
        "                f1_score=f1,\n",
        "                **stats\n",
        "            )\n",
        "            \n",
        "            return result\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Benchmark failed for {onnx_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "class ResultsManager:\n",
        "    \"\"\"Manages benchmark results and reporting\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def save_results(results: List[BenchmarkResult], output_dir: Path = Path(\"results\")):\n",
        "        \"\"\"Save results to CSV and JSON files\"\"\"\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "        \n",
        "        # Convert to DataFrame\n",
        "        df = pd.DataFrame([result.to_dict() for result in results])\n",
        "        \n",
        "        # Save files\n",
        "        df.to_csv(output_dir / \"benchmark_results.csv\", index=False)\n",
        "        df.to_json(output_dir / \"benchmark_results.json\", indent=2)\n",
        "        \n",
        "        logger.info(f\"Results saved to {output_dir}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    @staticmethod\n",
        "    def print_summary(results: List[BenchmarkResult]):\n",
        "        \"\"\"Print benchmark summary\"\"\"\n",
        "        if not results:\n",
        "            logger.warning(\"No results to summarize\")\n",
        "            return\n",
        "        \n",
        "        df = pd.DataFrame([result.to_dict() for result in results])\n",
        "        \n",
        "        # Select columns for summary\n",
        "        summary_cols = [\n",
        "            \"model\", \"batch_size\", \"avg_latency_ms\", \"p99_latency_ms\",\n",
        "            \"throughput_samples_per_sec\", \"memory_delta_mb\", \"provider\"\n",
        "        ]\n",
        "        \n",
        "        # Add accuracy columns if available\n",
        "        if df[\"accuracy\"].notna().any():\n",
        "            summary_cols.extend([\"accuracy\", \"f1_score\"])\n",
        "        \n",
        "        summary = df[summary_cols].sort_values([\"model\", \"batch_size\"])\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" BENCHMARK SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "        print(summary.to_string(index=False, float_format=\"%.3f\"))\n",
        "        \n",
        "        # Print best performers by batch size\n",
        "        for batch_size in sorted(df[\"batch_size\"].unique()):\n",
        "            batch_df = df[df[\"batch_size\"] == batch_size]\n",
        "            fastest = batch_df.loc[batch_df[\"avg_latency_ms\"].idxmin()]\n",
        "            highest_throughput = batch_df.loc[batch_df[\"throughput_samples_per_sec\"].idxmax()]\n",
        "            \n",
        "            print(f\"\\nBatch Size {batch_size}:\")\n",
        "            print(f\"  Fastest: {fastest['model']} ({fastest['avg_latency_ms']:.1f} ms)\")\n",
        "            print(f\"  Highest throughput: {highest_throughput['model']} \"\n",
        "                  f\"({highest_throughput['throughput_samples_per_sec']:.1f} samples/s)\")\n",
        "\n",
        "# Convenience function to discover models in your directory structure\n",
        "def discover_models(models_dir: Union[str, Path]) -> List[Tuple[str, Path]]:\n",
        "    \"\"\"Discover all available models in the directory structure\"\"\"\n",
        "    models_dir = Path(models_dir)\n",
        "    discovered = []\n",
        "    \n",
        "    for model_dir in models_dir.glob(\"*\"):\n",
        "        if not model_dir.is_dir():\n",
        "            continue\n",
        "            \n",
        "        model_name = model_dir.name\n",
        "        onnx_path = model_dir / \"onnx\" / \"model_quantized.onnx\"\n",
        "        \n",
        "        if onnx_path.exists():\n",
        "            discovered.append((model_name, onnx_path))\n",
        "        else:\n",
        "            # Check for alternative ONNX files\n",
        "            onnx_dir = model_dir / \"onnx\"\n",
        "            if onnx_dir.exists():\n",
        "                onnx_files = list(onnx_dir.glob(\"*.onnx\"))\n",
        "                if onnx_files:\n",
        "                    discovered.append((model_name, onnx_files[0]))\n",
        "    \n",
        "    return discovered\n",
        "    \"\"\"Create a benchmark configuration with custom parameters\"\"\"\n",
        "    return BenchmarkConfig(**kwargs)\n",
        "\n",
        "# Convenience functions for notebook use\n",
        "def create_benchmark_config(**kwargs) -> BenchmarkConfig:\n",
        "    \"\"\"Create a benchmark configuration with custom parameters\"\"\"\n",
        "    return BenchmarkConfig(**kwargs)\n",
        "\n",
        "def run_single_benchmark(\n",
        "    onnx_path: Union[str, Path], \n",
        "    tokenizer, \n",
        "    batch_size: int = 1,\n",
        "    config: Optional[BenchmarkConfig] = None\n",
        ") -> Optional[BenchmarkResult]:\n",
        "    \"\"\"Run benchmark on a single model - convenient for notebook use\"\"\"\n",
        "    if config is None:\n",
        "        config = BenchmarkConfig()\n",
        "    \n",
        "    benchmarker = ONNXModelBenchmarker(config, tokenizer)\n",
        "    return benchmarker.benchmark_model(Path(onnx_path), batch_size)\n",
        "\n",
        "def run_full_benchmark(\n",
        "    models_dir: Union[str, Path],\n",
        "    tokenizer,\n",
        "    config: Optional[BenchmarkConfig] = None,\n",
        "    save_results: bool = True\n",
        ") -> List[BenchmarkResult]:\n",
        "    \"\"\"Run full benchmark suite - convenient for notebook use\"\"\"\n",
        "    if config is None:\n",
        "        config = BenchmarkConfig()\n",
        "    \n",
        "    models_dir = Path(models_dir)\n",
        "    benchmarker = ONNXModelBenchmarker(config, tokenizer)\n",
        "    \n",
        "    all_results = []\n",
        "    \n",
        "    # Look for models in the specific structure: models/___modelName___/onnx/model_quantized.onnx\n",
        "    model_dirs = [d for d in models_dir.glob(\"*\") if d.is_dir()]\n",
        "    \n",
        "    for model_dir in model_dirs:\n",
        "        model_name = model_dir.name\n",
        "        logger.info(f\"Looking for model: {model_name}\")\n",
        "        \n",
        "        # Check for the specific path structure\n",
        "        onnx_path = model_dir / \"onnx\" / \"model_quantized.onnx\"\n",
        "        \n",
        "        if not onnx_path.exists():\n",
        "            # Fallback: look for any .onnx files in onnx subdirectory\n",
        "            onnx_dir = model_dir / \"onnx\"\n",
        "            if onnx_dir.exists():\n",
        "                onnx_files = list(onnx_dir.glob(\"*.onnx\"))\n",
        "                if onnx_files:\n",
        "                    onnx_path = onnx_files[0]\n",
        "                    logger.info(f\"Found alternative ONNX file: {onnx_path.name}\")\n",
        "                else:\n",
        "                    logger.warning(f\"No ONNX files found in {onnx_dir}\")\n",
        "                    continue\n",
        "            else:\n",
        "                logger.warning(f\"No onnx directory found in {model_dir}\")\n",
        "                continue\n",
        "        else:\n",
        "            logger.info(f\"Found model_quantized.onnx for {model_name}\")\n",
        "        \n",
        "        # Benchmark across all batch sizes\n",
        "        for batch_size in config.batch_sizes:\n",
        "            logger.info(f\"Benchmarking {model_name} with batch size {batch_size}\")\n",
        "            result = benchmarker.benchmark_model(onnx_path, batch_size)\n",
        "            if result:\n",
        "                all_results.append(result)\n",
        "    \n",
        "    if save_results and all_results:\n",
        "        ResultsManager.save_results(all_results)\n",
        "    \n",
        "    ResultsManager.print_summary(all_results)\n",
        "    return all_results\n",
        "\n",
        "# Example usage functions for notebook\n",
        "def quick_test(models_dir: str = \"models\", batch_size: int = 1):\n",
        "    \"\"\"Quick test run with minimal iterations - useful for testing setup\"\"\"\n",
        "    from transformers import AutoTokenizer\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    \n",
        "    config = create_benchmark_config(\n",
        "        benchmark_iterations=5,  # Very fast for testing\n",
        "        warmup_iterations=2,\n",
        "        batch_sizes=[batch_size],\n",
        "        device_mode=\"cpu\"\n",
        "    )\n",
        "    \n",
        "    return run_full_benchmark(models_dir, tokenizer, config)\n",
        "\n",
        "def full_benchmark(models_dir: str = \"models\", include_accuracy: bool = True):\n",
        "    \"\"\"Complete benchmark with all metrics\"\"\"\n",
        "    from transformers import AutoTokenizer\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    \n",
        "    config = create_benchmark_config(\n",
        "        max_length=128,\n",
        "        benchmark_iterations=100,\n",
        "        warmup_iterations=20,\n",
        "        batch_sizes=[1, 2, 4, 8],\n",
        "        device_mode=\"cpu\",\n",
        "        test_csv_path=\"data/FinancialPhraseBank/all-data.csv\" if include_accuracy else None\n",
        "    )\n",
        "    \n",
        "    return run_full_benchmark(models_dir, tokenizer, config)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Quick discovery of available models\n",
        "    print(\"Discovering models...\")\n",
        "    models = discover_models(\"models\")\n",
        "    for name, path in models:\n",
        "        print(f\"  âœ“ {name}: {path}\")\n",
        "    \n",
        "    if models:\n",
        "        # Alternatively run a quick test using quick_test()\n",
        "        print(f\"\\nFound {len(models)} models. Running full benchmark...\")\n",
        "        results = full_benchmark()\n",
        "    else:\n",
        "        print(\"No models found!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPEaWosSVJk00KG/SJV/J7B",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
