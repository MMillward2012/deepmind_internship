{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMillward2012/deepmind_internship/blob/main/notebooks/7_benchmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/matthew/Documents/deepmind_internship\n",
            "README.md        \u001b[34mmodels\u001b[m\u001b[m           \u001b[34mresults\u001b[m\u001b[m\n",
            "\u001b[34mdata\u001b[m\u001b[m             \u001b[34mnotebooks\u001b[m\u001b[m        \u001b[34msrc\u001b[m\u001b[m\n",
            "\u001b[34mfigures\u001b[m\u001b[m          requirements.txt \u001b[34mvenv-py311\u001b[m\u001b[m\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RTwrJ4dSirfW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
        "from transformers.onnx import export\n",
        "from transformers.onnx.features import FeaturesManager\n",
        "import onnxruntime as ort\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASE_DIR = Path(\"models\")\n",
        "EXAMPLE_INPUT = \"Stocks surged after the company reported record earnings.\"\n",
        "MAX_LENGTH = 128\n",
        "ONNX_OPSET = 13\n",
        "BENCHMARK_ITERATIONS = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found models: ['all-MiniLM-L6-v2-financial-sentiment', 'distilbert-financial-sentiment', 'SmolLM2-360M-Instruct-financial-sentiment', 'tinybert-financial-classifier', 'mobilebert-uncased-financial-sentiment']\n"
          ]
        }
      ],
      "source": [
        "model_dirs = [d for d in BASE_DIR.iterdir() if d.is_dir()]\n",
        "print(\"Found models:\", [m.name for m in model_dirs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def export_to_onnx(model_dir, onnx_path, task=\"sequence-classification\"):\n",
        "    config = AutoConfig.from_pretrained(model_dir)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "    \n",
        "    model_kind, onnx_config_class = FeaturesManager.check_supported_model_or_raise(config, task=task)\n",
        "    onnx_config = onnx_config_class(config)\n",
        "    \n",
        "    export(\n",
        "        preprocessor=tokenizer,\n",
        "        model=model,\n",
        "        config=onnx_config,\n",
        "        opset=ONNX_OPSET,\n",
        "        output=onnx_path\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def benchmark_onnx(onnx_model_path, tokenizer, quantised=False):\n",
        "    sess = ort.InferenceSession(str(onnx_model_path), providers=[\"CPUExecutionProvider\"])\n",
        "    inputs = tokenizer(EXAMPLE_INPUT, return_tensors=\"np\", max_length=MAX_LENGTH, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # Warm-up\n",
        "    for _ in range(10):\n",
        "        sess.run(None, {\"input_ids\": inputs[\"input_ids\"]})\n",
        "\n",
        "    # Benchmark\n",
        "    times = []\n",
        "    for _ in range(BENCHMARK_ITERATIONS):\n",
        "        start = time.time()\n",
        "        sess.run(None, {\"input_ids\": inputs[\"input_ids\"]})\n",
        "        times.append((time.time() - start) * 1000)\n",
        "\n",
        "    return {\n",
        "        \"avg_latency_ms\": np.mean(times),\n",
        "        \"p99_latency_ms\": np.percentile(times, 99),\n",
        "        \"quantised\": quantised\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚è≥ Processing all-MiniLM-L6-v2-financial-sentiment...\n",
            "üì¶ Exporting to ONNX...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "FeaturesManager.check_supported_model_or_raise() got an unexpected keyword argument 'task'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m onnx_model_path.exists():\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müì¶ Exporting to ONNX...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[43mexport_to_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ ONNX already exists.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mexport_to_onnx\u001b[39m\u001b[34m(model_dir, onnx_path, task)\u001b[39m\n\u001b[32m      3\u001b[39m model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n\u001b[32m      4\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_dir)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model_kind, onnx_config_class = \u001b[43mFeaturesManager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_supported_model_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m onnx_config = onnx_config_class(config)\n\u001b[32m      9\u001b[39m export(\n\u001b[32m     10\u001b[39m     preprocessor=tokenizer,\n\u001b[32m     11\u001b[39m     model=model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     output=onnx_path\n\u001b[32m     15\u001b[39m )\n",
            "\u001b[31mTypeError\u001b[39m: FeaturesManager.check_supported_model_or_raise() got an unexpected keyword argument 'task'"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "for model_dir in model_dirs:\n",
        "    print(f\"\\n‚è≥ Processing {model_dir.name}...\")\n",
        "    \n",
        "    onnx_dir = model_dir / \"onnx\"\n",
        "    onnx_dir.mkdir(exist_ok=True)\n",
        "    onnx_model_path = onnx_dir / \"model.onnx\"\n",
        "    quantised_model_path = onnx_dir / \"model-int8.onnx\"\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "    # Export ONNX if not already done\n",
        "    if not onnx_model_path.exists():\n",
        "        print(\"üì¶ Exporting to ONNX...\")\n",
        "        export_to_onnx(model_dir, onnx_model_path)\n",
        "    else:\n",
        "        print(\"‚úÖ ONNX already exists.\")\n",
        "\n",
        "    # Benchmark original\n",
        "    print(\"üß™ Benchmarking original model...\")\n",
        "    result_fp32 = benchmark_onnx(onnx_model_path, tokenizer, quantised=False)\n",
        "    result_fp32[\"model\"] = model_dir.name\n",
        "    result_fp32[\"size_mb\"] = onnx_model_path.stat().st_size / 1e6\n",
        "    results.append(result_fp32)\n",
        "\n",
        "    # Quantise if not already done\n",
        "    if not quantised_model_path.exists():\n",
        "        print(\"‚öôÔ∏è  Quantising...\")\n",
        "        quantize_dynamic(str(onnx_model_path), str(quantised_model_path), weight_type=QuantType.QInt8)\n",
        "    else:\n",
        "        print(\"‚úÖ Quantised model already exists.\")\n",
        "\n",
        "    # Benchmark quantised\n",
        "    print(\"üß™ Benchmarking quantised model...\")\n",
        "    result_int8 = benchmark_onnx(quantised_model_path, tokenizer, quantised=True)\n",
        "    result_int8[\"model\"] = model_dir.name + \" (INT8)\"\n",
        "    result_int8[\"size_mb\"] = quantised_model_path.stat().st_size / 1e6\n",
        "    results.append(result_int8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(results)\n",
        "df = df[[\"model\", \"size_mb\", \"avg_latency_ms\", \"p99_latency_ms\", \"quantised\"]]\n",
        "df = df.sort_values(by=\"avg_latency_ms\")\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df.style.format({\n",
        "    \"size_mb\": \"{:.1f}\",\n",
        "    \"avg_latency_ms\": \"{:.2f}\",\n",
        "    \"p99_latency_ms\": \"{:.2f}\"\n",
        "})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPEaWosSVJk00KG/SJV/J7B",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
