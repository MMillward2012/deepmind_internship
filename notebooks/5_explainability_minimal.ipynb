{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271f2d6c",
   "metadata": {},
   "source": [
    "# üß† Financial Sentiment Model Explainability Dashboard\n",
    "\n",
    "## Overview\n",
    "This notebook provides comprehensive explainability analysis for the fine-tuned TinyBERT financial sentiment classification model. It includes four complementary explanation methods accessible through an interactive dashboard.\n",
    "\n",
    "### Explanation Methods\n",
    "- **üéØ SHAP**: Game-theory based feature importance\n",
    "- **üîç LIME**: Local interpretable model-agnostic explanations \n",
    "- **üëÅÔ∏è Attention**: Model attention head visualization\n",
    "- **üå°Ô∏è GradCAM**: Gradient-based visual attribution\n",
    "\n",
    "### Dashboard Features\n",
    "- **Mistake Analysis**: Examine specific model errors\n",
    "- **Custom Text Analysis**: Test any financial text\n",
    "- **Interactive Interface**: Tabbed layout for easy comparison\n",
    "- **On-demand Computation**: Optimized performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c922b2",
   "metadata": {},
   "source": [
    "## 1. üì¶ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc50c85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/matthew/Documents/deepmind_internship\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/Users/matthew/Documents/deepmind_internship')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f12404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model and tokenizer\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Explainability libraries\n",
    "import shap\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from bertviz import head_view\n",
    "from captum.attr import LayerGradCam\n",
    "\n",
    "# Dashboard components\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0ba13",
   "metadata": {},
   "source": [
    "## 2. ‚öôÔ∏è Data Loading\n",
    "\n",
    "**Simple Data Loading** - Using the exact working approach from the other notebook:\n",
    "\n",
    "- **Fixed Configuration**: Uses FinancialPhraseBank dataset with known structure\n",
    "- **Proven Encoding**: latin-1 encoding (works reliably)\n",
    "- **Standard Columns**: 'label' and 'sentence' columns with header=None\n",
    "- **Train-Test Split**: Consistent 25% test split with stratification\n",
    "- **Quote Handling**: Automatic removal of extra quotes from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f9cb64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded: models/tinybert-financial-classifier\n",
      "üìä Full dataset: 4846 samples\n",
      "üìä Test samples: 1000\n",
      "üè∑Ô∏è Labels: ['negative', 'neutral', 'positive']\n",
      "üìà Label distribution: {0: 129, 1: 601, 2: 270}\n",
      "üìù Sample text: Le Lay succeeds Walter G++nter and will be based in Finland ....\n",
      "‚úÖ Data loaded successfully with correct encoding\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "MODEL_DIR = Path('models/tinybert-financial-classifier')\n",
    "DATA_FILE = 'data/FinancialPhraseBank/all-data.csv'\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.25\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_DIR)\n",
    "\n",
    "# Load label encoder\n",
    "import pickle\n",
    "with open(MODEL_DIR / 'label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Load data using the EXACT working approach from the other notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data with correct encoding and column names (matching training notebook)\n",
    "df = pd.read_csv(DATA_FILE, header=None, names=[\"label\", \"sentence\"], encoding=\"latin-1\")\n",
    "df[\"sentence\"] = df[\"sentence\"].str.strip('\"')  # Remove extra quotes\n",
    "\n",
    "# Create train-test split with same parameters as training\n",
    "train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_SEED, \n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "# Extract test data\n",
    "test_texts = test_df['sentence'].tolist()[:1000]  # Limit to 1000 for demo\n",
    "test_labels = label_encoder.transform(test_df['label'].tolist()[:1000])\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {MODEL_DIR}\")\n",
    "print(f\"üìä Full dataset: {len(df)} samples\")\n",
    "print(f\"üìä Test samples: {len(test_texts)}\")\n",
    "print(f\"üè∑Ô∏è Labels: {list(label_encoder.classes_)}\")\n",
    "print(f\"üìà Label distribution: {dict(zip(*np.unique(test_labels, return_counts=True)))}\")\n",
    "print(f\"üìù Sample text: {test_texts[0][:100]}...\")\n",
    "print(\"‚úÖ Data loaded successfully with correct encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b5760",
   "metadata": {},
   "source": [
    "## 3. üîÆ Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d147141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Accuracy: 0.791\n",
      "‚ùå Misclassifications: 209\n"
     ]
    }
   ],
   "source": [
    "# Get model predictions\n",
    "model.eval()\n",
    "predictions = []\n",
    "confidences = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in test_texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        pred = torch.argmax(probs, dim=-1).item()\n",
    "        conf = torch.max(probs).item()\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        confidences.append(conf)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "confidences = np.array(confidences)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (predictions == test_labels).mean()\n",
    "print(f\"üéØ Accuracy: {accuracy:.3f}\")\n",
    "print(f\"‚ùå Misclassifications: {(predictions != test_labels).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65397795",
   "metadata": {},
   "source": [
    "## 4. üéõÔ∏è Explainability Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca8ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating explainability dashboard...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d66d56c9fa54e88b2ca591f3a6db152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>üß† Explainability Dashboard</h3>'), HBox(children=(Textarea(value='The company r‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ExplainabilityDashboard:\n",
    "    def __init__(self, model, tokenizer, test_texts, test_labels, predictions, label_encoder):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.test_texts = test_texts\n",
    "        self.test_labels = test_labels\n",
    "        self.predictions = predictions\n",
    "        self.label_encoder = label_encoder\n",
    "        self.setup_dashboard()\n",
    "    \n",
    "    def setup_dashboard(self):\n",
    "        # Input widgets\n",
    "        self.text_input = widgets.Textarea(\n",
    "            value=\"The company reported strong quarterly earnings with revenue growth of 15%.\",\n",
    "            placeholder=\"Enter financial text to analyze...\",\n",
    "            description=\"Text:\",\n",
    "            layout=widgets.Layout(width='100%', height='80px')\n",
    "        )\n",
    "        \n",
    "        # Get misclassified examples for dropdown\n",
    "        misclassified_indices = [i for i in range(len(self.test_texts)) \n",
    "                               if self.predictions[i] != self.test_labels[i]]\n",
    "        \n",
    "        self.mistake_dropdown = widgets.Dropdown(\n",
    "            options=[(f\"Mistake {i+1}: {self.test_texts[idx][:50]}...\", idx) \n",
    "                    for i, idx in enumerate(misclassified_indices[:20])],\n",
    "            description=\"Analyze Mistake:\"\n",
    "        )\n",
    "        \n",
    "        self.analyze_button = widgets.Button(\n",
    "            description=\"üîç Analyze Text\",\n",
    "            button_style='primary'\n",
    "        )\n",
    "        \n",
    "        self.mistake_button = widgets.Button(\n",
    "            description=\"üîç Analyze Mistake\",\n",
    "            button_style='warning'\n",
    "        )\n",
    "        \n",
    "        # Output area\n",
    "        self.output = widgets.Output()\n",
    "        \n",
    "        # Event handlers\n",
    "        self.analyze_button.on_click(self.analyze_custom_text)\n",
    "        self.mistake_button.on_click(self.analyze_mistake)\n",
    "        \n",
    "        # Layout\n",
    "        self.dashboard = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>üß† Explainability Dashboard</h3>\"),\n",
    "            widgets.HBox([self.text_input]),\n",
    "            widgets.HBox([self.analyze_button]),\n",
    "            widgets.HTML(\"<hr><h4>Or analyze a model mistake:</h4>\"),\n",
    "            widgets.HBox([self.mistake_dropdown, self.mistake_button]),\n",
    "            self.output\n",
    "        ])\n",
    "    \n",
    "    def analyze_custom_text(self, _):\n",
    "        if not self.text_input.value.strip():\n",
    "            return\n",
    "        \n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            self.run_analysis(self.text_input.value)\n",
    "    \n",
    "    def analyze_mistake(self, _):\n",
    "        idx = self.mistake_dropdown.value\n",
    "        text = self.test_texts[idx]\n",
    "        true_label = self.label_encoder.inverse_transform([self.test_labels[idx]])[0]\n",
    "        pred_label = self.label_encoder.inverse_transform([self.predictions[idx]])[0]\n",
    "        \n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            print(f\"üîç Analyzing Mistake: True={true_label}, Predicted={pred_label}\")\n",
    "            print(\"-\" * 60)\n",
    "            self.run_analysis(text)\n",
    "    \n",
    "    def run_analysis(self, text):\n",
    "        # Get prediction\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "        outputs = self.model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)\n",
    "        pred_idx = torch.argmax(probs, dim=-1).item()\n",
    "        confidence = torch.max(probs).item()\n",
    "        pred_label = self.label_encoder.inverse_transform([pred_idx])[0]\n",
    "        \n",
    "        print(f\"üìä Prediction: {pred_label} (confidence: {confidence:.3f})\")\n",
    "        print(f\"üìù Text: {text}\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        # Run all explanation methods\n",
    "        self.run_shap_analysis(text)\n",
    "        self.run_lime_analysis(text)\n",
    "        self.run_attention_analysis(text)\n",
    "        self.run_gradcam_analysis(text)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"‚úÖ Analysis complete!\")\n",
    "    \n",
    "    def run_shap_analysis(self, text):\n",
    "        \"\"\"SHAP explainability analysis\"\"\"\n",
    "        print(\"\\nüéØ SHAP Analysis:\")\n",
    "        try:\n",
    "            # Create SHAP explainer for transformers\n",
    "            def model_wrapper(texts):\n",
    "                \"\"\"Wrapper function for SHAP\"\"\"\n",
    "                if isinstance(texts, str):\n",
    "                    texts = [texts]\n",
    "                \n",
    "                predictions = []\n",
    "                for t in texts:\n",
    "                    if not t.strip():\n",
    "                        predictions.append([0.33, 0.33, 0.34])\n",
    "                        continue\n",
    "                    \n",
    "                    inputs = self.tokenizer(t, return_tensors=\"pt\", truncation=True, \n",
    "                                          padding=True, max_length=128)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model(**inputs)\n",
    "                        probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()[0]\n",
    "                    predictions.append(probs)\n",
    "                \n",
    "                return np.array(predictions)\n",
    "            \n",
    "            # Create SHAP explainer\n",
    "            explainer = shap.Explainer(model_wrapper, self.tokenizer)\n",
    "            \n",
    "            # Get SHAP values\n",
    "            shap_values = explainer([text], max_evals=100)\n",
    "            \n",
    "            # Display top features\n",
    "            pred_class = np.argmax(model_wrapper([text])[0])\n",
    "            if hasattr(shap_values, 'values') and len(shap_values.values.shape) > 2:\n",
    "                values = shap_values.values[0, :, pred_class]\n",
    "            else:\n",
    "                values = shap_values.values[0] if hasattr(shap_values, 'values') else shap_values[0]\n",
    "            \n",
    "            tokens = self.tokenizer.tokenize(text)[:len(values)]\n",
    "            \n",
    "            # Show top contributing tokens\n",
    "            token_scores = list(zip(tokens, values))\n",
    "            token_scores.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "            \n",
    "            print(\"Top contributing tokens:\")\n",
    "            for token, score in token_scores[:8]:\n",
    "                if token not in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                    print(f\"  {token}: {score:.4f}\")\n",
    "            \n",
    "            # Try to show visualization if possible\n",
    "            try:\n",
    "                shap.plots.text(shap_values[0])\n",
    "            except:\n",
    "                print(\"  (Visual plot not available in this environment)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è SHAP analysis failed: {e}\")\n",
    "            print(\"Falling back to simple gradient analysis...\")\n",
    "            self.run_simple_gradient_analysis(text)\n",
    "    \n",
    "    def run_simple_gradient_analysis(self, text):\n",
    "        \"\"\"Simple gradient-based token importance as fallback\"\"\"\n",
    "        try:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                                  padding=True, max_length=128)\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "            \n",
    "            # Enable gradients\n",
    "            inputs['input_ids'].requires_grad_(True)\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "            # Get target prediction\n",
    "            target_class = torch.argmax(outputs.logits, dim=-1)\n",
    "            target_prob = torch.softmax(outputs.logits, dim=-1)[0, target_class]\n",
    "            \n",
    "            # Backward pass\n",
    "            target_prob.backward()\n",
    "            gradients = inputs['input_ids'].grad\n",
    "            \n",
    "            # Get importance scores\n",
    "            importance_scores = torch.abs(gradients[0]).detach().numpy()\n",
    "            \n",
    "            # Show results\n",
    "            token_importance = list(zip(tokens, importance_scores))\n",
    "            token_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(\"Token importance (gradient-based):\")\n",
    "            for token, score in token_importance[:8]:\n",
    "                if token not in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                    print(f\"  {token}: {score:.4f}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Gradient analysis also failed: {e}\")\n",
    "    \n",
    "    def run_lime_analysis(self, text):\n",
    "        print(\"\\nüîç LIME Analysis:\")\n",
    "        try:\n",
    "            def predict_proba_fn(texts):\n",
    "                preds = []\n",
    "                for t in texts:\n",
    "                    if not t.strip():  # Handle empty strings\n",
    "                        preds.append([0.33, 0.33, 0.34])  # Default uniform distribution\n",
    "                        continue\n",
    "                        \n",
    "                    inputs = self.tokenizer(t, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model(**inputs)\n",
    "                        probs = torch.softmax(outputs.logits, dim=-1).numpy()[0]\n",
    "                    preds.append(probs)\n",
    "                return np.array(preds)\n",
    "            \n",
    "            explainer = LimeTextExplainer(\n",
    "                class_names=list(self.label_encoder.classes_),\n",
    "                mode='classification'\n",
    "            )\n",
    "            \n",
    "            explanation = explainer.explain_instance(\n",
    "                text, \n",
    "                predict_proba_fn, \n",
    "                num_features=8,\n",
    "                num_samples=100\n",
    "            )\n",
    "            \n",
    "            print(\"Top features:\")\n",
    "            for feature, score in explanation.as_list():\n",
    "                print(f\"  {feature}: {score:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è LIME analysis failed: {e}\")\n",
    "            print(\"Falling back to simple analysis...\")\n",
    "    \n",
    "    def run_attention_analysis(self, text):\n",
    "        \"\"\"Attention head visualization\"\"\"\n",
    "        print(\"\\nüëÅÔ∏è Attention Analysis:\")\n",
    "        try:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                                  padding=True, max_length=128)\n",
    "            \n",
    "            # Get model outputs with attention\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, output_attentions=True)\n",
    "            \n",
    "            # Extract attention weights\n",
    "            attention = outputs.attentions  # List of attention weights for each layer\n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "            \n",
    "            # Average attention across heads and layers for simplicity\n",
    "            if attention and len(attention) > 0:\n",
    "                # Use last layer attention\n",
    "                last_layer_attention = attention[-1][0]  # [num_heads, seq_len, seq_len]\n",
    "                \n",
    "                # Average across heads\n",
    "                avg_attention = last_layer_attention.mean(dim=0)  # [seq_len, seq_len]\n",
    "                \n",
    "                # Get attention to CLS token (classification)\n",
    "                cls_attention = avg_attention[0, 1:-1].cpu().numpy()  # Skip CLS and SEP\n",
    "                \n",
    "                # Show top attended tokens\n",
    "                token_attention = list(zip(tokens[1:-1], cls_attention))  # Skip CLS and SEP\n",
    "                token_attention.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                print(\"Most attended tokens:\")\n",
    "                for token, score in token_attention[:8]:\n",
    "                    if token not in ['[PAD]']:\n",
    "                        print(f\"  {token}: {score:.4f}\")\n",
    "                \n",
    "                # Try to show BertViz visualization\n",
    "                try:\n",
    "                    from bertviz import head_view\n",
    "                    # This works best in Jupyter with proper display\n",
    "                    print(\"  (For interactive attention visualization, use bertviz.head_view)\")\n",
    "                except ImportError:\n",
    "                    print(\"  (bertviz not available for interactive visualization)\")\n",
    "            else:\n",
    "                print(\"  No attention weights available from model\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Attention analysis failed: {e}\")\n",
    "    \n",
    "    def run_gradcam_analysis(self, text):\n",
    "        \"\"\"GradCAM analysis\"\"\"\n",
    "        print(\"\\nüå°Ô∏è GradCAM Analysis:\")\n",
    "        try:\n",
    "            inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                                  padding=True, max_length=128)\n",
    "            \n",
    "            # Get model prediction\n",
    "            outputs = self.model(**inputs)\n",
    "            pred_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "            \n",
    "            # Use Captum for GradCAM\n",
    "            from captum.attr import LayerGradCam\n",
    "            \n",
    "            # Create GradCAM for the last transformer layer\n",
    "            layer_gradcam = LayerGradCam(self.model, self.model.bert.encoder.layer[-1])\n",
    "            \n",
    "            # Get attributions\n",
    "            attributions = layer_gradcam.attribute(\n",
    "                inputs['input_ids'],\n",
    "                target=pred_class,\n",
    "                additional_forward_args=(inputs['attention_mask'],)\n",
    "            )\n",
    "            \n",
    "            # Average across dimensions to get token-level importance\n",
    "            if len(attributions.shape) > 2:\n",
    "                token_importance = attributions.mean(dim=-1).squeeze().detach().numpy()\n",
    "            else:\n",
    "                token_importance = attributions.squeeze().detach().numpy()\n",
    "            \n",
    "            tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "            \n",
    "            # Show top important tokens\n",
    "            token_scores = list(zip(tokens, token_importance))\n",
    "            token_scores.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "            \n",
    "            print(\"Most important regions (GradCAM):\")\n",
    "            for token, score in token_scores[:8]:\n",
    "                if token not in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                    print(f\"  {token}: {score:.4f}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è GradCAM analysis failed: {e}\")\n",
    "            print(\"  (This method requires specific model architecture compatibility)\")\n",
    "    \n",
    "    def display(self):\n",
    "        display(self.dashboard)\n",
    "\n",
    "# Create and display dashboard\n",
    "print(\"üîß Creating explainability dashboard...\")\n",
    "dashboard = ExplainabilityDashboard(model, tokenizer, test_texts, test_labels, predictions, label_encoder)\n",
    "dashboard.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d6038b",
   "metadata": {},
   "source": [
    "## 5. üîç Quick Misclassification Analysis\n",
    "\n",
    "Simple analysis to identify patterns for fine-tuning in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2422939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total misclassifications: 209\n",
      "üìà Error rate: 20.9%\n",
      "\n",
      "üîÑ Top Confusion Patterns:\n",
      "  neutral ‚Üí positive: 73 cases (34.9%)\n",
      "  positive ‚Üí neutral: 59 cases (28.2%)\n",
      "  neutral ‚Üí negative: 35 cases (16.7%)\n",
      "\n",
      "üîç Problematic Keywords:\n",
      "  solutions: 0.0169\n",
      "  new: 0.0160\n",
      "  mln: 0.0147\n",
      "  pct: 0.0134\n",
      "  compared: 0.0120\n",
      "  solutions: 0.0169\n",
      "  new: 0.0160\n",
      "  mln: 0.0147\n",
      "  pct: 0.0134\n",
      "  compared: 0.0120\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "keys must be str, int, float, bool or None, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     49\u001b[39m results = {\n\u001b[32m     50\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mconfusion_patterns\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mdict\u001b[39m(confusion_data),\n\u001b[32m     51\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mproblematic_keywords\u001b[39m\u001b[33m'\u001b[39m: problematic_keywords,\n\u001b[32m     52\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtotal_errors\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(misclassified_texts),\n\u001b[32m     53\u001b[39m     \u001b[33m'\u001b[39m\u001b[33merror_rate\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(misclassified_texts)/\u001b[38;5;28mlen\u001b[39m(test_texts)*\u001b[32m100\u001b[39m\n\u001b[32m     54\u001b[39m }\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33manalysis_results/misclassification_analysis.json\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müíæ Results saved to: analysis_results/misclassification_analysis.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìã Ready for fine-tuning in next notebook!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:179\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    173\u001b[39m     iterable = \u001b[38;5;28mcls\u001b[39m(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n\u001b[32m    174\u001b[39m         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n\u001b[32m    175\u001b[39m         separators=separators,\n\u001b[32m    176\u001b[39m         default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:432\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:406\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    405\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    408\u001b[39m     _current_indent_level -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.13/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/encoder.py:377\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mkeys must be str, int, float, bool or None, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    378\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first:\n\u001b[32m    380\u001b[39m     first = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: keys must be str, int, float, bool or None, not tuple"
     ]
    }
   ],
   "source": [
    "# Quick misclassification analysis\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Get misclassified examples\n",
    "misclassified_mask = predictions != test_labels\n",
    "misclassified_texts = [test_texts[i] for i in range(len(test_texts)) if misclassified_mask[i]]\n",
    "misclassified_true = test_labels[misclassified_mask]\n",
    "misclassified_pred = predictions[misclassified_mask]\n",
    "\n",
    "print(f\"üìä Total misclassifications: {len(misclassified_texts)}\")\n",
    "print(f\"üìà Error rate: {len(misclassified_texts)/len(test_texts)*100:.1f}%\")\n",
    "\n",
    "# 1. Confusion patterns\n",
    "print(\"\\nüîÑ Top Confusion Patterns:\")\n",
    "confusion_data = defaultdict(int)\n",
    "for true_idx, pred_idx in zip(misclassified_true, misclassified_pred):\n",
    "    true_label = label_encoder.inverse_transform([true_idx])[0]\n",
    "    pred_label = label_encoder.inverse_transform([pred_idx])[0]\n",
    "    confusion_data[(true_label, pred_label)] += 1\n",
    "\n",
    "for (true_label, pred_label), count in sorted(confusion_data.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "    percentage = count / len(misclassified_texts) * 100\n",
    "    print(f\"  {true_label} ‚Üí {pred_label}: {count} cases ({percentage:.1f}%)\")\n",
    "\n",
    "# 2. Problematic keywords\n",
    "print(\"\\nüîç Problematic Keywords:\")\n",
    "correctly_classified_texts = [test_texts[i] for i in range(len(test_texts)) if not misclassified_mask[i]]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=200, stop_words='english', ngram_range=(1, 2))\n",
    "all_texts = misclassified_texts + correctly_classified_texts[:len(misclassified_texts)]\n",
    "vectorizer.fit(all_texts)\n",
    "\n",
    "misc_tfidf = vectorizer.transform(misclassified_texts).mean(axis=0).A1\n",
    "correct_tfidf = vectorizer.transform(correctly_classified_texts[:len(misclassified_texts)]).mean(axis=0).A1\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "score_diff = misc_tfidf - correct_tfidf\n",
    "top_indices = score_diff.argsort()[-10:][::-1]\n",
    "\n",
    "problematic_keywords = [(feature_names[i], score_diff[i]) for i in top_indices if score_diff[i] > 0.001]\n",
    "for keyword, score in problematic_keywords[:5]:\n",
    "    print(f\"  {keyword}: {score:.4f}\")\n",
    "\n",
    "# Save results for fine-tuning notebook\n",
    "os.makedirs('analysis_results', exist_ok=True)\n",
    "results = {\n",
    "    'confusion_patterns': dict(confusion_data),\n",
    "    'problematic_keywords': problematic_keywords,\n",
    "    'total_errors': len(misclassified_texts),\n",
    "    'error_rate': len(misclassified_texts)/len(test_texts)*100\n",
    "}\n",
    "\n",
    "with open('analysis_results/misclassification_analysis.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Results saved to: analysis_results/misclassification_analysis.json\")\n",
    "print(f\"üìã Ready for fine-tuning in next notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756db6fa",
   "metadata": {},
   "source": [
    "## 6. üìã Summary\n",
    "\n",
    "### ‚úÖ Completed:\n",
    "- **Interactive Dashboard**: SHAP and LIME explanations for any text\n",
    "- **Mistake Analysis**: Analyze specific model errors\n",
    "- **Misclassification Patterns**: Key insights for fine-tuning\n",
    "\n",
    "### üìä Key Findings:\n",
    "- Error rate: ~20% on test data\n",
    "- Main confusion patterns identified\n",
    "- Problematic keywords extracted\n",
    "\n",
    "### üîú Next Steps:\n",
    "Results saved to `analysis_results/` for **Notebook 6: Fine-tuning with Pruning Methods**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
