{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMillward2012/deepmind_internship/blob/main/notebooks/7_benchmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial_benchmarks.ipynb    initial_train_finbert.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <u>1. Imports</u>\n",
        "\n",
        "**System & Data**: `pathlib` for handling file paths, `gc` for memory management, `pandas` for loading our data, and `scikit-learn` for splitting it.\n",
        "\n",
        "**Machine Learning**: `torch` for the base models and `onnxruntime` for inference and its quantisation tools.\n",
        "\n",
        "**Hugging Face**: `transformers` for loading our pre-trained models and tokenisers.\n",
        "\n",
        "We also import Python's `logging` library to set the log level for onnxruntime to `ERROR`. This prevents routine warnings from cluttering the output during the quantisation process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard & System\n",
        "import gc\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Data & ML\n",
        "import numpy as np\n",
        "import torch\n",
        "import onnxruntime as ort\n",
        "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Suppress ONNX Runtime logging\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"onnxruntime\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <u>2. Configuration & Model Discovery</u>\n",
        "\n",
        "This section sets up key configuration variables and prepares all the assets needed for the main loop.\n",
        "\n",
        "`BASE_DIR`: The root folder where your models are stored.\n",
        "\n",
        "`ONNX_OPSET_VERSION`: We set this to 17, a modern version required for newer model architectures.\n",
        "\n",
        "`DATA_FILE_PATH`, `RANDOM_SEED`, `TEST_SIZE`: Parameters to ensure we can reliably load and split our financial text data to get a consistent calibration set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Found 5 valid models.\n",
            "Loading data from data/FinancialPhraseBank/all-data.csv...\n",
            "‚úÖ Created a calibration dataset with 100 samples.\n"
          ]
        }
      ],
      "source": [
        "# Model & ONNX Configuration\n",
        "BASE_DIR = Path(\"models\")\n",
        "ONNX_OPSET_VERSION = 17\n",
        "\n",
        "# Data & Split Configuration \n",
        "DATA_FILE_PATH = Path(\"data/FinancialPhraseBank/all-data.csv\")\n",
        "RANDOM_SEED = 42\n",
        "TEST_SIZE = 0.25 # 25% for the test set\n",
        "\n",
        "\n",
        "def is_valid_model_dir(d: Path) -> bool:\n",
        "    \"\"\"Checks if a directory contains a valid Hugging Face model.\"\"\"\n",
        "    config_exists = (d / \"config.json\").exists()\n",
        "    model_file_exists = (d / \"pytorch_model.bin\").exists() or (d / \"model.safetensors\").exists()\n",
        "    return config_exists and model_file_exists\n",
        "\n",
        "\n",
        "def prepare_calibration_data(data_path, test_size, random_seed, num_samples=100):\n",
        "    \"\"\"Loads, splits, and samples the data to create a calibration set.\"\"\"\n",
        "    print(f\"Loading data from {data_path}...\")\n",
        "    df = pd.read_csv(\n",
        "        data_path,\n",
        "        header=None,\n",
        "        names=['sentiment', 'text'],\n",
        "        encoding='latin-1')\n",
        "\n",
        "    # Split data to get the test set\n",
        "    _, test_df = train_test_split(\n",
        "        df, test_size=test_size, random_state=random_seed, stratify=df['sentiment'])\n",
        "\n",
        "    # Sample the calibration set from the test data\n",
        "    calibration_df = test_df.sample(n=num_samples, random_state=random_seed)\n",
        "    print(f\"‚úÖ Created a calibration dataset with {len(calibration_df)} samples.\")\n",
        "    return calibration_df\n",
        "\n",
        "\n",
        "# Find all valid model directories\n",
        "model_dirs = [d for d in BASE_DIR.iterdir() if d.is_dir() and is_valid_model_dir(d)]\n",
        "print(f\"‚úÖ Found {len(model_dirs)} valid models.\")\n",
        "\n",
        "# Call the function to prepare data\n",
        "calibration_df = prepare_calibration_data(DATA_FILE_PATH, TEST_SIZE, RANDOM_SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <u>3. ONNX Helper Class & Export Function</u>\n",
        "\n",
        "Here we define the specialised helper classes for our pipeline, `ONNXExportWrapper`.\n",
        "\n",
        "This is a small `torch.nn.Module` that wraps our Hugging Face model. Its only job is to ensure the model's output is a simple logits tensor, which is a standard requirement for ONNX export.\n",
        "\n",
        "`TextCalibrationDataReader` is an essential class for performing static quantisation. Its role is to feed our calibration data to the ONNX Runtime tool. It's built to be robust:\n",
        "\n",
        "It inspects the ONNX model file to find out exactly which inputs it needs (e.g., `input_ids`, `attention_mask`).\n",
        "\n",
        "It tokenises the text from our calibration dataframe.\n",
        "\n",
        "It then provides this data one sample at a time, ensuring the dictionary it yields perfectly matches the model's required inputs. This prevents errors when quantising models with different architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ONNXExportWrapper(torch.nn.Module):\n",
        "    \"\"\"A wrapper to ensure model output is a simple tensor for ONNX compatibility.\"\"\"\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids, attention_mask=attention_mask, return_dict=False\n",
        "        )\n",
        "        return outputs[0]\n",
        "\n",
        "\n",
        "class TextCalibrationDataReader(CalibrationDataReader):\n",
        "    \"\"\"A robust data reader that adapts to the model's specific inputs.\"\"\"\n",
        "    def __init__(self, data_df: pd.DataFrame, tokenizer, onnx_model_path: Path):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data_list = data_df[\"text\"].tolist()\n",
        "        self.index = 0\n",
        "\n",
        "        # Find the model's required inputs\n",
        "        session = ort.InferenceSession(str(onnx_model_path), providers=[\"CPUExecutionProvider\"])\n",
        "        model_inputs = {input.name for input in session.get_inputs()}\n",
        "\n",
        "        # Tokenize all data and filter to only include the model's inputs\n",
        "        tokenized_data = self.tokenizer(\n",
        "            self.data_list, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"np\"\n",
        "        )\n",
        "        self.feed = {\n",
        "            key: tokenized_data[key] for key in tokenized_data if key in model_inputs\n",
        "        }\n",
        "        self.input_names = list(self.feed.keys())\n",
        "\n",
        "    def get_next(self):\n",
        "        if self.index >= len(self.data_list):\n",
        "            return None\n",
        "\n",
        "        item = {name: self.feed[name][self.index:self.index+1] for name in self.input_names}\n",
        "        self.index += 1\n",
        "        return item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <u>4. Main Processing & Export Loop</u>\n",
        "\n",
        "This is the main loop where everything comes together. For clarity, the export_model_to_onnx function is defined here. For each model found, the loop performs a sequence of optimised steps:\n",
        "\n",
        "**Step 1: Export to ONNX**\n",
        "\n",
        "It first checks if a standard ONNX version of the model already exists. If it doesn't, it loads the full PyTorch model and tokeniser, exports the model to the ONNX format using our helper function, and then clears the large model from memory to conserve resources.\n",
        "\n",
        "**Step 2: Static Quantisation**\n",
        "\n",
        "Next, it checks if a final, statically quantised model exists. If not, it uses our `TextCalibrationDataReader` to feed the calibration data into the `quantize_static` function. This tool analyses the data flow through the model and creates a highly efficient, production-ready `.onnx` file with 8-bit integer weights and activations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: all-MiniLM-L6-v2-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "   - ‚úÖ Statically quantised model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: distilbert-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "   - ‚úÖ Statically quantised model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: finbert-tone-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "   - ‚úÖ Statically quantised model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: tinybert-financial-classifier\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "   - ‚úÖ Statically quantised model already exists.\n",
            "----------------------------------------------------------------------\n",
            "‚è≥ Processing model: mobilebert-uncased-financial-sentiment\n",
            "   - ‚úÖ Standard ONNX model already exists.\n",
            "   - ‚úÖ Statically quantised model already exists.\n",
            "----------------------------------------------------------------------\n",
            "üéâ All models have been processed.\n"
          ]
        }
      ],
      "source": [
        "## üîÅ Main Processing Loop\n",
        "\n",
        "def export_model_to_onnx(model, tokenizer, onnx_path: Path, opset_version: int):\n",
        "    \"\"\"Exports a PyTorch model to the ONNX format.\"\"\"\n",
        "    print(\"   - Wrapping model for ONNX export...\")\n",
        "    wrapped_model = ONNXExportWrapper(model)\n",
        "    wrapped_model.eval()\n",
        "    dummy_input = tokenizer(\"This is a sample sentence.\", return_tensors=\"pt\")\n",
        "    print(f\"   - üöÄ Exporting to ONNX (Opset {opset_version})...\")\n",
        "    torch.onnx.export(\n",
        "        model=wrapped_model,\n",
        "        args=(dummy_input[\"input_ids\"], dummy_input[\"attention_mask\"]),\n",
        "        f=str(onnx_path), input_names=[\"input_ids\", \"attention_mask\"], output_names=[\"output\"],\n",
        "        dynamic_axes={\n",
        "            \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "            \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "            \"output\": {0: \"batch_size\"},\n",
        "        },\n",
        "        opset_version=opset_version, do_constant_folding=True,\n",
        "    )\n",
        "    print(f\"   - ‚úÖ Model successfully exported to {onnx_path.name}\")\n",
        "\n",
        "for model_dir in model_dirs:\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"‚è≥ Processing model: {model_dir.name}\")\n",
        "\n",
        "    onnx_dir = model_dir / \"onnx\"\n",
        "    onnx_dir.mkdir(exist_ok=True)\n",
        "    onnx_model_path = onnx_dir / \"model.onnx\"\n",
        "    quantised_model_path = onnx_dir / \"model-static-quant.onnx\"\n",
        "\n",
        "    # --- Step 1: Export to ONNX if needed ---\n",
        "    if not onnx_model_path.exists():\n",
        "        print(\"   - üì¶ ONNX model not found. Starting export...\")\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
        "            export_model_to_onnx(model, tokenizer, onnx_model_path, ONNX_OPSET_VERSION)\n",
        "            del model, tokenizer\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            print(f\"   - ‚ùå Export failed for {model_dir.name}: {e}\")\n",
        "            continue\n",
        "    else:\n",
        "        print(f\"   - ‚úÖ Standard ONNX model already exists.\")\n",
        "\n",
        "    # --- Step 2: Perform Static Quantisation if needed ---\n",
        "    if onnx_model_path.exists() and not quantised_model_path.exists():\n",
        "        print(f\"   - ‚öñÔ∏è Performing static quantisation for {onnx_model_path.name}...\")\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "            calibration_data_reader = TextCalibrationDataReader(calibration_df, tokenizer, onnx_model_path)\n",
        "            quantize_static(\n",
        "                model_input=onnx_model_path,\n",
        "                model_output=quantised_model_path,\n",
        "                calibration_data_reader=calibration_data_reader,\n",
        "                weight_type=QuantType.QInt8,\n",
        "            )\n",
        "            print(f\"   - ‚úÖ Statically quantised model saved to {quantised_model_path.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   - ‚ùå Static quantisation failed for {model_dir.name}: {e}\")\n",
        "    elif quantised_model_path.exists():\n",
        "         print(f\"   - ‚úÖ Statically quantised model already exists.\")\n",
        "\n",
        "print(\"-\" * 70)\n",
        "print(\"üéâ All models have been processed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <u> 1. Setup & Configuration </u>\n",
        "\n",
        "This first cell handles the initial setup. We import all necessary libraries, configure the logging to provide clean output, and define the dataclasses that will hold our configuration settings (BenchmarkConfig) and store the final measurements (BenchmarkResult)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import logging\n",
        "import platform\n",
        "import statistics\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "from dataclasses import dataclass, asdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "import pandas as pd\n",
        "import psutil\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Configure logging for clear output in the notebook\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkConfig:\n",
        "    \"\"\"Configuration for the entire benchmarking run.\"\"\"\n",
        "    benchmark_iterations: int = 100\n",
        "    warmup_iterations: int = 20\n",
        "    batch_sizes: List[int] = None\n",
        "    accuracy_sample_size: int = 500\n",
        "    test_csv_path: Optional[str] = None\n",
        "    device_mode: str = \"auto\"  # \"auto\", \"cpu\", or \"gpu\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.batch_sizes is None:\n",
        "            self.batch_sizes = [1, 2, 4, 8]\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkResult:\n",
        "    \"\"\"A structured class to hold results from a single benchmark run.\"\"\"\n",
        "    model: str\n",
        "    batch_size: int\n",
        "    avg_latency_ms: float\n",
        "    p95_latency_ms: float\n",
        "    throughput_samples_per_sec: float\n",
        "    peak_memory_mb: float\n",
        "    model_size_mb: float\n",
        "    provider: str\n",
        "    accuracy: Optional[float] = None\n",
        "    f1_score: Optional[float] = None\n",
        "\n",
        "    def to_dict(self) -> Dict:\n",
        "        return asdict(self)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <u> 2. Component: Hardware & Model Loading </u>\n",
        "\n",
        "These classes handle the initialisation of the ONNX Runtime session. The ExecutionProviderManager intelligently selects the best hardware available (e.g., CUDAExecutionProvider for NVIDIA GPUs), and the ModelLoader creates the inference session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExecutionProviderManager:\n",
        "    \"\"\"Manages ONNX execution providers based on platform and preferences.\"\"\"\n",
        "    @staticmethod\n",
        "    def get_execution_providers(mode: str = \"auto\") -> List[str]:\n",
        "        available = ort.get_available_providers()\n",
        "        # For Linux/Windows, prioritise GPU; for macOS, prioritise CPU\n",
        "        preferences = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
        "        if platform.system() == \"Darwin\":\n",
        "            preferences = [\"CPUExecutionProvider\", \"CoreMLExecutionProvider\"]\n",
        "        \n",
        "        chosen = [p for p in preferences if p in available]\n",
        "        logger.info(f\"Available providers: {available}. Auto-selected: {chosen}\")\n",
        "        return chosen\n",
        "\n",
        "class ModelLoader:\n",
        "    \"\"\"Handles loading an ONNX model into an inference session.\"\"\"\n",
        "    @staticmethod\n",
        "    def load_onnx_session(onnx_path: Path, providers: List[str]) -> ort.InferenceSession:\n",
        "        opts = ort.SessionOptions()\n",
        "        opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
        "        logger.info(f\"Creating ONNX session for {onnx_path.name} with providers: {providers}\")\n",
        "        return ort.InferenceSession(str(onnx_path), providers=providers, sess_options=opts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <u> 3. Component: Data Handling </u>\n",
        "\n",
        "The DataProcessor class is responsible for all data-related tasks. It can load the test dataset from a CSV file and prepare text inputs by tokenising them into the format required by the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "    \"\"\"Handles data preprocessing and batch preparation.\"\"\"\n",
        "    def __init__(self, tokenizer, max_length: int = 128):\n",
        "        self.tokenizer, self.max_length = tokenizer, max_length\n",
        "        self.example_inputs = [\"Stocks surged after the company reported record earnings.\"]\n",
        "\n",
        "    def prepare_batch_inputs(self, texts: List[str]) -> Dict[str, np.ndarray]:\n",
        "        encoding = self.tokenizer(\n",
        "            texts, return_tensors=\"np\", max_length=self.max_length,\n",
        "            padding=\"max_length\", truncation=True\n",
        "        )\n",
        "        # Ensure all model inputs are int64, as required by ONNX\n",
        "        return {k: v.astype(np.int64) for k, v in encoding.items()}\n",
        "\n",
        "    def load_test_dataset(self, csv_path: Path) -> Tuple[List[str], List[int]]:\n",
        "        df = pd.read_csv(csv_path, names=[\"label\", \"text\"], encoding=\"latin1\")\n",
        "        df = df.dropna(subset=[\"label\", \"text\"])\n",
        "        df[\"label\"] = df[\"label\"].astype('category').cat.codes\n",
        "        _, test_df = train_test_split(df, test_size=0.25, random_state=42, stratify=df[\"label\"])\n",
        "        logger.info(f\"Loaded {len(df)} rows; using {len(test_df)} for accuracy testing.\")\n",
        "        return test_df[\"text\"].tolist(), test_df[\"label\"].astype(int).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <u> 4. Component: Performance Measurement </u>\n",
        "\n",
        "These classes handle the actual performance measurements. PerformanceMonitor provides tools to track memory usage, while LatencyBenchmarker runs the model through many iterations to accurately measure inference speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PerformanceMonitor:\n",
        "    \"\"\"Monitors system performance during benchmarking.\"\"\"\n",
        "    @staticmethod\n",
        "    def measure_memory_usage() -> float:\n",
        "        \"\"\"Returns the current process's memory usage in MB.\"\"\"\n",
        "        return psutil.Process().memory_info().rss / (1024**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_model_size_mb(onnx_path: Path) -> float:\n",
        "        \"\"\"Returns the model's file size in MB.\"\"\"\n",
        "        return onnx_path.stat().st_size / (1024**2)\n",
        "\n",
        "class LatencyBenchmarker:\n",
        "    \"\"\"Handles the details of latency benchmarking.\"\"\"\n",
        "    def __init__(self, config: BenchmarkConfig):\n",
        "        self.config = config\n",
        "    \n",
        "    def warmup_session(self, session: ort.InferenceSession, inputs: Dict[str, np.ndarray]):\n",
        "        logger.info(f\"üî• Warming up with {self.config.warmup_iterations} iterations...\")\n",
        "        for _ in range(self.config.warmup_iterations):\n",
        "            session.run(None, inputs)\n",
        "    \n",
        "    def measure_latency(self, session: ort.InferenceSession, inputs: Dict[str, np.ndarray]) -> List[float]:\n",
        "        logger.info(f\"‚è±Ô∏è  Running latency benchmark ({self.config.benchmark_iterations} iterations)...\")\n",
        "        times = []\n",
        "        for _ in range(self.config.benchmark_iterations):\n",
        "            start = time.perf_counter()\n",
        "            session.run(None, inputs)\n",
        "            times.append((time.perf_counter() - start) * 1000) # Append time in ms\n",
        "        return times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <u> 5. Component: Accuracy Evaluation </u>\n",
        "\n",
        "The AccuracyEvaluator class is dedicated to measuring the model's correctness. It runs the model on the test dataset to calculate the accuracy and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AccuracyEvaluator:\n",
        "    \"\"\"Handles model accuracy and F1 score evaluation.\"\"\"\n",
        "    def __init__(self, session: ort.InferenceSession, data_processor: DataProcessor):\n",
        "        self.session, self.data_processor = session, data_processor\n",
        "    \n",
        "    def evaluate(self, texts: List[str], labels: List[int], batch_size: int, max_samples: int):\n",
        "        num_samples = min(len(texts), max_samples)\n",
        "        eval_texts, eval_labels = texts[:num_samples], labels[:num_samples]\n",
        "        \n",
        "        logger.info(f\"üéØ Evaluating accuracy on {num_samples} samples (batch size: {batch_size})...\")\n",
        "        all_predictions = []\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            batch_texts = eval_texts[i: i + batch_size]\n",
        "            inputs = self.data_processor.prepare_batch_inputs(batch_texts)\n",
        "            \n",
        "            # Filter inputs to only what the model actually needs\n",
        "            model_inputs = {inp.name for inp in self.session.get_inputs()}\n",
        "            valid_inputs = {k: v for k, v in inputs.items() if k in model_inputs}\n",
        "            \n",
        "            outputs = self.session.run(None, valid_inputs)\n",
        "            all_predictions.extend(np.argmax(outputs[0], axis=1))\n",
        "        \n",
        "        accuracy = accuracy_score(eval_labels, all_predictions)\n",
        "        f1 = f1_score(eval_labels, all_predictions, average=\"weighted\")\n",
        "        logger.info(f\"   -> Accuracy: {accuracy:.2%}, F1 Score: {f1:.2%}\")\n",
        "        return accuracy, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <u> 6. Main Orchestrator Class </u>\n",
        "\n",
        "This is the main orchestrator class. ONNXModelBenchmarker initialises all the helper components and contains the primary benchmark_model method, which executes the full benchmark pipeline for a single model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ONNXModelBenchmarker:\n",
        "    \"\"\"Orchestrates all components to run a benchmark for a single model.\"\"\"\n",
        "    def __init__(self, config: BenchmarkConfig, tokenizer):\n",
        "        self.config = config\n",
        "        self.data_processor = DataProcessor(tokenizer, max_length=128)\n",
        "        self.latency_benchmarker = LatencyBenchmarker(config)\n",
        "    \n",
        "    def benchmark_model(self, model_name: str, onnx_path: Path, batch_size: int) -> Optional[BenchmarkResult]:\n",
        "        try:\n",
        "            logger.info(f\"\\n{'='*60}\\nüöÄ BENCHMARKING: {model_name} | Batch Size: {batch_size}\\n{'='*60}\")\n",
        "            providers = ExecutionProviderManager.get_execution_providers(self.config.device_mode)\n",
        "            session = ModelLoader.load_onnx_session(onnx_path, providers)\n",
        "\n",
        "            # Prepare inputs and filter to what the model actually needs\n",
        "            inputs = self.data_processor.prepare_batch_inputs(self.data_processor.example_inputs * batch_size)\n",
        "            model_inputs = {inp.name for inp in session.get_inputs()}\n",
        "            valid_inputs = {k: v for k, v in inputs.items() if k in model_inputs}\n",
        "\n",
        "            mem_before = PerformanceMonitor.measure_memory_usage()\n",
        "            self.latency_benchmarker.warmup_session(session, valid_inputs)\n",
        "            times = self.latency_benchmarker.measure_latency(session, valid_inputs)\n",
        "            mem_after = PerformanceMonitor.measure_memory_usage()\n",
        "            \n",
        "            avg_latency = statistics.mean(times)\n",
        "            p95_latency = np.percentile(times, 95)\n",
        "            \n",
        "            accuracy, f1 = None, None\n",
        "            if self.config.test_csv_path:\n",
        "                evaluator = AccuracyEvaluator(session, self.data_processor)\n",
        "                texts, labels = self.data_processor.load_test_dataset(Path(self.config.test_csv_path))\n",
        "                accuracy, f1 = evaluator.evaluate(texts, labels, batch_size, self.config.accuracy_sample_size)\n",
        "\n",
        "            return BenchmarkResult(\n",
        "                model=model_name, batch_size=batch_size,\n",
        "                avg_latency_ms=avg_latency, p95_latency_ms=p95_latency,\n",
        "                throughput_samples_per_sec=(1000 * batch_size) / avg_latency if avg_latency > 0 else 0,\n",
        "                peak_memory_mb=mem_after,\n",
        "                model_size_mb=PerformanceMonitor.get_model_size_mb(onnx_path),\n",
        "                provider=session.get_providers()[0],\n",
        "                accuracy=accuracy, f1_score=f1\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Benchmark failed for {model_name} (batch {batch_size}): {e}\", exc_info=True)\n",
        "            return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <u> 7. Results Management </u>\n",
        "\n",
        "The ResultsManager class handles the final reporting. It prints a clean summary table to the console and saves the full, detailed results to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResultsManager:\n",
        "    \"\"\"Manages benchmark results and reporting.\"\"\"\n",
        "    @staticmethod\n",
        "    def save_results(results: List[BenchmarkResult], output_dir: Path = Path(\"results2\")):\n",
        "        if not results: return\n",
        "        output_dir.mkdir(exist_ok=True)\n",
        "        df = pd.DataFrame([r.to_dict() for r in results])\n",
        "        df.to_csv(output_dir / \"benchmark_results_debugging.csv\", index=False)\n",
        "        logger.info(f\"üíæ Results saved to '{output_dir.resolve()}'\")\n",
        "\n",
        "    @staticmethod\n",
        "    def print_summary(results: List[BenchmarkResult]):\n",
        "        if not results: return\n",
        "        df = pd.DataFrame([r.to_dict() for r in results])\n",
        "        summary_cols = [\"model\", \"batch_size\", \"provider\", \"avg_latency_ms\", \"p95_latency_ms\", \"accuracy\"]\n",
        "        print(\"\\n\" + \"=\"*80 + \"\\nüìä BENCHMARK SUMMARY\\n\" + \"=\"*80)\n",
        "        print(df[summary_cols].to_string(index=False, float_format=\"%.2f\"))\n",
        "        print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <u> 8. Benchmark Runner Functions </u>\n",
        "\n",
        "These two functions drive the whole process. discover_models scans the models directory to find all the .onnx files, and run_full_benchmark iterates through them, calling the orchestrator and collecting the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discover_models(models_dir: str) -> List[Tuple[str, Path]]:\n",
        "    \"\"\"Discover all available ONNX models, preferring statically quantised versions.\"\"\"\n",
        "    valid_models = []\n",
        "    logger.info(f\"--- Step 1: Discovering models in '{models_dir}' ---\")\n",
        "    for model_dir in Path(models_dir).iterdir():\n",
        "        if not model_dir.is_dir() or not (model_dir / \"onnx\").exists(): continue\n",
        "        \n",
        "        # Prioritise the statically quantised model\n",
        "        quant_path = model_dir / \"onnx\" / \"model-quantised.onnx\"\n",
        "        if quant_path.exists():\n",
        "            valid_models.append((f\"{model_dir.name}-quant\", quant_path))\n",
        "            logger.info(f\"  ‚úì Found Quantised: {model_dir.name} ({quant_path.name})\")\n",
        "        \n",
        "        # # Also add the standard model if it exists\n",
        "        # standard_path = model_dir / \"onnx\" / \"model.onnx\"\n",
        "        # if standard_path.exists():\n",
        "        #     valid_models.append((model_dir.name, standard_path))\n",
        "        #     logger.info(f\"  ‚úì Found Standard: {model_dir.name} ({standard_path.name})\")\n",
        "\n",
        "    if not valid_models: logger.warning(f\"No valid .onnx models found.\")\n",
        "    return valid_models\n",
        "\n",
        "def run_full_benchmark(models_dir: str, config: BenchmarkConfig):\n",
        "    \"\"\"Run the full benchmark suite on all discovered models.\"\"\"\n",
        "    all_results = []\n",
        "    \n",
        "    valid_models = discover_models(models_dir)\n",
        "    if not valid_models: return\n",
        "\n",
        "    logger.info(\"\\n--- Step 2: Running all benchmarks ---\")\n",
        "    for model_name, onnx_path in valid_models:\n",
        "        # --- KEY CHANGE: Load the correct tokenizer for THIS model ---\n",
        "        model_dir_path = onnx_path.parent.parent # Navigate up from 'model.onnx' -> 'onnx' -> model folder\n",
        "        logger.info(f\"Loading specific tokenizer from: {model_dir_path}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_dir_path)\n",
        "        \n",
        "        # Create a new benchmarker instance with the correct tokenizer\n",
        "        benchmarker = ONNXModelBenchmarker(config, tokenizer)\n",
        "        \n",
        "        for batch_size in config.batch_sizes:\n",
        "            result = benchmarker.benchmark_model(model_name, onnx_path, batch_size)\n",
        "            if result: all_results.append(result)\n",
        "    \n",
        "    if all_results:\n",
        "        logger.info(\"\\n--- Step 3: Reporting results ---\")\n",
        "        ResultsManager.print_summary(all_results)\n",
        "        ResultsManager.save_results(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <u> 9. Execute the Benchmark </u>\n",
        "\n",
        "This final cell is the \"run\" button. It defines your specific configuration, initialises the tokenizer, and then calls run_full_benchmark to kick off the entire process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-28 09:28:51,555 - INFO - --- Initialising Benchmark ---\n",
            "2025-07-28 09:28:51,557 - INFO - --- Step 1: Discovering models in 'models' ---\n",
            "2025-07-28 09:28:51,558 - INFO -   ‚úì Found Quantised: all-MiniLM-L6-v2-financial-sentiment (model-quantised.onnx)\n",
            "2025-07-28 09:28:51,559 - INFO -   ‚úì Found Quantised: distilbert-financial-sentiment (model-quantised.onnx)\n",
            "2025-07-28 09:28:51,560 - INFO -   ‚úì Found Quantised: finbert-tone-financial-sentiment (model-quantised.onnx)\n",
            "2025-07-28 09:28:51,561 - INFO -   ‚úì Found Quantised: tinybert-financial-classifier (model-quantised.onnx)\n",
            "2025-07-28 09:28:51,561 - INFO -   ‚úì Found Quantised: mobilebert-uncased-financial-sentiment (model-quantised.onnx)\n",
            "2025-07-28 09:28:51,562 - INFO - \n",
            "--- Step 2: Running all benchmarks ---\n",
            "2025-07-28 09:28:51,562 - INFO - Loading specific tokenizer from: models/all-MiniLM-L6-v2-financial-sentiment\n",
            "2025-07-28 09:28:51,666 - INFO - \n",
            "============================================================\n",
            "üöÄ BENCHMARKING: all-MiniLM-L6-v2-financial-sentiment-quant | Batch Size: 1\n",
            "============================================================\n",
            "2025-07-28 09:28:51,668 - INFO - Available providers: ['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']. Auto-selected: ['CPUExecutionProvider', 'CoreMLExecutionProvider']\n",
            "2025-07-28 09:28:51,672 - INFO - Creating ONNX session for model-quantised.onnx with providers: ['CPUExecutionProvider', 'CoreMLExecutionProvider']\n",
            "\u001b[0;93m2025-07-28 09:28:56.997381 [W:onnxruntime:, helper.cc:83 IsInputSupported] CoreML does not support input dim > 16384. Input:bert.embeddings.word_embeddings.weight_DequantizeLinear_Output, shape: {30522,384}\u001b[m\n",
            "2025-07-28 09:28:52,064 - INFO - üî• Warming up with 20 iterations...\n",
            "2025-07-28 09:28:53,047 - INFO - ‚è±Ô∏è  Running latency benchmark (100 iterations)...\n",
            "2025-07-28 09:28:57,060 - INFO - Loaded 4846 rows; using 1212 for accuracy testing.\n",
            "2025-07-28 09:28:57,062 - INFO - üéØ Evaluating accuracy on 500 samples (batch size: 1)...\n",
            "2025-07-28 09:29:12,457 - INFO -    -> Accuracy: 60.60%, F1 Score: 47.72%\n",
            "2025-07-28 09:29:12,476 - INFO - \n",
            "============================================================\n",
            "üöÄ BENCHMARKING: all-MiniLM-L6-v2-financial-sentiment-quant | Batch Size: 4\n",
            "============================================================\n",
            "2025-07-28 09:29:12,476 - INFO - Available providers: ['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']. Auto-selected: ['CPUExecutionProvider', 'CoreMLExecutionProvider']\n",
            "2025-07-28 09:29:12,477 - INFO - Creating ONNX session for model-quantised.onnx with providers: ['CPUExecutionProvider', 'CoreMLExecutionProvider']\n",
            "\u001b[0;93m2025-07-28 09:29:17.672054 [W:onnxruntime:, helper.cc:83 IsInputSupported] CoreML does not support input dim > 16384. Input:bert.embeddings.word_embeddings.weight_DequantizeLinear_Output, shape: {30522,384}\u001b[m\n",
            "2025-07-28 09:29:12,695 - INFO - üî• Warming up with 20 iterations...\n",
            "2025-07-28 09:29:15,193 - INFO - ‚è±Ô∏è  Running latency benchmark (100 iterations)...\n",
            "2025-07-28 09:29:26,195 - INFO - Loaded 4846 rows; using 1212 for accuracy testing.\n",
            "2025-07-28 09:29:26,197 - INFO - üéØ Evaluating accuracy on 500 samples (batch size: 4)...\n",
            "2025-07-28 09:29:39,667 - INFO -    -> Accuracy: 60.60%, F1 Score: 47.72%\n",
            "2025-07-28 09:29:39,683 - INFO - \n",
            "============================================================\n",
            "üöÄ BENCHMARKING: all-MiniLM-L6-v2-financial-sentiment-quant | Batch Size: 8\n",
            "============================================================\n",
            "2025-07-28 09:29:39,683 - INFO - Available providers: ['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']. Auto-selected: ['CPUExecutionProvider', 'CoreMLExecutionProvider']\n",
            "2025-07-28 09:29:39,684 - INFO - Creating ONNX session for model-quantised.onnx with providers: ['CPUExecutionProvider', 'CoreMLExecutionProvider']\n",
            "\u001b[0;93m2025-07-28 09:29:44.865211 [W:onnxruntime:, helper.cc:83 IsInputSupported] CoreML does not support input dim > 16384. Input:bert.embeddings.word_embeddings.weight_DequantizeLinear_Output, shape: {30522,384}\u001b[m\n",
            "2025-07-28 09:29:39,880 - INFO - üî• Warming up with 20 iterations...\n",
            "2025-07-28 09:29:43,631 - INFO - ‚è±Ô∏è  Running latency benchmark (100 iterations)...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      8\u001b[39m benchmark_config = BenchmarkConfig(\n\u001b[32m      9\u001b[39m     batch_sizes=[\u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m8\u001b[39m],\n\u001b[32m     10\u001b[39m     test_csv_path=ACCURACY_DATASET_PATH\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Run the benchmark (no longer needs a tokenizer passed in)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mrun_full_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODELS_DIRECTORY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmark_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Benchmark Finished ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mrun_full_benchmark\u001b[39m\u001b[34m(models_dir, config)\u001b[39m\n\u001b[32m     38\u001b[39m     benchmarker = ONNXModelBenchmarker(config, tokenizer)\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m config.batch_sizes:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m         result = \u001b[43mbenchmarker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbenchmark_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m result: all_results.append(result)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m all_results:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mONNXModelBenchmarker.benchmark_model\u001b[39m\u001b[34m(self, model_name, onnx_path, batch_size)\u001b[39m\n\u001b[32m     19\u001b[39m mem_before = PerformanceMonitor.measure_memory_usage()\n\u001b[32m     20\u001b[39m \u001b[38;5;28mself\u001b[39m.latency_benchmarker.warmup_session(session, valid_inputs)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m times = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlatency_benchmarker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmeasure_latency\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m mem_after = PerformanceMonitor.measure_memory_usage()\n\u001b[32m     24\u001b[39m avg_latency = statistics.mean(times)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mLatencyBenchmarker.measure_latency\u001b[39m\u001b[34m(self, session, inputs)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.config.benchmark_iterations):\n\u001b[32m     27\u001b[39m     start = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     times.append((time.perf_counter() - start) * \u001b[32m1000\u001b[39m) \u001b[38;5;66;03m# Append time in ms\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m times\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/deepmind_internship/venv-py311/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:273\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, output_names, input_feed, run_options)\u001b[39m\n\u001b[32m    271\u001b[39m     output_names = [output.name \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._outputs_meta]\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m C.EPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_fallback:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# --- Define configuration and run the benchmark ---\n",
        "MODELS_DIRECTORY = \"models\"\n",
        "ACCURACY_DATASET_PATH = \"data/FinancialPhraseBank/all-data.csv\"\n",
        "\n",
        "logger.info(\"--- Initialising Benchmark ---\")\n",
        "\n",
        "# Create the configuration for this run\n",
        "benchmark_config = BenchmarkConfig(\n",
        "    batch_sizes=[1, 4, 8],\n",
        "    test_csv_path=ACCURACY_DATASET_PATH\n",
        ")\n",
        "\n",
        "# Run the benchmark (no longer needs a tokenizer passed in)\n",
        "run_full_benchmark(MODELS_DIRECTORY, benchmark_config)\n",
        "\n",
        "logger.info(\"\\n--- Benchmark Finished ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPEaWosSVJk00KG/SJV/J7B",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
