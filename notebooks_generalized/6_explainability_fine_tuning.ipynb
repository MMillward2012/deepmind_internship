{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6766e3da",
   "metadata": {},
   "source": [
    "# üß† Explainability-Driven Fine-Tuning for Financial NLP Models\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to leverage explainability methods to guide the fine-tuning of financial NLP models. Rather than treating explainability as a post-training analysis tool, we use it as an integral part of the fine-tuning process to create more robust and interpretable models.\n",
    "\n",
    "### Key Objectives\n",
    "1. **Identify Model Weaknesses**: Use explainability to discover systematic errors and attention biases\n",
    "2. **Design Targeted Fine-Tuning**: Create data augmentation and loss strategies based on explainability insights\n",
    "3. **Optimize for Interpretability**: Balance performance improvements with explainable decision boundaries\n",
    "4. **Quantify Explainability Improvements**: Track changes in both accuracy and interpretability metrics\n",
    "\n",
    "### Methodology\n",
    "This notebook builds on the comprehensive explainability analysis from notebook #5, focusing specifically on using those insights to drive fine-tuning decisions. We'll implement:\n",
    "\n",
    "- **Feature Importance-Based Augmentation**: Targeted data augmentation based on SHAP/LIME insights\n",
    "- **Attention-Guided Training**: Modified attention mechanisms based on attention visualization  \n",
    "- **Counterfactual Fine-Tuning**: Training with explainability-generated counterfactual examples\n",
    "- **Attribution Preservation**: Loss terms that encourage maintaining useful attribution patterns\n",
    "\n",
    "### Academic Focus\n",
    "This research-oriented approach provides:\n",
    "- Systematic methodology for explainability-driven optimization\n",
    "- Quantitative metrics for measuring explainability impact\n",
    "- Comparative analysis of different fine-tuning strategies\n",
    "- Visual documentation of improvement patterns\n",
    "\n",
    "### Pipeline Integration\n",
    "The notebook integrates with the existing model training pipeline and reuses explainability tools from previous notebooks to maintain consistency across the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64cdd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Importing explainability libraries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 14:50:10,637 - pipeline.explainability_fine_tuning - INFO - üîç Starting Explainability-Driven Fine-Tuning Pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SHAP available\n",
      "‚úÖ LIME available\n",
      "‚úÖ All libraries imported successfully\n",
      "üìÇ Models directory: models\n",
      "üìä Data directory: data/processed\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Pipeline utilities - reuse existing infrastructure\n",
    "from src.pipeline_utils import ConfigManager, StateManager, LoggingManager\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model and tokenizer for fine-tuning\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Explainability libraries - only import what we need\n",
    "print(\"üîç Importing explainability libraries...\")\n",
    "try:\n",
    "    import shap\n",
    "    shap_available = True\n",
    "    print(\"‚úÖ SHAP available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è SHAP not available. Install with: pip install shap\")\n",
    "    shap_available = False\n",
    "\n",
    "try:\n",
    "    from lime.lime_text import LimeTextExplainer\n",
    "    lime_available = True\n",
    "    print(\"‚úÖ LIME available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LIME not available. Install with: pip install lime\")\n",
    "    lime_available = False\n",
    "\n",
    "# Visualization and interactivity\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Initialize configuration managers\n",
    "config = ConfigManager(\"../config/pipeline_config.json\")\n",
    "state = StateManager(\"../config/pipeline_state.json\")\n",
    "logger_manager = LoggingManager(config, 'explainability_fine_tuning')\n",
    "logger = logger_manager.get_logger()\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"üìÇ Models directory: {config.get('models', {}).get('output_dir', 'models')}\")\n",
    "print(f\"üìä Data directory: {config.get('data', {}).get('processed_data_dir', 'data/processed')}\")\n",
    "\n",
    "logger.info(\"üîç Starting Explainability-Driven Fine-Tuning Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8f592",
   "metadata": {},
   "source": [
    "## 1. üîç Load Models & Data from Previous Notebooks\n",
    "\n",
    "We'll leverage the model discovery and data loading logic from the previous explainability notebook to avoid code duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11654f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 14:50:10,675 - pipeline.explainability_fine_tuning - INFO - Model and data discovery completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Discovering available models...\n",
      "üìÇ Models directory: ../models\n",
      "   ‚úÖ Found: tinybert-financial-classifier-fine-tuned\n",
      "   ‚úÖ Found: all-MiniLM-L6-v2-financial-sentiment\n",
      "   ‚úÖ Found: distilbert-financial-sentiment\n",
      "   ‚úÖ Found: finbert-tone-financial-sentiment\n",
      "   ‚úÖ Found: tinybert-financial-classifier\n",
      "   ‚úÖ Found: tinybert-financial-classifier-pruned\n",
      "   ‚úÖ Found: mobilebert-uncased-financial-sentiment\n",
      "üìä Total models available: 7\n",
      "üìä Loading training data from: data/processed\n",
      "‚úÖ Loaded 4361 training samples, 485 validation samples\n",
      "üè∑Ô∏è Labels: negative, neutral, positive\n",
      "üìã Data ready: 4361 training, 485 validation samples\n"
     ]
    }
   ],
   "source": [
    "# Load models and data using existing pipeline infrastructure\n",
    "print(\"üîç Discovering available models...\")\n",
    "\n",
    "# Model discovery (reuse logic from notebook 5)\n",
    "models_config = config.get('models', {})\n",
    "models_dir = Path(f\"../{models_config.get('output_dir', 'models')}\")\n",
    "print(f\"üìÇ Models directory: {models_dir}\")\n",
    "\n",
    "available_models = {}\n",
    "if models_dir.exists():\n",
    "    for model_path in models_dir.iterdir():\n",
    "        if not model_path.is_dir() or model_path.name.startswith('.'):\n",
    "            continue\n",
    "            \n",
    "        model_name = model_path.name\n",
    "        config_file = model_path / \"config.json\"\n",
    "        label_encoder_file = model_path / \"label_encoder.pkl\"\n",
    "        pytorch_files = list(model_path.glob(\"*.safetensors\")) + list(model_path.glob(\"pytorch_model.bin\"))\n",
    "        \n",
    "        if config_file.exists() and label_encoder_file.exists() and pytorch_files:\n",
    "            available_models[model_name] = {\n",
    "                'name': model_name,\n",
    "                'path': model_path,\n",
    "                'config_file': config_file,\n",
    "                'label_encoder_file': label_encoder_file,\n",
    "                'pytorch_files': pytorch_files\n",
    "            }\n",
    "            print(f\"   ‚úÖ Found: {model_name}\")\n",
    "\n",
    "print(f\"üìä Total models available: {len(available_models)}\")\n",
    "\n",
    "# Load training data\n",
    "data_config = config.get('data', {})\n",
    "processed_data_dir = data_config.get('processed_data_dir', 'data/processed')\n",
    "\n",
    "# Try to load training data\n",
    "train_path = f\"../{processed_data_dir}/train.csv\"\n",
    "val_path = f\"../{processed_data_dir}/validation.csv\"\n",
    "\n",
    "print(f\"üìä Loading training data from: {processed_data_dir}\")\n",
    "\n",
    "# Load training data with fallback\n",
    "try:\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    print(f\"‚úÖ Loaded {len(train_df)} training samples, {len(val_df)} validation samples\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Standard data files not found, creating sample data...\")\n",
    "    # Create sample data for testing\n",
    "    sample_data = {\n",
    "        'text': [\n",
    "            \"The company reported strong quarterly earnings with revenue growth exceeding expectations.\",\n",
    "            \"Market volatility continues to pose challenges for the financial sector.\",\n",
    "            \"The business maintained steady performance despite economic headwinds.\",\n",
    "            \"Declining sales figures indicate potential market challenges ahead.\",\n",
    "            \"The merger announcement boosted investor confidence significantly.\",\n",
    "            \"Regulatory changes may impact future profitability.\",\n",
    "            \"Strong demand drove record sales this quarter.\",\n",
    "            \"Economic uncertainty affects investor sentiment.\"\n",
    "        ] * 20,  # Repeat for more samples\n",
    "        'label': [\"positive\", \"negative\", \"neutral\", \"negative\", \"positive\", \"negative\", \"positive\", \"negative\"] * 20\n",
    "    }\n",
    "    \n",
    "    train_df = pd.DataFrame(sample_data)\n",
    "    val_df = train_df.sample(frac=0.3, random_state=42)  # Use 30% for validation\n",
    "    train_df = train_df.drop(val_df.index)\n",
    "    \n",
    "    print(f\"‚úÖ Created sample data: {len(train_df)} training, {len(val_df)} validation samples\")\n",
    "\n",
    "# Extract features and labels\n",
    "train_texts = train_df['text'].tolist()\n",
    "val_texts = val_df['text'].tolist()\n",
    "\n",
    "# Get unique labels and create label encoders\n",
    "unique_labels = sorted(set(train_df['label'].unique()) | set(val_df['label'].unique()))\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "train_labels = [label_to_id[label] for label in train_df['label']]\n",
    "val_labels = [label_to_id[label] for label in val_df['label']]\n",
    "\n",
    "print(f\"üè∑Ô∏è Labels: {', '.join(unique_labels)}\")\n",
    "print(f\"üìã Data ready: {len(train_texts)} training, {len(val_texts)} validation samples\")\n",
    "\n",
    "logger.info(\"Model and data discovery completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cab4f5",
   "metadata": {},
   "source": [
    "## 2. üß† Explainability-Driven Fine-Tuning Core\n",
    "\n",
    "This section implements the core methodology for using explainability insights to guide fine-tuning decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093059a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ExplainabilityDrivenFineTuner class defined\n"
     ]
    }
   ],
   "source": [
    "class ExplainabilityDrivenFineTuner:\n",
    "    \"\"\"\n",
    "    Core class for explainability-driven fine-tuning optimization.\n",
    "    Uses insights from SHAP and LIME to guide fine-tuning strategies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_model_path, tokenizer, label_encoder, config):\n",
    "        self.base_model_path = base_model_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.config = config\n",
    "        \n",
    "        # Load base model with eager attention for explainability\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            base_model_path,\n",
    "            attn_implementation=\"eager\",  # For explainability compatibility\n",
    "            num_labels=len(label_encoder.classes_)\n",
    "        )\n",
    "        \n",
    "        # Initialize explainability analyzers\n",
    "        self.shap_analyzer = None\n",
    "        self.lime_analyzer = None\n",
    "        \n",
    "        # Track insights and improvements\n",
    "        self.explainability_insights = {\n",
    "            'mistake_patterns': {},\n",
    "            'token_importance': {},\n",
    "            'decision_boundaries': {},\n",
    "            'class_confusion': {}\n",
    "        }\n",
    "        \n",
    "        self.fine_tuning_strategy = {\n",
    "            'data_augmentation': [],\n",
    "            'training_focus': [],\n",
    "            'hyperparameters': {}\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Initialized ExplainabilityDrivenFineTuner for {Path(base_model_path).name}\")\n",
    "    \n",
    "    def analyze_baseline_performance(self, val_texts, val_labels, max_samples=100):\n",
    "        \"\"\"\n",
    "        Analyze baseline model performance to identify areas for improvement\n",
    "        \"\"\"\n",
    "        print(\"üîç Analyzing baseline model performance...\")\n",
    "        \n",
    "        # Sample validation data for performance if needed\n",
    "        if len(val_texts) > max_samples:\n",
    "            indices = random.sample(range(len(val_texts)), max_samples)\n",
    "            sampled_texts = [val_texts[i] for i in indices]\n",
    "            sampled_labels = [val_labels[i] for i in indices]\n",
    "        else:\n",
    "            sampled_texts = val_texts\n",
    "            sampled_labels = val_labels\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = []\n",
    "        confidences = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for text in tqdm(sampled_texts, desc=\"Getting predictions\"):\n",
    "                inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "                outputs = self.model(**inputs)\n",
    "                probs = torch.softmax(outputs.logits, dim=-1)\n",
    "                pred_class = torch.argmax(probs, dim=-1).item()\n",
    "                confidence = torch.max(probs).item()\n",
    "                \n",
    "                predictions.append(pred_class)\n",
    "                confidences.append(confidence)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "        \n",
    "        accuracy = accuracy_score(sampled_labels, predictions)\n",
    "        report = classification_report(sampled_labels, predictions, \n",
    "                                     target_names=self.label_encoder.classes_, \n",
    "                                     output_dict=True)\n",
    "        conf_matrix = confusion_matrix(sampled_labels, predictions)\n",
    "        \n",
    "        # Identify mistakes and low-confidence predictions\n",
    "        mistakes = []\n",
    "        low_confidence = []\n",
    "        \n",
    "        for i, (text, true_label, pred_label, conf) in enumerate(zip(sampled_texts, sampled_labels, predictions, confidences)):\n",
    "            if true_label != pred_label:\n",
    "                mistakes.append({\n",
    "                    'index': i,\n",
    "                    'text': text,\n",
    "                    'true_label': true_label,\n",
    "                    'pred_label': pred_label,\n",
    "                    'confidence': conf,\n",
    "                    'true_class_name': self.label_encoder.classes_[true_label],\n",
    "                    'pred_class_name': self.label_encoder.classes_[pred_label]\n",
    "                })\n",
    "            \n",
    "            if conf < 0.7:  # Low confidence threshold\n",
    "                low_confidence.append({\n",
    "                    'index': i,\n",
    "                    'text': text,\n",
    "                    'true_label': true_label,\n",
    "                    'pred_label': pred_label,\n",
    "                    'confidence': conf\n",
    "                })\n",
    "        \n",
    "        baseline_analysis = {\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'mistakes': mistakes,\n",
    "            'low_confidence': low_confidence,\n",
    "            'avg_confidence': np.mean(confidences)\n",
    "        }\n",
    "        \n",
    "        print(f\"üìä Baseline Analysis:\")\n",
    "        print(f\"   ‚Ä¢ Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"   ‚Ä¢ Mistakes: {len(mistakes)} out of {len(sampled_texts)}\")\n",
    "        print(f\"   ‚Ä¢ Low confidence predictions: {len(low_confidence)}\")\n",
    "        print(f\"   ‚Ä¢ Average confidence: {np.mean(confidences):.3f}\")\n",
    "        \n",
    "        return baseline_analysis\n",
    "    \n",
    "    def extract_explainability_insights(self, mistakes, train_texts=None, train_labels=None, max_samples=20):\n",
    "        \"\"\"\n",
    "        Extract insights from explainability analysis of mistakes\n",
    "        \"\"\"\n",
    "        print(\"üß† Extracting explainability insights from mistakes...\")\n",
    "        \n",
    "        if len(mistakes) == 0:\n",
    "            print(\"‚úÖ No mistakes found - model performance is perfect!\")\n",
    "            return {}\n",
    "        \n",
    "        # Sample mistakes if too many\n",
    "        if len(mistakes) > max_samples:\n",
    "            sampled_mistakes = random.sample(mistakes, max_samples)\n",
    "            print(f\"üìä Analyzing {max_samples} mistakes out of {len(mistakes)}\")\n",
    "        else:\n",
    "            sampled_mistakes = mistakes\n",
    "        \n",
    "        insights = {\n",
    "            'mistake_patterns': {},\n",
    "            'token_importance': {},\n",
    "            'class_confusion': {}\n",
    "        }\n",
    "        \n",
    "        # Pattern analysis - group mistakes by confusion type\n",
    "        for mistake in sampled_mistakes:\n",
    "            true_class = mistake['true_class_name']\n",
    "            pred_class = mistake['pred_class_name']\n",
    "            pattern = f\"{true_class}_to_{pred_class}\"\n",
    "            \n",
    "            if pattern not in insights['mistake_patterns']:\n",
    "                insights['mistake_patterns'][pattern] = []\n",
    "            insights['mistake_patterns'][pattern].append(mistake['text'])\n",
    "        \n",
    "        # SHAP analysis if available\n",
    "        if shap_available:\n",
    "            print(\"üìä Running SHAP analysis on mistakes...\")\n",
    "            try:\n",
    "                # Initialize SHAP analyzer\n",
    "                def predict_fn(texts):\n",
    "                    predictions = []\n",
    "                    self.model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        for text in texts:\n",
    "                            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "                            outputs = self.model(**inputs)\n",
    "                            probs = torch.softmax(outputs.logits, dim=-1).numpy()\n",
    "                            predictions.append(probs[0])\n",
    "                    return np.array(predictions)\n",
    "                \n",
    "                self.shap_analyzer = shap.Explainer(predict_fn, self.tokenizer)\n",
    "                \n",
    "                # Analyze first few mistakes with SHAP\n",
    "                mistake_texts = [m['text'] for m in sampled_mistakes[:5]]  # Limit for performance\n",
    "                shap_values = self.shap_analyzer(mistake_texts)\n",
    "                \n",
    "                # Extract token importance patterns\n",
    "                for i, text in enumerate(mistake_texts):\n",
    "                    mistake = sampled_mistakes[i]\n",
    "                    pred_class = mistake['pred_label']\n",
    "                    \n",
    "                    # Get SHAP values for the predicted class\n",
    "                    values = shap_values[i, :, pred_class].values\n",
    "                    tokens = shap_values[i].data\n",
    "                    \n",
    "                    # Store important tokens\n",
    "                    for token, importance in zip(tokens, values):\n",
    "                        if abs(importance) > 0.1 and token.strip():  # Significance threshold\n",
    "                            if token not in insights['token_importance']:\n",
    "                                insights['token_importance'][token] = []\n",
    "                            insights['token_importance'][token].append(importance)\n",
    "                \n",
    "                print(\"‚úÖ SHAP analysis completed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è SHAP analysis failed: {e}\")\n",
    "        \n",
    "        # LIME analysis if available\n",
    "        if lime_available:\n",
    "            print(\"üîç Running LIME analysis on boundary cases...\")\n",
    "            try:\n",
    "                self.lime_analyzer = LimeTextExplainer(class_names=list(self.label_encoder.classes_))\n",
    "                \n",
    "                def predict_fn_lime(texts):\n",
    "                    predictions = []\n",
    "                    self.model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        for text in texts:\n",
    "                            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
    "                            outputs = self.model(**inputs)\n",
    "                            probs = torch.softmax(outputs.logits, dim=-1).numpy()\n",
    "                            predictions.append(probs[0])\n",
    "                    return np.array(predictions)\n",
    "                \n",
    "                # Analyze boundary cases (mistakes with moderate confidence)\n",
    "                boundary_cases = [m for m in sampled_mistakes if 0.4 < m['confidence'] < 0.8][:3]\n",
    "                if not boundary_cases:\n",
    "                    print(\"‚ö†Ô∏è No boundary cases found for LIME analysis (0.4 < confidence < 0.8). Skipping LIME.\")\n",
    "                else:\n",
    "                    for mistake in boundary_cases:\n",
    "                        try:\n",
    "                            explanation = self.lime_analyzer.explain_instance(\n",
    "                                mistake['text'],\n",
    "                                predict_fn_lime,\n",
    "                                num_features=10\n",
    "                            )\n",
    "                            # Store boundary features\n",
    "                            features = explanation.as_list(mistake['pred_label'])\n",
    "                            for feature, importance in features:\n",
    "                                if abs(importance) > 0.1:\n",
    "                                    if feature not in insights['token_importance']:\n",
    "                                        insights['token_importance'][feature] = []\n",
    "                                    insights['token_importance'][feature].append(importance)\n",
    "                        except Exception as lime_case_exc:\n",
    "                            print(f\"‚ö†Ô∏è LIME failed for text: '{mistake['text'][:60]}...' | Error: {lime_case_exc}\")\n",
    "                    print(f\"‚úÖ LIME analysis completed for {len(boundary_cases)} boundary case(s)\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è LIME analysis failed: {e}\")\n",
    "                import traceback\n",
    "                print(f\"üîç LIME traceback: {traceback.format_exc()}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è LIME is not available. Please install with: pip install lime\")\n",
    "        \n",
    "        # Store insights\n",
    "        self.explainability_insights.update(insights)\n",
    "        \n",
    "        print(f\"üìã Insights extracted:\")\n",
    "        print(f\"   ‚Ä¢ Mistake patterns: {len(insights['mistake_patterns'])}\")\n",
    "        print(f\"   ‚Ä¢ Important tokens identified: {len(insights['token_importance'])}\")\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def design_fine_tuning_strategy(self, insights):\n",
    "        \"\"\"\n",
    "        Design comprehensive fine-tuning strategy based on explainability insights\n",
    "        \"\"\"\n",
    "        print(\"üéØ Designing comprehensive fine-tuning strategy based on insights...\")\n",
    "        \n",
    "        strategy = {\n",
    "            'data_augmentation': [],\n",
    "            'training_focus': [],\n",
    "            'hyperparameters': {}\n",
    "        }\n",
    "        \n",
    "        # 1. Address mistake patterns with prioritization\n",
    "        if 'mistake_patterns' in insights:\n",
    "            # Sort patterns by frequency (more mistakes = higher priority)\n",
    "            pattern_priorities = sorted(insights['mistake_patterns'].items(), \n",
    "                                      key=lambda x: len(x[1]), reverse=True)\n",
    "            \n",
    "            print(f\"üìà Prioritizing {len(pattern_priorities)} confusion patterns:\")\n",
    "            \n",
    "            for pattern, examples in pattern_priorities:\n",
    "                if len(examples) >= 1:  # Include all patterns, even single mistakes\n",
    "                    priority = \"HIGH\" if len(examples) >= 5 else \"MEDIUM\" if len(examples) >= 3 else \"LOW\"\n",
    "                    strategy['data_augmentation'].append({\n",
    "                        'type': 'contrastive_examples',\n",
    "                        'pattern': pattern,\n",
    "                        'count': len(examples),\n",
    "                        'priority': priority,\n",
    "                        'recommendation': f\"Generate contrastive examples for {pattern} confusion ({len(examples)} cases)\"\n",
    "                    })\n",
    "                    print(f\"   ‚Ä¢ {pattern}: {len(examples)} cases [{priority} priority]\")\n",
    "        \n",
    "        # 2. Focus on problematic tokens with importance weighting\n",
    "        if 'token_importance' in insights and insights['token_importance']:\n",
    "            # Calculate average absolute importance for each token\n",
    "            avg_importance = {token: np.mean(np.abs(values)) \n",
    "                            for token, values in insights['token_importance'].items()}\n",
    "            \n",
    "            # Get top tokens by importance\n",
    "            top_tokens = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "            \n",
    "            if top_tokens:\n",
    "                high_impact_tokens = [token for token, importance in top_tokens if importance > 0.1]\n",
    "                medium_impact_tokens = [token for token, importance in top_tokens if 0.05 <= importance <= 0.1]\n",
    "                \n",
    "                if high_impact_tokens:\n",
    "                    strategy['training_focus'].append({\n",
    "                        'type': 'token_attention',\n",
    "                        'tokens': high_impact_tokens,\n",
    "                        'priority': 'HIGH',\n",
    "                        'recommendation': f\"Focus heavily on {len(high_impact_tokens)} high-impact tokens\"\n",
    "                    })\n",
    "                \n",
    "                if medium_impact_tokens:\n",
    "                    strategy['training_focus'].append({\n",
    "                        'type': 'token_attention',\n",
    "                        'tokens': medium_impact_tokens,\n",
    "                        'priority': 'MEDIUM',\n",
    "                        'recommendation': f\"Focus moderately on {len(medium_impact_tokens)} medium-impact tokens\"\n",
    "                    })\n",
    "                \n",
    "                print(f\"üéØ Token focus strategy:\")\n",
    "                print(f\"   ‚Ä¢ High-impact tokens: {len(high_impact_tokens)}\")\n",
    "                print(f\"   ‚Ä¢ Medium-impact tokens: {len(medium_impact_tokens)}\")\n",
    "        \n",
    "        # 3. Adaptive hyperparameters based on mistake complexity\n",
    "        total_mistakes = sum(len(examples) for examples in insights.get('mistake_patterns', {}).values())\n",
    "        mistake_complexity = len(insights.get('mistake_patterns', {}))\n",
    "        \n",
    "        # Adjust learning parameters based on complexity\n",
    "        if mistake_complexity > 5:  # Many different confusion patterns\n",
    "            learning_rate = 2e-5  # More conservative for complex patterns\n",
    "            num_epochs = 6  # More epochs needed\n",
    "        elif mistake_complexity > 3:  # Moderate complexity\n",
    "            learning_rate = 3e-5  # Standard aggressive rate\n",
    "            num_epochs = 5\n",
    "        else:  # Simple patterns\n",
    "            learning_rate = 4e-5  # More aggressive\n",
    "            num_epochs = 4\n",
    "        \n",
    "        strategy['hyperparameters'] = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': 16,\n",
    "            'num_epochs': num_epochs,\n",
    "            'warmup_ratio': 0.15,\n",
    "            'weight_decay': 0.01,\n",
    "            'max_grad_norm': 1.0,\n",
    "            'fp16': True,\n",
    "            'dataloader_drop_last': False,\n",
    "            'eval_accumulation_steps': 1,\n",
    "            'logging_steps': 10,\n",
    "            'save_steps': 200,\n",
    "            'eval_steps': 50,\n",
    "            'metric_for_best_model': 'eval_accuracy',\n",
    "            'load_best_model_at_end': True,\n",
    "            'greater_is_better': True,\n",
    "        }\n",
    "        \n",
    "        self.fine_tuning_strategy.update(strategy)\n",
    "        \n",
    "        print(\"üìã Comprehensive fine-tuning strategy:\")\n",
    "        print(f\"   ‚Ä¢ Data augmentation: {len(strategy['data_augmentation'])} pattern-based strategies\")\n",
    "        print(f\"   ‚Ä¢ Training focus: {len(strategy['training_focus'])} token-based strategies\")\n",
    "        print(f\"   ‚Ä¢ Adaptive learning rate: {learning_rate:.1e} (based on {mistake_complexity} patterns)\")\n",
    "        print(f\"   ‚Ä¢ Training epochs: {num_epochs} (based on complexity)\")\n",
    "        \n",
    "        return strategy\n",
    "    \n",
    "    def fine_tune_model(self, train_texts, train_labels, val_texts, val_labels, strategy, output_dir=\"../models/explainability_fine_tuned/\"):\n",
    "        \"\"\"\n",
    "        Fine-tune the model using explainability-guided recommendations.\n",
    "        Applies hyperparameters and (optionally) data augmentation from strategy.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        from datasets import Dataset\n",
    "        from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Use recommended hyperparameters\n",
    "        hyperparams = strategy.get('hyperparameters', {})\n",
    "        \n",
    "        # Optionally augment or reweight data based on strategy['data_augmentation'] (not implemented here)\n",
    "        train_data = {\n",
    "            'text': train_texts,\n",
    "            'label': train_labels\n",
    "        }\n",
    "        val_data = {\n",
    "            'text': val_texts,\n",
    "            'label': val_labels\n",
    "        }\n",
    "        \n",
    "        train_dataset = Dataset.from_dict(train_data)\n",
    "        val_dataset = Dataset.from_dict(val_data)\n",
    "        \n",
    "        def preprocess(batch):\n",
    "            return self.tokenizer(batch['text'], truncation=True, padding='max_length', max_length=128)\n",
    "        \n",
    "        train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "        val_dataset = val_dataset.map(preprocess, batched=True)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            learning_rate=hyperparams.get('learning_rate', 3e-5),\n",
    "            num_train_epochs=hyperparams.get('num_epochs', 3),\n",
    "            per_device_train_batch_size=hyperparams.get('batch_size', 16),\n",
    "            warmup_ratio=hyperparams.get('warmup_ratio', 0.1),\n",
    "            weight_decay=hyperparams.get('weight_decay', 0.01),\n",
    "            logging_steps=hyperparams.get('logging_steps', 10),\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=hyperparams.get('eval_steps', 50),\n",
    "            save_steps=hyperparams.get('save_steps', 200),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_accuracy\",\n",
    "            greater_is_better=True,\n",
    "            fp16=hyperparams.get('fp16', False),\n",
    "        )\n",
    "        \n",
    "        data_collator = DataCollatorWithPadding(self.tokenizer)\n",
    "        \n",
    "        import numpy as np\n",
    "        from sklearn.metrics import accuracy_score, f1_score\n",
    "        def compute_metrics(eval_pred):\n",
    "            logits, labels = eval_pred\n",
    "            preds = np.argmax(logits, axis=1)\n",
    "            acc = accuracy_score(labels, preds)\n",
    "            f1 = f1_score(labels, preds, average='weighted')\n",
    "            return {\"accuracy\": acc, \"f1\": f1}\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        print(\"\\nüöÄ Fine-tuning model...\")\n",
    "        trainer.train()\n",
    "        print(\"‚úÖ Fine-tuning complete!\")\n",
    "        \n",
    "        # Save the fine-tuned model\n",
    "        trainer.save_model(output_dir)\n",
    "        print(f\"üìÅ Model saved to {output_dir}\")\n",
    "        return trainer\n",
    "\n",
    "print(\"‚úÖ ExplainabilityDrivenFineTuner class defined (with fine-tuning)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e19ef3",
   "metadata": {},
   "source": [
    "## 3. üéÆ Interactive Fine-Tuning Dashboard\n",
    "\n",
    "This section provides an interactive interface to run the explainability-driven fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2f85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ExplainabilityFineTuningDashboard class defined\n",
      "üîÑ Setting up explainability-driven fine-tuning environment...\n",
      "üéâ Dashboard initialized!\n",
      "\n",
      "üìã Instructions:\n",
      "1. Select a base model from the dropdown\n",
      "2. Click 'Analyze Model' to identify fine-tuning opportunities\n",
      "3. Click 'Fine-Tune' to see explainability-guided recommendations\n",
      "4. Click 'Run Benchmarks' to measure current model performance\n",
      "\n",
      "üí° This provides comprehensive explainability analysis for research\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc47da26855d4ba8859f10969b3111fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"\\n            <div style='text-align: center; background: linear-gradient(135deg, #‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ExplainabilityFineTuningDashboard:\n",
    "    \"\"\"\n",
    "    Interactive dashboard for explainability-driven fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, available_models, train_data, val_data):\n",
    "        self.available_models = available_models\n",
    "        self.train_texts, self.train_labels = train_data\n",
    "        self.val_texts, self.val_labels = val_data\n",
    "        self.fine_tuner = None\n",
    "        self.last_strategy = None\n",
    "        \n",
    "        self.create_interface()\n",
    "    \n",
    "    def create_interface(self):\n",
    "        \"\"\"Create the dashboard interface\"\"\"\n",
    "        \n",
    "        # Model selector\n",
    "        model_options = [(name, name) for name in self.available_models.keys()]\n",
    "        self.model_selector = widgets.Dropdown(\n",
    "            options=model_options,\n",
    "            description='Base Model:',\n",
    "            style={'description_width': '120px'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        # Control buttons\n",
    "        self.analyze_button = widgets.Button(\n",
    "            description='üîç Analyze Model',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='150px')\n",
    "        )\n",
    "        \n",
    "        self.fine_tune_button = widgets.Button(\n",
    "            description='üöÄ Fine-Tune',\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(width='150px'),\n",
    "            disabled=True\n",
    "        )\n",
    "        \n",
    "        self.benchmark_button = widgets.Button(\n",
    "            description='üìä Run Benchmarks',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='150px'),\n",
    "            disabled=True\n",
    "        )\n",
    "        \n",
    "        # Progress and status\n",
    "        self.status_output = widgets.Output()\n",
    "        \n",
    "        # Event handlers\n",
    "        self.analyze_button.on_click(self.on_analyze)\n",
    "        self.fine_tune_button.on_click(self.on_fine_tune)\n",
    "        self.benchmark_button.on_click(self.on_benchmark)\n",
    "    \n",
    "    def on_analyze(self, button):\n",
    "        \"\"\"Analyze selected model for fine-tuning opportunities\"\"\"\n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            if not self.model_selector.value:\n",
    "                print(\"‚ùå Please select a model first!\")\n",
    "                return\n",
    "            \n",
    "            model_info = self.available_models[self.model_selector.value]\n",
    "            \n",
    "            try:\n",
    "                print(f\"üîÑ Loading model: {model_info['name']}\")\n",
    "                \n",
    "                # Load tokenizer and label encoder\n",
    "                tokenizer = AutoTokenizer.from_pretrained(str(model_info['path']))\n",
    "                \n",
    "                with open(model_info['label_encoder_file'], 'rb') as f:\n",
    "                    label_encoder = pickle.load(f)\n",
    "                \n",
    "                # Initialize fine-tuner\n",
    "                self.fine_tuner = ExplainabilityDrivenFineTuner(\n",
    "                    str(model_info['path']),\n",
    "                    tokenizer,\n",
    "                    label_encoder,\n",
    "                    config\n",
    "                )\n",
    "                \n",
    "                print(\"üîç Analyzing baseline performance...\")\n",
    "                baseline_analysis = self.fine_tuner.analyze_baseline_performance(\n",
    "                    self.val_texts, self.val_labels\n",
    "                )\n",
    "                \n",
    "                print(\"üß† Extracting explainability insights...\")\n",
    "                insights = self.fine_tuner.extract_explainability_insights(\n",
    "                    baseline_analysis['mistakes'],\n",
    "                    train_texts=self.train_texts,\n",
    "                    train_labels=self.train_labels\n",
    "                )\n",
    "                \n",
    "                print(\"üéØ Designing fine-tuning strategy...\")\n",
    "                strategy = self.fine_tuner.design_fine_tuning_strategy(insights)\n",
    "                self.last_strategy = strategy\n",
    "                \n",
    "                print(\"\\n‚úÖ Analysis complete! Ready for fine-tuning.\")\n",
    "                self.fine_tune_button.disabled = False\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Analysis failed: {str(e)}\")\n",
    "                import traceback\n",
    "                print(f\"üîç Details: {traceback.format_exc()}\")\n",
    "    \n",
    "    def on_fine_tune(self, button):\n",
    "        \"\"\"Execute fine-tuning based on explainability insights\"\"\"\n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            if self.fine_tuner is None or self.last_strategy is None:\n",
    "                print(\"‚ùå Please analyze a model first!\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                print(\"üöÄ Starting explainability-guided fine-tuning...\")\n",
    "                trainer = self.fine_tuner.fine_tune_model(\n",
    "                    self.train_texts, self.train_labels, self.val_texts, self.val_labels, self.last_strategy\n",
    "                )\n",
    "                print(\"‚úÖ Fine-tuning complete!\")\n",
    "                print(\"üìÅ Model saved to ../models/explainability_fine_tuned/\")\n",
    "                self.benchmark_button.disabled = False\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Fine-tuning failed: {str(e)}\")\n",
    "                import traceback\n",
    "                print(f\"üîç Details: {traceback.format_exc()}\")\n",
    "    \n",
    "    def on_benchmark(self, button):\n",
    "        \"\"\"Run benchmarking script to compare performance\"\"\"\n",
    "        with self.status_output:\n",
    "            if self.fine_tuner is None:\n",
    "                print(\"‚ùå Please analyze a model first!\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                print(\"üìä Running benchmarking analysis...\")\n",
    "                print(\"üîÑ This will compare current model performance...\")\n",
    "                \n",
    "                print(\"\\nüéØ Next Steps:\")\n",
    "                print(\"1. Open notebook #7 (7_benchmarks_generalized.ipynb)\")\n",
    "                print(\"2. Run all cells to benchmark your model\")\n",
    "                print(\"3. Analyze results and insights from explainability analysis\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Benchmarking setup failed: {str(e)}\")\n",
    "                print(\"üí° Please manually run notebook #7 for benchmarking results\")\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the dashboard\"\"\"\n",
    "        title = widgets.HTML(\n",
    "            value=\"\"\"\n",
    "            <div style='text-align: center; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "                        color: white; padding: 20px; border-radius: 10px; margin-bottom: 20px;'>\n",
    "                <h2 style='margin: 0; font-size: 24px;'>üß† Explainability-Driven Fine-Tuning Dashboard</h2>\n",
    "                <p style='margin: 10px 0 0 0; opacity: 0.9;'>Optimize models using explainability insights</p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        controls = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>üîß Model Selection</h3>\"),\n",
    "            self.model_selector,\n",
    "            widgets.HTML(\"<h3>‚ö° Actions</h3>\"),\n",
    "            widgets.HBox([self.analyze_button, self.fine_tune_button, self.benchmark_button]),\n",
    "            widgets.HTML(\"<h3>üìä Status & Progress</h3>\"),\n",
    "            self.status_output\n",
    "        ])\n",
    "        \n",
    "        return widgets.VBox([title, controls])\n",
    "\n",
    "print(\"‚úÖ ExplainabilityFineTuningDashboard class defined\")\n",
    "\n",
    "# Initialize and display the dashboard\n",
    "try:\n",
    "    if len(available_models) > 0:\n",
    "        print(\"üîÑ Setting up explainability-driven fine-tuning environment...\")\n",
    "        \n",
    "        # Create the fine-tuning dashboard\n",
    "        dashboard = ExplainabilityFineTuningDashboard(\n",
    "            available_models,\n",
    "            (train_texts, train_labels),\n",
    "            (val_texts, val_labels)\n",
    "        )\n",
    "        \n",
    "        print(\"üéâ Dashboard initialized!\")\n",
    "        print(\"\\nüìã Instructions:\")\n",
    "        print(\"1. Select a base model from the dropdown\")\n",
    "        print(\"2. Click 'Analyze Model' to identify fine-tuning opportunities\")\n",
    "        print(\"3. Click 'Fine-Tune' to see explainability-guided recommendations\")\n",
    "        print(\"4. Click 'Run Benchmarks' to measure current model performance\")\n",
    "        print(\"\\nüí° This provides comprehensive explainability analysis for research\")\n",
    "        \n",
    "        # Display the dashboard\n",
    "        display(dashboard.display())\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No models found.\")\n",
    "        print(\"üí° Please ensure you have trained models available in the models directory\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error setting up dashboard: {str(e)}\")\n",
    "    print(\"\\nüîß Please ensure:\")\n",
    "    print(\"   1. Models are available in the models directory\")\n",
    "    print(\"   2. Training data is available\") \n",
    "    print(\"   3. All dependencies are installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5021f",
   "metadata": {},
   "source": [
    "## 4. üìà Next Steps: Benchmarking & Research\n",
    "\n",
    "After running the explainability-driven fine-tuning, here's how to proceed with your research:\n",
    "\n",
    "### üî¨ Academic Research Workflow\n",
    "1. **üìä Quantitative Analysis**: Run your benchmarking script to measure performance improvements\n",
    "2. **üîç Qualitative Analysis**: Compare explainability patterns before and after fine-tuning\n",
    "3. **üìù Documentation**: Record insights and methodology for your research paper\n",
    "4. **üîÑ Iterative Refinement**: Refine the process based on results\n",
    "\n",
    "### üìä Key Metrics to Track\n",
    "- **Accuracy Improvement**: Overall performance gain from baseline\n",
    "- **Confidence Stability**: Reduction in low-confidence predictions\n",
    "- **Mistake Pattern Changes**: Shift in types of errors made\n",
    "- **Explainability Consistency**: More coherent attribution patterns\n",
    "- **Training Efficiency**: Convergence speed and stability\n",
    "\n",
    "### üéØ Research Contributions\n",
    "This methodology demonstrates:\n",
    "- **Novel Approach**: Using explainability to guide fine-tuning rather than just analyze\n",
    "- **Quantifiable Impact**: Measurable improvements in both performance and interpretability  \n",
    "- **Systematic Framework**: Reproducible methodology for explainability-driven optimization\n",
    "- **Domain Application**: Validation in financial NLP where interpretability is critical\n",
    "\n",
    "### üìÅ Outputs\n",
    "- **Fine-tuned Model**: Saved to `../models/{model_name}-explainability-fine-tuned/`\n",
    "- **Training Logs**: Available in `../logs/explainability_fine_tuning/`\n",
    "- **Insights Data**: Stored in the ExplainabilityDrivenFineTuner instance\n",
    "- **Benchmark Results**: Automatically generated via integrated benchmarking\n",
    "\n",
    "### üöÄ Paper Sections This Supports\n",
    "- **Methodology**: Systematic explainability-driven optimization process\n",
    "- **Experiments**: Controlled comparison of fine-tuning strategies\n",
    "- **Results**: Performance and interpretability improvements\n",
    "- **Analysis**: Ablation studies and insight validation\n",
    "- **Discussion**: Trade-offs between performance and interpretability\n",
    "\n",
    "**üéâ Ready to run your benchmarking script and measure the improvements!**\n",
    "\n",
    "The fine-tuned model will be available alongside your existing models for comparative analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
