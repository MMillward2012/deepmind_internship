{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Financial Sentiment Model Explainability Dashboard - Generalized\n",
    "\n",
    "## Overview\n",
    "This notebook provides comprehensive explainability analysis for **any** trained financial sentiment model in your collection. It includes four complementary explanation methods accessible through an interactive dashboard.\n",
    "\n",
    "### Explanation Methods\n",
    "- **üéØ SHAP**: Game-theory based feature importance\n",
    "- **üîç LIME**: Local interpretable model-agnostic explanations \n",
    "- **üëÅÔ∏è Attention**: Model attention head visualization\n",
    "- **üå°Ô∏è GradCAM**: Gradient-based visual attribution\n",
    "\n",
    "### Dashboard Features\n",
    "- **Model Selection**: Choose any model from your trained collection\n",
    "- **Mistake Analysis**: Examine specific model errors\n",
    "- **Custom Text Analysis**: Test any financial text\n",
    "- **Interactive Interface**: Tabbed layout for easy comparison\n",
    "- **On-demand Computation**: Optimized performance\n",
    "- **Configuration-Driven**: All settings loaded from pipeline config\n",
    "\n",
    "**Configuration-driven approach:** All settings loaded from `../config/pipeline_config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 00:13:08,346 - pipeline.explainability - INFO - üìä Starting Model Explainability Analysis - Generalized Pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import configuration system and explainability utilities\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.pipeline_utils import ConfigManager, StateManager, LoggingManager\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model and tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# ONNX support for explainability\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "    onnx_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ONNX Runtime not available. Only PyTorch models will be supported.\")\n",
    "    onnx_available = False\n",
    "\n",
    "# Explainability libraries\n",
    "try:\n",
    "    import shap\n",
    "    shap_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è SHAP not available. Install with: pip install shap\")\n",
    "    shap_available = False\n",
    "\n",
    "try:\n",
    "    from lime.lime_text import LimeTextExplainer\n",
    "    lime_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LIME not available. Install with: pip install lime\")\n",
    "    lime_available = False\n",
    "\n",
    "try:\n",
    "    from bertviz import head_view\n",
    "    bertviz_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è BertViz not available. Install with: pip install bertviz\")\n",
    "    bertviz_available = False\n",
    "\n",
    "try:\n",
    "    from captum.attr import LayerGradCam\n",
    "    captum_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Captum not available. Install with: pip install captum\")\n",
    "    captum_available = False\n",
    "\n",
    "# Dashboard components\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Initialize managers\n",
    "config = ConfigManager(\"../config/pipeline_config.json\")\n",
    "state = StateManager(\"../config/pipeline_state.json\")\n",
    "logger_manager = LoggingManager(config, 'explainability')\n",
    "logger = logger_manager.get_logger()\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "logger.info(\"üìä Starting Model Explainability Analysis - Generalized Pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üîç Model Discovery & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 00:13:08,377 - pipeline.explainability - INFO - üîç Discovering available models...\n",
      "2025-08-13 00:13:08,383 - pipeline.explainability - INFO - Model discovery completed: 7 models available\n",
      "2025-08-13 00:13:08,383 - pipeline.explainability - INFO - Model discovery completed: 7 models available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Models directory: ../models\n",
      "   ‚úÖ Found: tinybert-financial-classifier-fine-tuned (PyTorch, ONNX)\n",
      "   ‚úÖ Found: all-MiniLM-L6-v2-financial-sentiment (PyTorch, ONNX)\n",
      "   ‚úÖ Found: distilbert-financial-sentiment (PyTorch, ONNX)\n",
      "   ‚úÖ Found: finbert-tone-financial-sentiment (PyTorch, ONNX)\n",
      "   ‚ö†Ô∏è Invalid model directory: tinybert-financial-classifier_explainability_fine_tuned (missing required files)\n",
      "   ‚ö†Ô∏è Invalid model directory: SmolLM2-360M-Instruct-financial-sentiment (missing required files)\n",
      "   ‚úÖ Found: tinybert-financial-classifier (PyTorch, ONNX)\n",
      "   ‚úÖ Found: tinybert-financial-classifier-pruned (PyTorch, ONNX)\n",
      "   ‚úÖ Found: mobilebert-uncased-financial-sentiment (PyTorch, ONNX)\n",
      "\n",
      "üìä Discovery Summary:\n",
      "   ü§ñ Total models found: 7\n",
      "   üî• PyTorch models: 7\n",
      "   ‚ö° ONNX models: 7\n"
     ]
    }
   ],
   "source": [
    "# Discover available models for explainability analysis\n",
    "logger.info(\"üîç Discovering available models...\")\n",
    "\n",
    "# Load configuration\n",
    "explainability_config = config.get('explainability', {})\n",
    "models_config = config.get('models', {})\n",
    "data_config = config.get('data', {})\n",
    "\n",
    "# Model discovery\n",
    "models_dir = Path(f\"../{models_config.get('output_dir', 'models')}\")\n",
    "print(f\"üìÇ Models directory: {models_dir}\")\n",
    "\n",
    "# Discover available models\n",
    "available_models = {}\n",
    "\n",
    "if models_dir.exists():\n",
    "    for model_path in models_dir.iterdir():\n",
    "        if not model_path.is_dir() or model_path.name.startswith('.'):\n",
    "            continue\n",
    "            \n",
    "        model_name = model_path.name\n",
    "        \n",
    "        # Check for required files\n",
    "        config_file = model_path / \"config.json\"\n",
    "        label_encoder_file = model_path / \"label_encoder.pkl\"\n",
    "        pytorch_files = list(model_path.glob(\"*.safetensors\")) + list(model_path.glob(\"pytorch_model.bin\"))\n",
    "        \n",
    "        # Check for ONNX model files\n",
    "        onnx_dir = model_path / \"onnx\"\n",
    "        onnx_files = list(onnx_dir.glob(\"*.onnx\")) if onnx_dir.exists() else []\n",
    "        \n",
    "        if config_file.exists() and label_encoder_file.exists() and (pytorch_files or onnx_files):\n",
    "            model_info = {\n",
    "                'name': model_name,\n",
    "                'path': model_path,\n",
    "                'config_file': config_file,\n",
    "                'label_encoder_file': label_encoder_file,\n",
    "                'has_pytorch': len(pytorch_files) > 0,\n",
    "                'has_onnx': len(onnx_files) > 0,\n",
    "                'pytorch_files': pytorch_files,\n",
    "                'onnx_files': onnx_files\n",
    "            }\n",
    "            available_models[model_name] = model_info\n",
    "            \n",
    "            status = []\n",
    "            if model_info['has_pytorch']:\n",
    "                status.append(\"PyTorch\")\n",
    "            if model_info['has_onnx']:\n",
    "                status.append(\"ONNX\")\n",
    "            \n",
    "            print(f\"   ‚úÖ Found: {model_name} ({', '.join(status)})\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Invalid model directory: {model_name} (missing required files)\")\n",
    "\n",
    "print(f\"\\nüìä Discovery Summary:\")\n",
    "print(f\"   ü§ñ Total models found: {len(available_models)}\")\n",
    "pytorch_count = sum(1 for m in available_models.values() if m['has_pytorch'])\n",
    "onnx_count = sum(1 for m in available_models.values() if m['has_onnx'])\n",
    "print(f\"   üî• PyTorch models: {pytorch_count}\")\n",
    "print(f\"   ‚ö° ONNX models: {onnx_count}\")\n",
    "\n",
    "if len(available_models) == 0:\n",
    "    logger.error(\"No valid models found for explainability analysis\")\n",
    "    raise RuntimeError(\"No models found. Please ensure models have been trained and have label encoders.\")\n",
    "\n",
    "logger.info(f\"Model discovery completed: {len(available_models)} models available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üìä Data Loading & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 00:13:08,411 - pipeline.explainability - INFO - Data loading completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading validation data from: ../data/processed/validation.csv\n",
      "üìÅ Absolute path: /Users/matthew/Documents/deepmind_internship/data/processed/validation.csv\n",
      "‚úÖ Loaded 485 validation samples\n",
      "üìä Label distribution:\n",
      "  negative: 61 samples (12.6%)\n",
      "  neutral: 288 samples (59.4%)\n",
      "  positive: 136 samples (28.0%)\n",
      "üéØ Available labels: ['negative', 'neutral', 'positive']\n",
      "üìà Sample texts preview:\n",
      "  [neutral]: The solution will be installed in the USA to support the North American operatio...\n",
      "  [negative]: Scanfil , a systems supplier and contract manufacturer to the communications sec...\n",
      "  [neutral]: `` The sale of the oxygen measurement business strengthens our goal to focus on ...\n",
      "‚úÖ Data loading complete\n",
      "üìã Final dataset: 485 samples, 3 classes\n",
      "üîß Explainability Configuration:\n",
      "   üìã max_sequence_length: 512\n",
      "   üìã batch_size: 8\n",
      "   üìã sample_size: 100\n",
      "   üìã random_seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Load validation data from the config-specified path\n",
    "data_config = config.get('data', {})\n",
    "processed_data_dir = data_config.get('processed_data_dir', 'data/processed')\n",
    "validation_path = os.path.join('..', processed_data_dir, 'validation.csv')\n",
    "\n",
    "print(f\"üìÇ Loading validation data from: {validation_path}\")\n",
    "print(f\"üìÅ Absolute path: {os.path.abspath(validation_path)}\")\n",
    "\n",
    "try:\n",
    "    validation_df = pd.read_csv(validation_path)\n",
    "    print(f\"‚úÖ Loaded {len(validation_df)} validation samples\")\n",
    "    \n",
    "    # Extract texts and labels\n",
    "    validation_texts = validation_df['text'].tolist()\n",
    "    validation_labels_text = validation_df['label'].tolist()\n",
    "    \n",
    "    # Get unique labels for encoding\n",
    "    unique_labels = sorted(validation_df['label'].unique())\n",
    "    label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    validation_labels = [label_to_id[label] for label in validation_labels_text]\n",
    "    \n",
    "    print(f\"üìä Label distribution:\")\n",
    "    for label in unique_labels:\n",
    "        count = validation_labels_text.count(label)\n",
    "        print(f\"  {label}: {count} samples ({count/len(validation_df)*100:.1f}%)\")\n",
    "        \n",
    "    print(f\"üéØ Available labels: {unique_labels}\")\n",
    "    print(f\"üìà Sample texts preview:\")\n",
    "    for i, (text, label) in enumerate(zip(validation_texts[:3], validation_labels_text[:3])):\n",
    "        print(f\"  [{label}]: {text[:80]}...\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Validation file not found at: {validation_path}\")\n",
    "    print(\"üìù Trying alternative validation data sources...\")\n",
    "    \n",
    "    # Try to load from test data as fallback\n",
    "    alternative_paths = [\n",
    "        '../data/processed/full_processed.csv',\n",
    "        '../data/FinancialClassification/test.csv',\n",
    "        '../data/FinancialAuditor/test.csv',\n",
    "        '../data/FinancialPhraseBank/all-data.csv'\n",
    "    ]\n",
    "    \n",
    "    validation_df = None\n",
    "    for alt_path in alternative_paths:\n",
    "        try:\n",
    "            print(f\"üîç Checking: {alt_path}\")\n",
    "            if os.path.exists(alt_path):\n",
    "                validation_df = pd.read_csv(alt_path)\n",
    "                print(f\"‚úÖ Using fallback data from: {alt_path}\")\n",
    "                \n",
    "                # Handle different column naming conventions\n",
    "                if 'text' in validation_df.columns:\n",
    "                    text_col = 'text'\n",
    "                elif 'sentence' in validation_df.columns:\n",
    "                    text_col = 'sentence'\n",
    "                elif len(validation_df.columns) >= 2:  # Handle FinancialPhraseBank format\n",
    "                    # FinancialPhraseBank has no headers, assume first col is label, second is text\n",
    "                    validation_df.columns = ['label', 'text']\n",
    "                    text_col = 'text'\n",
    "                else:\n",
    "                    print(f\"‚ùå Unknown data format in {alt_path}\")\n",
    "                    continue\n",
    "                \n",
    "                if 'label' in validation_df.columns:\n",
    "                    label_col = 'label'\n",
    "                elif 'sentiment' in validation_df.columns:\n",
    "                    label_col = 'sentiment'\n",
    "                else:\n",
    "                    print(f\"‚ùå No label column found in {alt_path}\")\n",
    "                    continue\n",
    "                \n",
    "                validation_texts = validation_df[text_col].tolist()\n",
    "                validation_labels_text = validation_df[label_col].tolist()\n",
    "                \n",
    "                # Clean up any malformed data\n",
    "                clean_data = []\n",
    "                for text, label in zip(validation_texts, validation_labels_text):\n",
    "                    if pd.notna(text) and pd.notna(label) and str(text).strip() and str(label).strip():\n",
    "                        clean_data.append((str(text).strip(), str(label).strip()))\n",
    "                \n",
    "                if not clean_data:\n",
    "                    print(f\"‚ùå No valid data found in {alt_path}\")\n",
    "                    continue\n",
    "                \n",
    "                validation_texts = [item[0] for item in clean_data]\n",
    "                validation_labels_text = [item[1] for item in clean_data]\n",
    "                \n",
    "                # Sample for performance if too large\n",
    "                if len(validation_df) > 500:\n",
    "                    sample_indices = np.random.choice(len(validation_df), 500, replace=False)\n",
    "                    validation_texts = [validation_texts[i] for i in sample_indices]\n",
    "                    validation_labels_text = [validation_labels_text[i] for i in sample_indices]\n",
    "                    print(f\"üìã Sampled {len(validation_texts)} examples for performance\")\n",
    "                \n",
    "                unique_labels = sorted(set(validation_labels_text))\n",
    "                label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "                validation_labels = [label_to_id[label] for label in validation_labels_text]\n",
    "                \n",
    "                print(f\"üìä Label distribution:\")\n",
    "                for label in unique_labels:\n",
    "                    count = validation_labels_text.count(label)\n",
    "                    print(f\"  {label}: {count} samples ({count/len(validation_labels_text)*100:.1f}%)\")\n",
    "                \n",
    "                break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if validation_df is None:\n",
    "        print(\"‚ùå No validation data found. Creating sample data...\")\n",
    "        # Create sample validation data\n",
    "        sample_texts = [\n",
    "            \"The company reported strong quarterly earnings with revenue growth exceeding expectations.\",\n",
    "            \"Market volatility continues to pose challenges for the financial sector.\",\n",
    "            \"The business maintained steady performance despite economic headwinds.\",\n",
    "            \"Declining sales figures indicate potential market challenges ahead.\",\n",
    "            \"The merger announcement boosted investor confidence significantly.\"\n",
    "        ]\n",
    "        sample_labels = [\"positive\", \"negative\", \"neutral\", \"negative\", \"positive\"]\n",
    "        \n",
    "        validation_texts = sample_texts\n",
    "        validation_labels_text = sample_labels  \n",
    "        unique_labels = [\"positive\", \"negative\", \"neutral\"]\n",
    "        label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "        validation_labels = [label_to_id[label] for label in validation_labels_text]\n",
    "        \n",
    "        print(f\"‚úÖ Using sample data with {len(validation_texts)} samples\")\n",
    "\n",
    "print(\"‚úÖ Data loading complete\")\n",
    "print(f\"üìã Final dataset: {len(validation_texts)} samples, {len(unique_labels)} classes\")\n",
    "\n",
    "# Configuration for explainability\n",
    "EXPLAINABILITY_CONFIG = {\n",
    "    'max_sequence_length': explainability_config.get('max_sequence_length', 512),\n",
    "    'batch_size': explainability_config.get('batch_size', 8),\n",
    "    'sample_size': explainability_config.get('sample_size', min(100, len(validation_texts))),\n",
    "    'random_seed': explainability_config.get('random_seed', 42)\n",
    "}\n",
    "\n",
    "print(f\"üîß Explainability Configuration:\")\n",
    "for key, value in EXPLAINABILITY_CONFIG.items():\n",
    "    print(f\"   üìã {key}: {value}\")\n",
    "\n",
    "logger.info(\"Data loading completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üîß Universal Model Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 00:13:08,433 - pipeline.explainability - INFO - Universal model wrapper class ready\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Universal Model Wrapper defined\n"
     ]
    }
   ],
   "source": [
    "class UniversalModelWrapper:\n",
    "    \"\"\"Universal wrapper that works with both PyTorch and ONNX models for explainability\"\"\"\n",
    "    \n",
    "    def __init__(self, model_info: Dict, model_type: str = 'auto'):\n",
    "        \"\"\"\n",
    "        Initialize model wrapper\n",
    "        \n",
    "        Args:\n",
    "            model_info: Model information dictionary from discovery\n",
    "            model_type: 'pytorch', 'onnx', or 'auto' (auto-detect)\n",
    "        \"\"\"\n",
    "        self.model_info = model_info\n",
    "        self.model_path = model_info['path']\n",
    "        self.model_name = model_info['name']\n",
    "        \n",
    "        # Auto-detect model type if not specified\n",
    "        if model_type == 'auto':\n",
    "            if model_info['has_pytorch']:\n",
    "                self.model_type = 'pytorch'\n",
    "            elif model_info['has_onnx']:\n",
    "                self.model_type = 'onnx'\n",
    "            else:\n",
    "                raise ValueError(f\"No supported model format found for {self.model_name}\")\n",
    "        else:\n",
    "            self.model_type = model_type\n",
    "        \n",
    "        # Load tokenizer (same for both types)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(str(self.model_path))\n",
    "        \n",
    "        # Load label encoder\n",
    "        with open(self.model_info['label_encoder_file'], 'rb') as f:\n",
    "            self.label_encoder = pickle.load(f)\n",
    "        \n",
    "        # Load appropriate model\n",
    "        if self.model_type == 'onnx':\n",
    "            if not model_info['has_onnx']:\n",
    "                raise ValueError(f\"ONNX model not available for {self.model_name}\")\n",
    "            onnx_path = self.model_path / 'onnx' / 'model.onnx'\n",
    "            self.session = ort.InferenceSession(str(onnx_path))\n",
    "            self.model = None  # No PyTorch model for ONNX\n",
    "            print(f\"‚úÖ Loaded ONNX model: {self.model_name}\")\n",
    "        else:\n",
    "            if not model_info['has_pytorch']:\n",
    "                raise ValueError(f\"PyTorch model not available for {self.model_name}\")\n",
    "            \n",
    "            # Load PyTorch model with eager attention implementation to support explainability\n",
    "            try:\n",
    "                # First attempt: Load with explicit eager attention implementation\n",
    "                self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    str(self.model_path),\n",
    "                    attn_implementation=\"eager\"  # Force eager attention for explainability compatibility\n",
    "                )\n",
    "                print(f\"‚úÖ Loaded PyTorch model with eager attention: {self.model_name}\")\n",
    "            except Exception as eager_error:\n",
    "                print(f\"‚ö†Ô∏è Could not load with eager attention: {eager_error}\")\n",
    "                print(\"üîÑ Loading with default implementation...\")\n",
    "                \n",
    "                # Fallback: Load normally and set implementation afterward\n",
    "                self.model = AutoModelForSequenceClassification.from_pretrained(str(self.model_path))\n",
    "                \n",
    "                # Try to set eager attention implementation for explainability\n",
    "                if hasattr(self.model.config, 'attn_implementation'):\n",
    "                    self.model.config.attn_implementation = 'eager'\n",
    "                if hasattr(self.model.config, '_attn_implementation'):\n",
    "                    self.model.config._attn_implementation = 'eager'\n",
    "                \n",
    "                print(f\"‚úÖ Loaded PyTorch model (attention implementation set post-load): {self.model_name}\")\n",
    "            \n",
    "            self.model.eval()\n",
    "            self.session = None  # No ONNX session for PyTorch\n",
    "        \n",
    "        logger.info(f\"Model wrapper initialized: {self.model_name} ({self.model_type})\")\n",
    "    \n",
    "    def predict_class(self, texts):\n",
    "        \"\"\"Predict sentiment class for text(s)\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        if self.model_type == 'onnx':\n",
    "            predictions = self._predict_onnx_class(texts)\n",
    "        else:\n",
    "            predictions = self._predict_pytorch_class(texts)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_probs(self, texts):\n",
    "        \"\"\"Get prediction probabilities for text(s)\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        if self.model_type == 'onnx':\n",
    "            return self._predict_onnx_probs(texts)\n",
    "        else:\n",
    "            return self._predict_pytorch_probs(texts)\n",
    "    \n",
    "    def _predict_pytorch_class(self, texts):\n",
    "        \"\"\"PyTorch class prediction\"\"\"\n",
    "        predictions = []\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                encoding = self.tokenizer(text, return_tensors='pt', \n",
    "                                        max_length=EXPLAINABILITY_CONFIG['max_sequence_length'], \n",
    "                                        truncation=True, padding=True)\n",
    "                outputs = self.model(**encoding)\n",
    "                predicted_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "                predictions.append(predicted_class)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def _predict_pytorch_probs(self, texts):\n",
    "        \"\"\"PyTorch probability prediction\"\"\"\n",
    "        all_probs = []\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                encoding = self.tokenizer(text, return_tensors='pt', \n",
    "                                        max_length=EXPLAINABILITY_CONFIG['max_sequence_length'], \n",
    "                                        truncation=True, padding=True)\n",
    "                outputs = self.model(**encoding)\n",
    "                probs = torch.softmax(outputs.logits, dim=-1).squeeze().cpu().numpy()\n",
    "                all_probs.append(probs)\n",
    "        \n",
    "        return np.array(all_probs)\n",
    "    \n",
    "    def _predict_onnx_class(self, texts):\n",
    "        \"\"\"ONNX class prediction\"\"\"\n",
    "        probabilities = self._predict_onnx_probs(texts)\n",
    "        return np.argmax(probabilities, axis=-1).tolist()\n",
    "    \n",
    "    def _predict_onnx_probs(self, texts):\n",
    "        \"\"\"ONNX probability prediction\"\"\"\n",
    "        all_probs = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # Tokenize\n",
    "            encoding = self.tokenizer(text, return_tensors='np', \n",
    "                                    max_length=EXPLAINABILITY_CONFIG['max_sequence_length'], \n",
    "                                    truncation=True, padding=True)\n",
    "            \n",
    "            # Prepare inputs\n",
    "            inputs = {\n",
    "                'input_ids': encoding['input_ids'].astype(np.int64),\n",
    "                'attention_mask': encoding['attention_mask'].astype(np.int64)\n",
    "            }\n",
    "            \n",
    "            # Run inference\n",
    "            outputs = self.session.run(None, inputs)\n",
    "            logits = outputs[0]\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probs = self._softmax(logits[0])  # Take first (and only) sample\n",
    "            all_probs.append(probs)\n",
    "        \n",
    "        return np.array(all_probs)\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        \"\"\"Numpy softmax implementation for ONNX\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / np.sum(exp_x)\n",
    "    \n",
    "    def get_model_for_attention(self):\n",
    "        \"\"\"Get the PyTorch model for attention visualization (ONNX not supported)\"\"\"\n",
    "        if self.model_type != 'pytorch':\n",
    "            raise ValueError(\"Attention visualization only supported for PyTorch models\")\n",
    "        return self.model\n",
    "    \n",
    "    def get_tokenizer(self):\n",
    "        \"\"\"Get the tokenizer\"\"\"\n",
    "        return self.tokenizer\n",
    "    \n",
    "    def get_label_encoder(self):\n",
    "        \"\"\"Get the label encoder\"\"\"\n",
    "        return self.label_encoder\n",
    "    \n",
    "    def get_class_names(self):\n",
    "        \"\"\"Get class names\"\"\"\n",
    "        return list(self.label_encoder.classes_)\n",
    "\n",
    "print(\"‚úÖ Universal Model Wrapper defined\")\n",
    "logger.info(\"Universal model wrapper class ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üß© Explainability Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 SHAP Implementation (Universal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SHAP implementation ready\n"
     ]
    }
   ],
   "source": [
    "# SHAP explainer cache (per model)\n",
    "_shap_explainers = {}\n",
    "\n",
    "def get_shap_explainer(model_wrapper):\n",
    "    \"\"\"Get SHAP explainer for model (lazy initialization)\"\"\"\n",
    "    if not shap_available:\n",
    "        raise ImportError(\"SHAP not available. Install with: pip install shap\")\n",
    "    \n",
    "    model_key = f\"{model_wrapper.model_name}_{model_wrapper.model_type}\"\n",
    "    \n",
    "    if model_key not in _shap_explainers:\n",
    "        print(f\"üßÆ Initializing SHAP explainer for {model_wrapper.model_name}...\")\n",
    "        \n",
    "        # Create prediction function for SHAP\n",
    "        def predict_for_shap(texts):\n",
    "            return model_wrapper.predict_probs(texts)\n",
    "        \n",
    "        _shap_explainers[model_key] = shap.Explainer(predict_for_shap, model_wrapper.tokenizer)\n",
    "    \n",
    "    return _shap_explainers[model_key]\n",
    "\n",
    "def explain_with_shap(model_wrapper, text, target_class=None):\n",
    "    \"\"\"Generate SHAP explanation for text\"\"\"\n",
    "    if not shap_available:\n",
    "        print(\"‚ùå SHAP not available. Install with: pip install shap\")\n",
    "        return\n",
    "    \n",
    "    # print(\"‚è≥ Computing SHAP values...\")\n",
    "    \n",
    "    try:\n",
    "        explainer = get_shap_explainer(model_wrapper)\n",
    "        shap_values = explainer([text])\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = model_wrapper.predict_class(text)[0]\n",
    "        \n",
    "        # Display SHAP plot\n",
    "        shap.plots.text(shap_values[0, :, target_class])\n",
    "        \n",
    "        pred_label = model_wrapper.label_encoder.inverse_transform([target_class])[0]\n",
    "        print(f\"üìä SHAP explanation for class: {pred_label}\")\n",
    "        print(f\"üîß Model: {model_wrapper.model_name} ({model_wrapper.model_type})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå SHAP explanation failed: {str(e)}\")\n",
    "        print(\"üí° Common issues:\")\n",
    "        print(\"   - Model compatibility with SHAP\")\n",
    "        print(\"   - Text preprocessing differences\")\n",
    "        print(\"   - Try using LIME as alternative\")\n",
    "\n",
    "print(\"‚úÖ SHAP implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 LIME Implementation (Universal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LIME implementation ready\n"
     ]
    }
   ],
   "source": [
    "# LIME explainer cache (per model)\n",
    "_lime_explainers = {}\n",
    "\n",
    "def get_lime_explainer(model_wrapper):\n",
    "    \"\"\"Get LIME explainer for model (lazy initialization)\"\"\"\n",
    "    if not lime_available:\n",
    "        raise ImportError(\"LIME not available. Install with: pip install lime\")\n",
    "    \n",
    "    model_key = f\"{model_wrapper.model_name}_{model_wrapper.model_type}\"\n",
    "    \n",
    "    if model_key not in _lime_explainers:\n",
    "        _lime_explainers[model_key] = LimeTextExplainer(\n",
    "            class_names=model_wrapper.get_class_names()\n",
    "        )\n",
    "    \n",
    "    return _lime_explainers[model_key]\n",
    "\n",
    "def explain_with_lime(model_wrapper, text):\n",
    "    \"\"\"Generate LIME explanation for text\"\"\"\n",
    "    if not lime_available:\n",
    "        print(\"‚ùå LIME not available. Install with: pip install lime\")\n",
    "        return\n",
    "    \n",
    "    print(\"‚è≥ Computing LIME explanation...\")\n",
    "    \n",
    "    try:\n",
    "        explainer = get_lime_explainer(model_wrapper)\n",
    "        \n",
    "        # Create prediction function for LIME\n",
    "        def predict_for_lime(texts):\n",
    "            \"\"\"Prediction function for LIME (expects different format)\"\"\"\n",
    "            if isinstance(texts, str):\n",
    "                texts = [texts]\n",
    "            elif isinstance(texts, list) and len(texts) == 1 and isinstance(texts[0], str):\n",
    "                texts = texts\n",
    "            elif isinstance(texts, (list, tuple)) and all(isinstance(t, str) for t in texts):\n",
    "                texts = list(texts)\n",
    "            else:\n",
    "                texts = [str(t) for t in texts]\n",
    "            \n",
    "            try:\n",
    "                return model_wrapper.predict_probs(texts)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in LIME prediction: {e}\")\n",
    "                # Return default probabilities if processing fails\n",
    "                return np.array([[0.33, 0.33, 0.34]] * len(texts))\n",
    "        \n",
    "        explanation = explainer.explain_instance(\n",
    "            text,\n",
    "            predict_for_lime,\n",
    "            num_features=20,\n",
    "            labels=tuple(range(len(model_wrapper.get_class_names())))\n",
    "        )\n",
    "        \n",
    "        display(HTML(explanation.as_html()))\n",
    "        print(f\"üìä LIME explanation generated\")\n",
    "        print(f\"üîß Model: {model_wrapper.model_name} ({model_wrapper.model_type})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LIME explanation failed: {str(e)}\")\n",
    "        print(\"üí° Common LIME issues:\")\n",
    "        print(\"   - Text preprocessing differences\")\n",
    "        print(\"   - Prediction function format mismatch\")\n",
    "        print(\"   - Try using SHAP instead\")\n",
    "\n",
    "print(\"‚úÖ LIME implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Attention Visualization (PyTorch Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Attention visualization ready\n"
     ]
    }
   ],
   "source": [
    "def explain_with_attention(model_wrapper, text):\n",
    "    \"\"\"Generate attention visualization for text (PyTorch only)\"\"\"\n",
    "    if model_wrapper.model_type != 'pytorch':\n",
    "        print(\"‚ùå Attention visualization only supported for PyTorch models\")\n",
    "        print(\"üí° Switch to a PyTorch model to use this feature\")\n",
    "        return\n",
    "    \n",
    "    if not bertviz_available:\n",
    "        print(\"‚ö†Ô∏è BertViz not available. Using custom attention visualization...\")\n",
    "        \n",
    "    print(\"‚è≥ Generating attention visualization...\")\n",
    "    \n",
    "    try:\n",
    "        model = model_wrapper.get_model_for_attention()\n",
    "        tokenizer = model_wrapper.get_tokenizer()\n",
    "        \n",
    "        # Tokenize with attention output\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, \n",
    "                          max_length=EXPLAINABILITY_CONFIG['max_sequence_length'])\n",
    "        \n",
    "        # Get model outputs with attention\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_attentions=True)\n",
    "            attentions = outputs.attentions\n",
    "            tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        \n",
    "        # Check if we have valid attention and tokens\n",
    "        if attentions is None or len(attentions) == 0:\n",
    "            print(\"‚ùå No attention weights available\")\n",
    "            return\n",
    "            \n",
    "        if len(tokens) == 0:\n",
    "            print(\"‚ùå No tokens available\")\n",
    "            return\n",
    "        \n",
    "        pred_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "        pred_label = model_wrapper.label_encoder.inverse_transform([pred_class])[0]\n",
    "        \n",
    "        # Try BertViz first if available\n",
    "        if bertviz_available:\n",
    "            try:\n",
    "                print(\"üéØ Attempting interactive attention visualization...\")\n",
    "                head_view(attentions, tokens)\n",
    "                print(f\"üëÅÔ∏è Interactive attention visualization for prediction: {pred_label}\")\n",
    "                print(f\"üîß Model: {model_wrapper.model_name} ({model_wrapper.model_type})\")\n",
    "                return\n",
    "                \n",
    "            except Exception as viz_error:\n",
    "                print(f\"‚ùå BertViz interactive view failed: {viz_error}\")\n",
    "                print(\"üí° Using custom attention heatmap...\")\n",
    "        \n",
    "        # Custom attention visualization fallback\n",
    "        _visualize_attention_heatmap(attentions, tokens, pred_label, model_wrapper.model_name)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Attention analysis failed: {str(e)}\")\n",
    "        print(\"üí° This might be due to model architecture compatibility issues\")\n",
    "        print(\"üîß Try reloading the model or use other explainability methods\")\n",
    "\n",
    "def _visualize_attention_heatmap(attentions, tokens, pred_label, model_name):\n",
    "    \"\"\"Create custom attention heatmap visualization\"\"\"\n",
    "    # Get average attention across all layers and heads\n",
    "    avg_attention = torch.stack(attentions).mean(dim=0)  # Average across layers\n",
    "    avg_attention = avg_attention.mean(dim=1)  # Average across heads\n",
    "    attention_matrix = avg_attention[0].detach().cpu().numpy()  # Get first batch\n",
    "    \n",
    "    # Clean tokens for display\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.startswith('##'):\n",
    "            clean_tokens.append(token[2:])\n",
    "        elif token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            clean_tokens.append(token)\n",
    "        else:\n",
    "            clean_tokens.append(token)\n",
    "    \n",
    "    # Limit to reasonable size for visualization\n",
    "    max_len = min(len(clean_tokens), 50)\n",
    "    attention_matrix = attention_matrix[:max_len, :max_len]\n",
    "    display_tokens = clean_tokens[:max_len]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # 1. Full attention heatmap\n",
    "    sns.heatmap(attention_matrix, \n",
    "                xticklabels=display_tokens,\n",
    "                yticklabels=display_tokens,\n",
    "                cmap='Blues',\n",
    "                ax=ax1,\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    ax1.set_title(f'Attention Heatmap\\nModel: {model_name}\\nPrediction: {pred_label}', \n",
    "                 fontsize=14, weight='bold')\n",
    "    ax1.set_xlabel('Attended Tokens')\n",
    "    ax1.set_ylabel('Query Tokens')\n",
    "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.setp(ax1.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    # 2. CLS token attention (what the model focuses on for classification)\n",
    "    cls_attention = attention_matrix[0, 1:]  # CLS token attention to other tokens\n",
    "    tokens_for_cls = display_tokens[1:]  # Skip CLS token\n",
    "    \n",
    "    # Sort by attention weight\n",
    "    token_attention_pairs = list(zip(tokens_for_cls, cls_attention))\n",
    "    token_attention_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Take top 15 for readability\n",
    "    top_tokens, top_weights = zip(*token_attention_pairs[:15])\n",
    "    \n",
    "    bars = ax2.barh(range(len(top_tokens)), top_weights, color='skyblue')\n",
    "    ax2.set_yticks(range(len(top_tokens)))\n",
    "    ax2.set_yticklabels(top_tokens)\n",
    "    ax2.set_xlabel('Attention Weight')\n",
    "    ax2.set_title(f'Top Attended Tokens\\n(CLS token attention)', fontsize=14, weight='bold')\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, weight) in enumerate(zip(bars, top_weights)):\n",
    "        ax2.text(weight + 0.001, i, f'{weight:.3f}', \n",
    "                va='center', ha='left', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"üìä Attention Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Model: {model_name}\")\n",
    "    print(f\"   ‚Ä¢ Number of layers: {len(attentions)}\")\n",
    "    print(f\"   ‚Ä¢ Number of heads per layer: {attentions[0].shape[2]}\")\n",
    "    print(f\"   ‚Ä¢ Sequence length: {len(tokens)}\")\n",
    "    print(f\"   ‚Ä¢ Max attention weight: {attention_matrix.max():.4f}\")\n",
    "    print(f\"   ‚Ä¢ Average attention weight: {attention_matrix.mean():.4f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ Top 5 tokens by CLS attention:\")\n",
    "    for i, (token, weight) in enumerate(token_attention_pairs[:5]):\n",
    "        if token not in ['[SEP]', '[PAD]']:\n",
    "            print(f\"   {i+1}. '{token}': {weight:.4f}\")\n",
    "    \n",
    "    print(f\"üëÅÔ∏è Custom attention visualization complete for: {pred_label}\")\n",
    "\n",
    "print(\"‚úÖ Attention visualization ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 GradCAM Implementation (PyTorch Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GradCAM implementation ready\n"
     ]
    }
   ],
   "source": [
    "def explain_with_gradcam(model_wrapper, text, target_layer=None):\n",
    "    \"\"\"Generate GradCAM explanation for text (PyTorch only)\"\"\"\n",
    "    if model_wrapper.model_type != 'pytorch':\n",
    "        print(\"‚ùå GradCAM visualization only supported for PyTorch models\")\n",
    "        print(\"üí° Switch to a PyTorch model to use this feature\")\n",
    "        return\n",
    "    \n",
    "    if not captum_available:\n",
    "        print(\"‚ùå Captum not available. Install with: pip install captum\")\n",
    "        return\n",
    "        \n",
    "    print(\"‚è≥ Generating GradCAM visualization...\")\n",
    "    \n",
    "    try:\n",
    "        from captum.attr import LayerGradCam, TokenReferenceBase\n",
    "        from captum.attr import visualization as viz\n",
    "        \n",
    "        model = model_wrapper.get_model_for_attention()\n",
    "        tokenizer = model_wrapper.get_tokenizer()\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, \n",
    "                          max_length=EXPLAINABILITY_CONFIG['max_sequence_length'],\n",
    "                          padding=True)\n",
    "        \n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        # Get model prediction\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            pred_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "            pred_label = model_wrapper.label_encoder.inverse_transform([pred_class])[0]\n",
    "        \n",
    "        print(f\"üéØ Model prediction: {pred_label}\")\n",
    "        \n",
    "        # Determine target layer for GradCAM\n",
    "        if target_layer is None:\n",
    "            # Try to find the last transformer layer\n",
    "            if hasattr(model, 'bert'):  # BERT-based models\n",
    "                target_layer = model.bert.encoder.layer[-1]\n",
    "            elif hasattr(model, 'distilbert'):  # DistilBERT\n",
    "                target_layer = model.distilbert.transformer.layer[-1]\n",
    "            elif hasattr(model, 'roberta'):  # RoBERTa\n",
    "                target_layer = model.roberta.encoder.layer[-1]\n",
    "            elif hasattr(model, 'albert'):  # ALBERT\n",
    "                target_layer = model.albert.encoder.albert_layer_groups[-1].albert_layers[-1]\n",
    "            else:\n",
    "                # Generic fallback - try to find encoder layers\n",
    "                for name, module in model.named_modules():\n",
    "                    if 'encoder' in name.lower() and 'layer' in name.lower():\n",
    "                        target_layer = module\n",
    "                        break\n",
    "                \n",
    "                if target_layer is None:\n",
    "                    print(\"‚ùå Could not automatically detect target layer for GradCAM\")\n",
    "                    print(\"üí° Model architecture not supported for automatic layer detection\")\n",
    "                    return\n",
    "        \n",
    "        print(f\"üîç Using target layer: {target_layer.__class__.__name__}\")\n",
    "        \n",
    "        # Create wrapper function that returns only logits\n",
    "        def forward_func(input_ids, attention_mask=None):\n",
    "            \"\"\"Forward function that returns only logits tensor for Captum\"\"\"\n",
    "            if attention_mask is not None:\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            else:\n",
    "                outputs = model(input_ids=input_ids)\n",
    "            return outputs.logits\n",
    "        \n",
    "        # Create GradCAM with the forward function\n",
    "        grad_cam = LayerGradCam(forward_func, target_layer)\n",
    "        \n",
    "        # Generate attributions\n",
    "        try:\n",
    "            # Try with attention mask first\n",
    "            attributions = grad_cam.attribute(\n",
    "                inputs=input_ids,\n",
    "                target=pred_class,\n",
    "                additional_forward_args=(attention_mask,)\n",
    "            )\n",
    "        except Exception as attr_error:\n",
    "            print(f\"‚ùå Attribution failed with attention mask: {attr_error}\")\n",
    "            print(\"üîÑ Retrying without attention mask...\")\n",
    "            try:\n",
    "                # Create new GradCAM without attention mask support\n",
    "                def simple_forward_func(input_ids):\n",
    "                    return model(input_ids=input_ids).logits\n",
    "                \n",
    "                grad_cam_simple = LayerGradCam(simple_forward_func, target_layer)\n",
    "                attributions = grad_cam_simple.attribute(\n",
    "                    inputs=input_ids,\n",
    "                    target=pred_class\n",
    "                )\n",
    "            except Exception as attr_error2:\n",
    "                print(f\"‚ùå Attribution failed: {attr_error2}\")\n",
    "                print(\"üí° Trying alternative approach...\")\n",
    "                \n",
    "                # Final fallback - use a wrapper class\n",
    "                try:\n",
    "                    class ModelWrapper(torch.nn.Module):\n",
    "                        def __init__(self, model):\n",
    "                            super().__init__()\n",
    "                            self.model = model\n",
    "                        \n",
    "                        def forward(self, input_ids):\n",
    "                            return self.model(input_ids=input_ids).logits\n",
    "                    \n",
    "                    wrapped_model = ModelWrapper(model)\n",
    "                    grad_cam_wrapped = LayerGradCam(wrapped_model, target_layer)\n",
    "                    attributions = grad_cam_wrapped.attribute(\n",
    "                        inputs=input_ids,\n",
    "                        target=pred_class\n",
    "                    )\n",
    "                except Exception as attr_error3:\n",
    "                    print(f\"‚ùå All attribution methods failed: {attr_error3}\")\n",
    "                    print(\"üí° This model architecture might not be compatible with GradCAM\")\n",
    "                    return\n",
    "        \n",
    "        # Convert to numpy and process\n",
    "        attributions_np = attributions.squeeze().detach().cpu().numpy()\n",
    "        \n",
    "        # Get tokens for visualization\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "        \n",
    "        # Create visualization\n",
    "        _visualize_gradcam(attributions_np, tokens, text, pred_label, model_wrapper.model_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå GradCAM analysis failed: {str(e)}\")\n",
    "        print(\"üí° Common issues:\")\n",
    "        print(\"   - Model architecture not supported\")\n",
    "        print(\"   - Layer selection problems\")\n",
    "        print(\"   - Memory constraints with large models\")\n",
    "\n",
    "def _visualize_gradcam(attributions, tokens, original_text, pred_label, model_name):\n",
    "    \"\"\"Create GradCAM visualization with text highlighting\"\"\"\n",
    "    \n",
    "    # Process attributions\n",
    "    if len(attributions.shape) > 1:\n",
    "        # If multi-dimensional, take mean across dimensions\n",
    "        attr_scores = np.mean(attributions, axis=tuple(range(1, len(attributions.shape))))\n",
    "    else:\n",
    "        attr_scores = attributions\n",
    "    \n",
    "    # Ensure we have the right number of scores for tokens\n",
    "    min_len = min(len(attr_scores), len(tokens))\n",
    "    attr_scores = attr_scores[:min_len]\n",
    "    tokens = tokens[:min_len]\n",
    "    \n",
    "    # Normalize attributions to [0, 1]\n",
    "    attr_scores = attr_scores - attr_scores.min()\n",
    "    if attr_scores.max() > 0:\n",
    "        attr_scores = attr_scores / attr_scores.max()\n",
    "    \n",
    "    # Clean tokens and merge subwords for better text reconstruction\n",
    "    display_tokens = []\n",
    "    display_scores = []\n",
    "    reconstructed_words = []\n",
    "    word_scores = []\n",
    "    current_word = \"\"\n",
    "    current_score = 0\n",
    "    word_token_count = 0\n",
    "    \n",
    "    for token, score in zip(tokens, attr_scores):\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "            \n",
    "        if token.startswith('##'):\n",
    "            # Subword continuation\n",
    "            current_word += token[2:]\n",
    "            current_score += score\n",
    "            word_token_count += 1\n",
    "        else:\n",
    "            # New word - finalize previous word if exists\n",
    "            if current_word:\n",
    "                reconstructed_words.append(current_word)\n",
    "                word_scores.append(current_score / word_token_count)  # Average score for word\n",
    "            \n",
    "            # Start new word\n",
    "            current_word = token\n",
    "            current_score = score\n",
    "            word_token_count = 1\n",
    "            \n",
    "        # Keep individual tokens for detailed analysis\n",
    "        if token not in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            display_tokens.append(token[2:] if token.startswith('##') else token)\n",
    "            display_scores.append(score)\n",
    "    \n",
    "    # Don't forget the last word\n",
    "    if current_word:\n",
    "        reconstructed_words.append(current_word)\n",
    "        word_scores.append(current_score / word_token_count)\n",
    "    \n",
    "    # Create clean, focused visualization\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    # Simplified layout: Large text panel + compact side panels\n",
    "    gs = fig.add_gridspec(2, 2, height_ratios=[3, 1], width_ratios=[2.5, 1], \n",
    "                         hspace=0.2, wspace=0.3, \n",
    "                         left=0.05, right=0.95, top=0.92, bottom=0.08)\n",
    "    \n",
    "    # 1. Enhanced text highlighting (main focus - spans top row)\n",
    "    ax_text = fig.add_subplot(gs[0, :])\n",
    "    _create_enhanced_text_highlight(ax_text, reconstructed_words, word_scores, original_text, pred_label, model_name)\n",
    "    \n",
    "    # 2. Top words bar chart (bottom left)\n",
    "    ax_bars = fig.add_subplot(gs[1, 0])\n",
    "    _create_word_importance_chart(ax_bars, reconstructed_words, word_scores)\n",
    "    \n",
    "    # 3. Statistics summary (bottom right)\n",
    "    ax_stats = fig.add_subplot(gs[1, 1])\n",
    "    _create_stats_summary(ax_stats, word_scores, reconstructed_words, pred_label, model_name)\n",
    "    \n",
    "    plt.suptitle('GradCAM Text Explainability Analysis', fontsize=18, fontweight='bold', y=0.97)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print enhanced statistics without emojis\n",
    "    print(f\"GradCAM Analysis Summary:\")\n",
    "    print(f\"   Model: {model_name}\")\n",
    "    print(f\"   Prediction: {pred_label}\")\n",
    "    print(f\"   Words analyzed: {len(reconstructed_words)}\")\n",
    "    print(f\"   Tokens analyzed: {len(tokens)}\")\n",
    "    print(f\"   Max word importance: {max(word_scores):.4f}\")\n",
    "    print(f\"   Average word importance: {np.mean(word_scores):.4f}\")\n",
    "    \n",
    "    # Show top important words with better formatting\n",
    "    word_importance = list(zip(reconstructed_words, word_scores))\n",
    "    word_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nMost Important Words for '{pred_label}' prediction:\")\n",
    "    for i, (word, score) in enumerate(word_importance[:8]):\n",
    "        if score > 0.8:\n",
    "            intensity = \"[CRITICAL]\"\n",
    "        elif score > 0.65:\n",
    "            intensity = \"[HIGH]\"\n",
    "        elif score > 0.45:\n",
    "            intensity = \"[MEDIUM]\"\n",
    "        elif score > 0.25:\n",
    "            intensity = \"[LOW]\"\n",
    "        else:\n",
    "            intensity = \"[MINIMAL]\"\n",
    "        print(f\"   {i+1}. {intensity} '{word}': {score:.3f}\")\n",
    "    \n",
    "    print(f\"\\nGradCAM text analysis complete!\")\n",
    "\n",
    "def _create_enhanced_text_highlight(ax, words, scores, original_text, pred_label, model_name):\n",
    "    \"\"\"Create enhanced text highlighting with more focus and better information display\"\"\"\n",
    "    ax.set_xlim(0, 12)\n",
    "    ax.set_ylim(0, 7)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Clean title without emojis\n",
    "    title_box = dict(boxstyle=\"round,pad=0.6\", facecolor='#2C3E50', edgecolor='#1A252F', linewidth=2)\n",
    "    ax.text(6, 6.6, f\"GradCAM Analysis: {pred_label.upper()}\", \n",
    "            ha='center', va='center', fontsize=18, weight='bold', color='white',\n",
    "            bbox=title_box)\n",
    "    \n",
    "    # Model info\n",
    "    ax.text(6, 6.1, f\"Model: {model_name}\", ha='center', va='center', \n",
    "            fontsize=12, style='italic', color='#34495E')\n",
    "    \n",
    "    # Text highlighting with enhanced visual hierarchy\n",
    "    y_pos = 5.0\n",
    "    x_pos = 0.4\n",
    "    max_width = 11.2\n",
    "    line_height = 0.5\n",
    "    \n",
    "    # Better score normalization for clearer visual differences\n",
    "    if max(scores) > 0:\n",
    "        norm_scores = np.array(scores) / max(scores)\n",
    "        # Apply sigmoid-like transformation for better visual separation\n",
    "        norm_scores = 1 / (1 + np.exp(-5 * (norm_scores - 0.5)))\n",
    "    else:\n",
    "        norm_scores = np.array(scores)\n",
    "    \n",
    "    # Add importance scale indicator\n",
    "    ax.text(0.4, 5.4, \"Importance Scale:\", fontsize=12, weight='bold', color='#2C3E50')\n",
    "    \n",
    "    for i, (word, score) in enumerate(zip(words, norm_scores)):\n",
    "        # Refined color scheme for better contrast and readability\n",
    "        if score > 0.80:\n",
    "            bg_color = '#E74C3C'  # Strong red\n",
    "            text_color = 'white'\n",
    "            weight = 'bold'\n",
    "            border_color = '#C0392B'\n",
    "            importance = 'CRITICAL'\n",
    "        elif score > 0.65:\n",
    "            bg_color = '#E67E22'  # Orange\n",
    "            text_color = 'white'\n",
    "            weight = 'bold'\n",
    "            border_color = '#D35400'\n",
    "            importance = 'HIGH'\n",
    "        elif score > 0.45:\n",
    "            bg_color = '#F39C12'  # Yellow-orange\n",
    "            text_color = 'black'\n",
    "            weight = 'bold'\n",
    "            border_color = '#E67E22'\n",
    "            importance = 'MEDIUM'\n",
    "        elif score > 0.25:\n",
    "            bg_color = '#27AE60'  # Green\n",
    "            text_color = 'white'\n",
    "            weight = 'normal'\n",
    "            border_color = '#229954'\n",
    "            importance = 'LOW'\n",
    "        else:\n",
    "            bg_color = '#BDC3C7'  # Light gray\n",
    "            text_color = '#2C3E50'\n",
    "            weight = 'normal'\n",
    "            border_color = '#95A5A6'\n",
    "            importance = 'MINIMAL'\n",
    "        \n",
    "        # Calculate word width\n",
    "        word_width = len(word) * 0.09 + 0.5\n",
    "        \n",
    "        # Line wrapping\n",
    "        if x_pos + word_width > max_width:\n",
    "            y_pos -= line_height\n",
    "            x_pos = 0.4\n",
    "        \n",
    "        # Enhanced word styling with subtle shadow effect\n",
    "        word_box = dict(boxstyle=\"round,pad=0.3\", facecolor=bg_color, \n",
    "                       edgecolor=border_color, linewidth=1.5, alpha=0.95)\n",
    "        \n",
    "        word_text = ax.text(x_pos, y_pos, word, fontsize=14, weight=weight, color=text_color,\n",
    "                           bbox=word_box, ha='left', va='center')\n",
    "        \n",
    "        # Add importance score as small text above critical words\n",
    "        if score > 0.65:\n",
    "            ax.text(x_pos + word_width/2, y_pos + 0.2, f'{score:.2f}', \n",
    "                   ha='center', va='center', fontsize=9, weight='bold', \n",
    "                   color=border_color, alpha=0.8)\n",
    "        \n",
    "        x_pos += word_width + 0.15\n",
    "    \n",
    "    # Enhanced legend with cleaner design\n",
    "    legend_y = 1.8\n",
    "    legend_spacing = 2.2\n",
    "    \n",
    "    # Legend background\n",
    "    ax.add_patch(plt.Rectangle((0.2, 1.2), 11.6, 1.4, \n",
    "                              facecolor='#ECF0F1', edgecolor='#BDC3C7', \n",
    "                              linewidth=1.5, alpha=0.9))\n",
    "    \n",
    "    ax.text(6, 2.4, \"Word Importance Legend\", ha='center', va='center',\n",
    "            fontsize=14, weight='bold', color='#2C3E50')\n",
    "    \n",
    "    legend_items = [\n",
    "        (\"CRITICAL\", \"#E74C3C\", \"white\"),\n",
    "        (\"HIGH\", \"#E67E22\", \"white\"), \n",
    "        (\"MEDIUM\", \"#F39C12\", \"black\"),\n",
    "        (\"LOW\", \"#27AE60\", \"white\"),\n",
    "        (\"MINIMAL\", \"#BDC3C7\", \"#2C3E50\")\n",
    "    ]\n",
    "    \n",
    "    for i, (label, color, text_color) in enumerate(legend_items):\n",
    "        x_pos = 0.8 + i * legend_spacing\n",
    "        legend_box = dict(boxstyle=\"round,pad=0.3\", facecolor=color, \n",
    "                         edgecolor='#2C3E50', linewidth=1.2, alpha=0.95)\n",
    "        ax.text(x_pos, legend_y, label, fontsize=11, weight='bold',\n",
    "                bbox=legend_box, ha='center', va='center', color=text_color)\n",
    "    \n",
    "    # Add key statistics in the legend area\n",
    "    stats_text = f\"Total words: {len(words)} | Max score: {max(scores):.3f} | Avg score: {np.mean(scores):.3f}\"\n",
    "    ax.text(6, 1.4, stats_text, ha='center', va='center',\n",
    "            fontsize=11, color='#7F8C8D', style='italic')\n",
    "\n",
    "def _create_word_importance_chart(ax, words, scores):\n",
    "    \"\"\"Create a polished word importance bar chart for poster presentation\"\"\"\n",
    "    # Get top words\n",
    "    word_scores = list(zip(words, scores))\n",
    "    word_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_n = min(8, len(word_scores))  # Reduced for cleaner look\n",
    "    top_words, top_scores = zip(*word_scores[:top_n])\n",
    "    \n",
    "    # Enhanced color scheme\n",
    "    colors = ['#FF4444', '#FF6644', '#FF8844', '#FFAA44', \n",
    "              '#FFCC44', '#DDCC44', '#BBAA44', '#99AA44'][:top_n]\n",
    "    \n",
    "    bars = ax.barh(range(len(top_words)), top_scores, color=colors, alpha=0.9, \n",
    "                   edgecolor='#333', linewidth=1.2)\n",
    "    \n",
    "    # Clean word labels (truncate if too long)\n",
    "    clean_words = [w[:12] + '...' if len(w) > 12 else w for w in top_words]\n",
    "    ax.set_yticks(range(len(clean_words)))\n",
    "    ax.set_yticklabels(clean_words, fontsize=11, weight='bold')\n",
    "    ax.set_xlabel('Importance Score', fontsize=11, weight='bold', color='#333')\n",
    "    ax.set_title('Top Important Words', fontsize=13, weight='bold', pad=12, color='#333')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    # Enhanced value labels\n",
    "    for i, (bar, score) in enumerate(zip(bars, top_scores)):\n",
    "        ax.text(score + 0.005, i, f'{score:.3f}', \n",
    "                va='center', ha='left', fontsize=10, weight='bold', color='#333')\n",
    "    \n",
    "    # Styling improvements\n",
    "    ax.grid(True, axis='x', alpha=0.3, linestyle='--')\n",
    "    ax.set_axisbelow(True)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('#333')\n",
    "        spine.set_linewidth(1.2)\n",
    "    ax.tick_params(colors='#333', labelsize=10)\n",
    "\n",
    "def _create_stats_summary(ax, scores, words, pred_label, model_name):\n",
    "    \"\"\"Create a clean statistics summary panel for poster presentation\"\"\"\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Title\n",
    "    ax.text(0.5, 0.95, 'Analysis Statistics', ha='center', va='top', \n",
    "            fontsize=14, weight='bold', color='#2C3E50',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='#ECF0F1', \n",
    "                     edgecolor='#34495E', linewidth=2))\n",
    "    \n",
    "    # Key statistics with cleaner formatting\n",
    "    stats_text = f\"\"\"Model: {model_name}\n",
    "Prediction: {pred_label}\n",
    "\n",
    "STATISTICS:\n",
    "Words analyzed: {len(words)}\n",
    "Max importance: {max(scores):.3f}\n",
    "Mean importance: {np.mean(scores):.3f}\n",
    "Std deviation: {np.std(scores):.3f}\n",
    "\n",
    "DISTRIBUTION:\n",
    "Critical (>0.80): {sum(1 for s in scores if s > 0.80)}\n",
    "High (0.65-0.80): {sum(1 for s in scores if 0.65 <= s <= 0.80)}\n",
    "Medium (0.45-0.65): {sum(1 for s in scores if 0.45 <= s <= 0.65)}\n",
    "Low (0.25-0.45): {sum(1 for s in scores if 0.25 <= s <= 0.45)}\n",
    "Minimal (<0.25): {sum(1 for s in scores if s < 0.25)}\"\"\"\n",
    "    \n",
    "    ax.text(0.05, 0.85, stats_text.strip(), ha='left', va='top', \n",
    "            fontsize=10, color='#2C3E50', linespacing=1.5,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.4\", facecolor='#F8F9FA', \n",
    "                     edgecolor='#BDC3C7', linewidth=1, alpha=0.95))\n",
    "\n",
    "def _create_attribution_distribution(ax, scores, words):\n",
    "    \"\"\"Create attribution score distribution\"\"\"\n",
    "    ax.hist(scores, bins=20, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Attribution Score', fontsize=11)\n",
    "    ax.set_ylabel('Number of Words', fontsize=11)\n",
    "    ax.set_title('Distribution of Word Importance Scores', fontsize=12, weight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "    ax.axvline(mean_score, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_score:.3f}')\n",
    "    ax.axvline(mean_score + std_score, color='orange', linestyle='--', alpha=0.7, label=f'+1œÉ: {mean_score + std_score:.3f}')\n",
    "    ax.legend(fontsize=10)\n",
    "\n",
    "print(\"‚úÖ GradCAM implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üéõÔ∏è Interactive Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced Dashboard class defined\n"
     ]
    }
   ],
   "source": [
    "class GeneralizedExplainabilityDashboard:\n",
    "    \"\"\"Interactive dashboard for model explainability analysis with model selection\"\"\"\n",
    "    \n",
    "    def __init__(self, available_models, validation_texts, validation_labels, validation_labels_text):\n",
    "        self.available_models = available_models\n",
    "        self.validation_texts = validation_texts\n",
    "        self.validation_labels = validation_labels\n",
    "        self.validation_labels_text = validation_labels_text\n",
    "        self.current_model_wrapper = None\n",
    "        self.setup_data()\n",
    "        self.create_interface()\n",
    "    \n",
    "    def setup_data(self):\n",
    "        \"\"\"Setup data for mistake analysis (will be updated when model changes)\"\"\"\n",
    "        self.incorrect_indices = []\n",
    "        print(\"üìä Dashboard initialized - select a model to analyze mistakes\")\n",
    "    \n",
    "    def update_mistake_data(self):\n",
    "        \"\"\"Update mistake analysis data when model changes\"\"\"\n",
    "        if self.current_model_wrapper is None:\n",
    "            return\n",
    "        \n",
    "        print(f\"üîÑ Analyzing mistakes for {self.current_model_wrapper.model_name}...\")\n",
    "        predictions = self.current_model_wrapper.predict_class(self.validation_texts)\n",
    "        self.incorrect_indices = np.where(predictions != np.array(self.validation_labels))[0]\n",
    "        \n",
    "        # Update mistake selector options\n",
    "        mistake_options = [(f\"Mistake {i+1}: {self.validation_texts[idx][:50]}...\", i) \n",
    "                          for i, idx in enumerate(self.incorrect_indices[:20])]  # Limit for performance\n",
    "        \n",
    "        if len(mistake_options) == 0:\n",
    "            mistake_options = [(\"No mistakes found!\", 0)]\n",
    "        \n",
    "        self.mistake_selector.options = mistake_options\n",
    "        print(f\"üìä Found {len(self.incorrect_indices)} mistakes out of {len(self.validation_texts)} samples\")\n",
    "    \n",
    "    def create_interface(self):\n",
    "        \"\"\"Create the dashboard interface\"\"\"\n",
    "        # Model selector\n",
    "        model_options = [(name, name) for name in self.available_models.keys()]\n",
    "        self.model_selector = widgets.Dropdown(\n",
    "            options=model_options,\n",
    "            value=None,  # No default selection\n",
    "            description='Select Model:',\n",
    "            style={'description_width': '120px'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        # Model type selector\n",
    "        self.model_type_selector = widgets.ToggleButtons(\n",
    "            options=[('PyTorch', 'pytorch'), ('ONNX', 'onnx')],\n",
    "            value='pytorch',  # Default to PyTorch\n",
    "            description='Model Type:',\n",
    "            style={'description_width': '120px'},\n",
    "            disabled=True  # Enable after model selection\n",
    "        )\n",
    "        \n",
    "        # Input mode selector\n",
    "        self.input_mode = widgets.ToggleButtons(\n",
    "            options=[('Custom Text', 'custom'), ('Analyze Mistakes', 'mistakes')],\n",
    "            value='custom',  # Start with custom text since no model is selected yet\n",
    "            description='Analysis Mode:',\n",
    "            style={'description_width': '120px'}\n",
    "        )\n",
    "        \n",
    "        # Mistake selector\n",
    "        self.mistake_selector = widgets.Dropdown(\n",
    "            options=[(\"Select a model first\", 0)],\n",
    "            description='Select Mistake:',\n",
    "            style={'description_width': '120px'},\n",
    "            layout=widgets.Layout(width='600px'),\n",
    "            disabled=True  # Enable after model loading\n",
    "        )\n",
    "        \n",
    "        # Custom text input\n",
    "        self.text_input = widgets.Textarea(\n",
    "            value='The company reported strong quarterly earnings with revenue growth exceeding expectations.',\n",
    "            placeholder='Enter financial text to analyze...',\n",
    "            description='Text:',\n",
    "            layout=widgets.Layout(width='100%', height='80px'),\n",
    "            style={'description_width': '60px'}\n",
    "        )\n",
    "        \n",
    "        # Control buttons\n",
    "        self.load_model_button = widgets.Button(\n",
    "            description='üîÑ Load Model',\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='150px'),\n",
    "            disabled=True  # Enable after model selection\n",
    "        )\n",
    "        \n",
    "        self.analyze_button = widgets.Button(\n",
    "            description='üöÄ Analyze',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='120px'),\n",
    "            disabled=True  # Enable after model loading\n",
    "        )\n",
    "        \n",
    "        self.clear_button = widgets.Button(\n",
    "            description='üßπ Clear',\n",
    "            button_style='warning',\n",
    "            layout=widgets.Layout(width='120px')\n",
    "        )\n",
    "        \n",
    "        # Method selection\n",
    "        available_methods = []\n",
    "        if shap_available:\n",
    "            available_methods.append('SHAP')\n",
    "        if lime_available:\n",
    "            available_methods.append('LIME')\n",
    "        available_methods.append('Attention')  # Will check PyTorch requirement later\n",
    "        if captum_available:\n",
    "            available_methods.append('GradCAM')  # Will check PyTorch requirement later\n",
    "        \n",
    "        self.method_selector = widgets.SelectMultiple(\n",
    "            options=available_methods,\n",
    "            value=[available_methods[0]] if available_methods else [],\n",
    "            description='Methods:',\n",
    "            style={'description_width': '80px'},\n",
    "            layout=widgets.Layout(width='200px', height='120px')\n",
    "        )\n",
    "        \n",
    "        # Output tabs\n",
    "        self.output_tabs = widgets.Tab()\n",
    "        self.method_outputs = {}\n",
    "        \n",
    "        # Status output\n",
    "        self.status_output = widgets.Output()\n",
    "        \n",
    "        # Event handlers\n",
    "        self.model_selector.observe(self.on_model_change, names='value')\n",
    "        self.model_type_selector.observe(self.on_model_type_change, names='value')\n",
    "        self.input_mode.observe(self.on_mode_change, names='value')\n",
    "        self.load_model_button.on_click(self.on_load_model)\n",
    "        self.analyze_button.on_click(self.on_analyze)\n",
    "        self.clear_button.on_click(self.on_clear)\n",
    "        \n",
    "        # Initialize with first model if available\n",
    "        if model_options:\n",
    "            # Don't auto-select, let user choose\n",
    "            pass\n",
    "    \n",
    "    def on_model_change(self, change):\n",
    "        \"\"\"Handle model selection change\"\"\"\n",
    "        if change['new'] is None:\n",
    "            self.model_type_selector.disabled = True\n",
    "            self.load_model_button.disabled = True\n",
    "            return\n",
    "            \n",
    "        model_info = self.available_models[change['new']]\n",
    "        available_types = []\n",
    "        \n",
    "        if model_info['has_pytorch']:\n",
    "            available_types.append(('PyTorch', 'pytorch'))\n",
    "        if model_info['has_onnx']:\n",
    "            available_types.append(('ONNX', 'onnx'))\n",
    "        \n",
    "        self.model_type_selector.options = available_types\n",
    "        self.model_type_selector.disabled = False\n",
    "        self.load_model_button.disabled = False\n",
    "        \n",
    "        # Set default selection\n",
    "        if available_types:\n",
    "            if model_info['has_pytorch']:\n",
    "                self.model_type_selector.value = 'pytorch'\n",
    "            else:\n",
    "                self.model_type_selector.value = 'onnx'\n",
    "        \n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"üìã Selected: {change['new']}\")\n",
    "            print(f\"‚úÖ Available formats: {', '.join([t[0] for t in available_types])}\")\n",
    "            print(\"üëÜ Click 'Load Model' to initialize\")\n",
    "    \n",
    "    def on_model_type_change(self, change):\n",
    "        \"\"\"Handle model type change\"\"\"\n",
    "        if self.model_selector.value and change['new']:\n",
    "            with self.status_output:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"üìã Model: {self.model_selector.value}\")\n",
    "                print(f\"üîß Type: {change['new']}\")\n",
    "                print(\"üëÜ Click 'Load Model' to initialize\")\n",
    "    \n",
    "    def on_mode_change(self, change):\n",
    "        \"\"\"Handle input mode change\"\"\"\n",
    "        # Interface will be updated in display method\n",
    "        pass\n",
    "    \n",
    "    def on_load_model(self, button):\n",
    "        \"\"\"Load the selected model\"\"\"\n",
    "        try:\n",
    "            if not self.model_selector.value:\n",
    "                with self.status_output:\n",
    "                    clear_output(wait=True)\n",
    "                    print(\"‚ùå Please select a model first!\")\n",
    "                return\n",
    "            \n",
    "            model_info = self.available_models[self.model_selector.value]\n",
    "            model_type = self.model_type_selector.value\n",
    "            \n",
    "            with self.status_output:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"üîÑ Loading {model_info['name']} ({model_type})...\")\n",
    "                print(\"‚è≥ This may take a moment...\")\n",
    "            \n",
    "            # Load the model\n",
    "            self.current_model_wrapper = UniversalModelWrapper(model_info, model_type)\n",
    "            \n",
    "            # Update mistake data\n",
    "            self.update_mistake_data()\n",
    "            \n",
    "            # Enable analysis controls\n",
    "            self.analyze_button.disabled = False\n",
    "            self.mistake_selector.disabled = False\n",
    "            \n",
    "            # Update method availability based on model type\n",
    "            available_methods = []\n",
    "            if shap_available:\n",
    "                available_methods.append('SHAP')\n",
    "            if lime_available:\n",
    "                available_methods.append('LIME')\n",
    "            if model_type == 'pytorch':  # Attention and GradCAM only work with PyTorch\n",
    "                available_methods.append('Attention')\n",
    "                if captum_available:\n",
    "                    available_methods.append('GradCAM')\n",
    "            \n",
    "            self.method_selector.options = available_methods\n",
    "            if available_methods:\n",
    "                self.method_selector.value = [available_methods[0]]\n",
    "            \n",
    "            with self.status_output:\n",
    "                clear_output(wait=True)\n",
    "                print(\"‚úÖ Model loaded successfully!\")\n",
    "                print(f\"üìä Model: {model_info['name']} ({model_type})\")\n",
    "                print(f\"üéØ Classes: {', '.join(self.current_model_wrapper.get_class_names())}\")\n",
    "                print(f\"üìã Available methods: {', '.join(available_methods)}\")\n",
    "                print(f\"üîç Found {len(self.incorrect_indices)} mistakes for analysis\")\n",
    "                print(\"\\nüöÄ Ready for analysis! Select methods and click 'Analyze'\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            with self.status_output:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"‚ùå Error loading model: {str(e)}\")\n",
    "                print(\"üí° Try selecting a different model or format\")\n",
    "    \n",
    "    def on_analyze(self, button):\n",
    "        \"\"\"Handle analyze button click\"\"\"\n",
    "        try:\n",
    "            if self.current_model_wrapper is None:\n",
    "                with self.status_output:\n",
    "                    clear_output(wait=True)\n",
    "                    print(\"‚ùå Please load a model first!\")\n",
    "                return\n",
    "            \n",
    "            # Get text and prediction info\n",
    "            if self.input_mode.value == 'mistakes':\n",
    "                if not self.incorrect_indices.size:\n",
    "                    with self.status_output:\n",
    "                        clear_output(wait=True)\n",
    "                        print(\"‚ùå No mistakes found for this model!\")\n",
    "                    return\n",
    "                \n",
    "                mistake_idx = self.mistake_selector.value\n",
    "                if mistake_idx >= len(self.incorrect_indices):\n",
    "                    with self.status_output:\n",
    "                        clear_output(wait=True)\n",
    "                        print(\"‚ùå Invalid mistake selection!\")\n",
    "                    return\n",
    "                \n",
    "                sample_idx = self.incorrect_indices[mistake_idx]\n",
    "                text = self.validation_texts[sample_idx]\n",
    "                true_label = self.validation_labels_text[sample_idx]\n",
    "                pred_class = int(self.current_model_wrapper.predict_class(text)[0])\n",
    "                pred_label = self.current_model_wrapper.label_encoder.inverse_transform([pred_class])[0]\n",
    "            else:\n",
    "                text = self.text_input.value.strip()\n",
    "                if not text:\n",
    "                    with self.status_output:\n",
    "                        clear_output(wait=True)\n",
    "                        print(\"‚ùå Please enter some text to analyze!\")\n",
    "                    return\n",
    "                pred_class = int(self.current_model_wrapper.predict_class(text)[0])\n",
    "                pred_label = self.current_model_wrapper.label_encoder.inverse_transform([pred_class])[0]\n",
    "                true_label = \"Unknown\"\n",
    "            \n",
    "            # Generate explanations for selected methods\n",
    "            self.generate_explanations(text, pred_label, true_label, pred_class)\n",
    "            \n",
    "        except Exception as e:\n",
    "            with self.status_output:\n",
    "                clear_output(wait=True)\n",
    "                print(f\"‚ùå Error during analysis: {str(e)}\")\n",
    "                import traceback\n",
    "                print(f\"üîç Details: {traceback.format_exc()}\")\n",
    "    \n",
    "    def generate_explanations(self, text, pred_label, true_label, pred_class):\n",
    "        \"\"\"Generate selected explanations for the text\"\"\"\n",
    "        selected_methods = list(self.method_selector.value)\n",
    "        \n",
    "        if not selected_methods:\n",
    "            with self.status_output:\n",
    "                clear_output(wait=True)\n",
    "                print(\"‚ùå Please select at least one explanation method!\")\n",
    "            return\n",
    "        \n",
    "        # Create method outputs\n",
    "        self.method_outputs = {}\n",
    "        for method in selected_methods:\n",
    "            self.method_outputs[method] = widgets.Output()\n",
    "        \n",
    "        self.output_tabs.children = list(self.method_outputs.values())\n",
    "        for i, method in enumerate(self.method_outputs.keys()):\n",
    "            self.output_tabs.set_title(i, f'{method}')\n",
    "        \n",
    "        # Create header\n",
    "        header_html = f\"\"\"\n",
    "        <div style='background: #f8f9fa; padding: 15px; margin: 10px 0; border-radius: 8px; \n",
    "                    border-left: 4px solid #007bff; box-shadow: 0 2px 8px rgba(0,0,0,0.1);'>\n",
    "            <h4 style='margin: 0 0 10px 0; color: #007bff;'>üìù Analysis Summary</h4>\n",
    "            <p style='margin: 5px 0;'><strong>Text:</strong> <em>\"{text[:200]}{'...' if len(text) > 200 else ''}\"</em></p>\n",
    "            <p style='margin: 5px 0;'><strong>Model:</strong> {self.current_model_wrapper.model_name} ({self.current_model_wrapper.model_type})</p>\n",
    "            <p style='margin: 5px 0;'><strong>Prediction:</strong> \n",
    "               <span style='color: #28a745; font-weight: bold;'>{pred_label}</span></p>\n",
    "            {f'<p style=\"margin: 5px 0;\"><strong>True Label:</strong> <span style=\"color: #dc3545; font-weight: bold;\">{true_label}</span></p>' if true_label != \"Unknown\" else ''}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"üß† Generating explanations...\")\n",
    "            print(f\"üìã Methods: {', '.join(selected_methods)}\")\n",
    "            print(\"‚è≥ This may take a moment...\")\n",
    "        \n",
    "        # Generate explanations for each selected method\n",
    "        for i, method in enumerate(selected_methods):\n",
    "            with self.method_outputs[method]:\n",
    "                display(HTML(header_html))\n",
    "                \n",
    "                try:\n",
    "                    # print(f\"üîç Running {method} analysis...\")\n",
    "                    if method == 'SHAP':\n",
    "                        explain_with_shap(self.current_model_wrapper, text, pred_class)\n",
    "                    elif method == 'LIME':\n",
    "                        explain_with_lime(self.current_model_wrapper, text)\n",
    "                    elif method == 'Attention':\n",
    "                        explain_with_attention(self.current_model_wrapper, text)\n",
    "                    elif method == 'GradCAM':\n",
    "                        explain_with_gradcam(self.current_model_wrapper, text)\n",
    "                    print(f\"‚úÖ {method} analysis complete!\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå {method} failed: {str(e)}\")\n",
    "                    import traceback\n",
    "                    print(f\"üîç Error details: {traceback.format_exc()}\")\n",
    "        \n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"‚úÖ Analysis complete!\")\n",
    "            print(\"üìä Check the tabs above for detailed explanations\")\n",
    "            print(\"üîÑ Modify text or methods and click 'Analyze' again for new results\")\n",
    "    \n",
    "    def on_clear(self, button):\n",
    "        \"\"\"Clear all outputs\"\"\"\n",
    "        for output in self.method_outputs.values():\n",
    "            with output:\n",
    "                clear_output()\n",
    "        \n",
    "        with self.status_output:\n",
    "            clear_output(wait=True)\n",
    "            print(\"üßπ All results cleared! Ready for new analysis.\")\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the dashboard\"\"\"\n",
    "        # Title\n",
    "        title = widgets.HTML(\n",
    "            value=\"\"\"\n",
    "            <div style='text-align: center; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "                        color: white; padding: 20px; border-radius: 10px; margin-bottom: 20px;'>\n",
    "                <h2 style='margin: 0; font-size: 24px;'>üß† Generalised Financial Sentiment Explainability Dashboard</h2>\n",
    "                <p style='margin: 10px 0 0 0; opacity: 0.9;'>Universal AI model explanation and analysis</p>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        # Model selection section\n",
    "        model_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>üîß Model Configuration</h3>\"),\n",
    "            widgets.HBox([self.model_selector, self.model_type_selector, self.load_model_button],\n",
    "                        layout=widgets.Layout(align_items='center'))\n",
    "        ])\n",
    "        \n",
    "        # Create dynamic input container\n",
    "        def get_input_widget():\n",
    "            if self.input_mode.value == 'mistakes':\n",
    "                return self.mistake_selector\n",
    "            else:\n",
    "                return self.text_input\n",
    "        \n",
    "        # Input section\n",
    "        input_container = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>üìù Analysis Input</h3>\"),\n",
    "            self.input_mode,\n",
    "            get_input_widget()\n",
    "        ])\n",
    "        \n",
    "        # Update input container based on mode\n",
    "        def update_input_display(*args):\n",
    "            input_container.children = [\n",
    "                widgets.HTML(\"<h3>üìù Analysis Input</h3>\"),\n",
    "                self.input_mode, \n",
    "                get_input_widget()\n",
    "            ]\n",
    "        \n",
    "        self.input_mode.observe(update_input_display, names='value')\n",
    "        \n",
    "        # Controls section\n",
    "        controls_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>‚ö° Analysis Controls</h3>\"),\n",
    "            widgets.HBox([\n",
    "                self.method_selector,\n",
    "                widgets.VBox([self.analyze_button, self.clear_button],\n",
    "                           layout=widgets.Layout(align_items='center'))\n",
    "            ])\n",
    "        ])\n",
    "        \n",
    "        # Status section\n",
    "        status_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>üìä Status</h3>\"),\n",
    "            self.status_output\n",
    "        ])\n",
    "        \n",
    "        # Results section\n",
    "        results_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>üìà Results</h3>\"),\n",
    "            self.output_tabs\n",
    "        ])\n",
    "        \n",
    "        # Main dashboard\n",
    "        dashboard = widgets.VBox([\n",
    "            title,\n",
    "            model_section,\n",
    "            input_container,\n",
    "            controls_section,\n",
    "            status_section,\n",
    "            results_section\n",
    "        ], layout=widgets.Layout(padding='10px'))\n",
    "        \n",
    "        return dashboard\n",
    "\n",
    "print(\"‚úÖ Enhanced Dashboard class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dashboard initialized - select a model to analyze mistakes\n",
      "üöÄ Enhanced Dashboard initialized successfully!\n",
      " Found 7 models ready for analysis\n",
      "\n",
      "  Quick Start Instructions:\n",
      "1. üéØ Select a model from the dropdown\n",
      "2. üîß Choose PyTorch or ONNX format\n",
      "3. üîÑ Click 'Load Model' to initialize\n",
      "4. üìù Enter text or select 'Analyze Mistakes'\n",
      "5. ‚úÖ Pick explanation methods and click 'Analyze'\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6176caac82f40a38396888e56e5fa2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"\\n            <div style='text-align: center; background: linear-gradient(135deg, #‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ Dashboard ready! Start by selecting a model above.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and display the dashboard\n",
    "try:\n",
    "    # Clear any existing dashboard\n",
    "    from IPython.display import clear_output\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    dashboard = GeneralizedExplainabilityDashboard(\n",
    "        available_models=available_models,\n",
    "        validation_texts=validation_texts,\n",
    "        validation_labels=validation_labels,\n",
    "        validation_labels_text=validation_labels_text\n",
    "    )\n",
    "    \n",
    "    print(\"üöÄ Enhanced Dashboard initialized successfully!\")\n",
    "    print(f\" Found {len(available_models)} models ready for analysis\")\n",
    "    print(\"\\n  Quick Start Instructions:\")\n",
    "    print(\"1. üéØ Select a model from the dropdown\")\n",
    "    print(\"2. üîß Choose PyTorch or ONNX format\") \n",
    "    print(\"3. üîÑ Click 'Load Model' to initialize\")\n",
    "    print(\"4. üìù Enter text or select 'Analyze Mistakes'\")\n",
    "    print(\"5. ‚úÖ Pick explanation methods and click 'Analyze'\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Display the dashboard with proper widget rendering\n",
    "    dashboard_widget = dashboard.display()\n",
    "    display(dashboard_widget)\n",
    "    \n",
    "    print(\"\\nüéâ Dashboard ready! Start by selecting a model above.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing dashboard: {str(e)}\")\n",
    "    import traceback\n",
    "    print(f\"üîç Full error details:\\n{traceback.format_exc()}\")\n",
    "    print(\"\\nüîß Fallback: Individual analysis functions available:\")\n",
    "    print(\"- explain_with_shap(model_wrapper, text, pred_class)\")\n",
    "    print(\"- explain_with_lime(model_wrapper, text)\")\n",
    "    print(\"- explain_with_attention(model_wrapper, text)\")\n",
    "    print(\"- explain_with_gradcam(model_wrapper, text)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üìö Example Usage & Next Steps\n",
    "\n",
    "This generalized explainability notebook provides comprehensive model interpretation capabilities across your entire model collection. Here's how to make the most of it:\n",
    "\n",
    "### üéØ Key Features\n",
    "- **Universal Model Support**: Works with both PyTorch and ONNX models from your collection\n",
    "- **Four Explainability Methods**: SHAP, LIME, Attention Visualization, and GradCAM\n",
    "- **Interactive Dashboard**: Point-and-click interface for quick analysis\n",
    "- **Mistake Analysis**: Automatically finds and analyzes model misclassifications\n",
    "- **Custom Text Analysis**: Test any financial text with multiple explanation methods\n",
    "\n",
    "### üîß Configuration-Driven Approach\n",
    "All settings are loaded from `../config/pipeline_config.json`, ensuring consistency with your training and evaluation pipeline:\n",
    "- Model paths and metadata\n",
    "- Validation datasets\n",
    "- Analysis parameters\n",
    "- Output configurations\n",
    "\n",
    "### üöÄ For Fine-Tuning Integration\n",
    "This notebook is designed to support your fine-tuning workflow:\n",
    "\n",
    "1. **Pre-Fine-Tuning Analysis**: Understand what your base models focus on\n",
    "2. **Mistake Identification**: Find systematic errors to address in fine-tuning\n",
    "3. **Post-Fine-Tuning Comparison**: Compare explanations before and after fine-tuning\n",
    "4. **Method Selection**: Choose the best explanation methods for your specific use case\n",
    "\n",
    "### üé® Visualization Options\n",
    "- **SHAP**: Feature importance with bidirectional influence\n",
    "- **LIME**: Local interpretable model-agnostic explanations\n",
    "- **Attention**: Attention weight heatmaps (PyTorch models only)\n",
    "- **GradCAM**: Gradient-based class activation mapping (PyTorch models only)\n",
    "- **Interactive Interface**: Compare multiple methods side-by-side\n",
    "\n",
    "### üìà Performance Considerations\n",
    "- Models are loaded on-demand to manage memory\n",
    "- ONNX models typically provide faster inference\n",
    "- Explanation methods have different computational requirements\n",
    "- Results are cached for repeated analysis\n",
    "\n",
    "### üîÑ Next Steps\n",
    "1. Run the dashboard above to start exploring your models\n",
    "2. Analyze systematic mistakes across different model architectures\n",
    "3. Use insights to guide fine-tuning data augmentation\n",
    "4. Compare explanation consistency across model types\n",
    "5. Document findings for model selection decisions\n",
    "\n",
    "Happy exploring! üß†‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
