{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Performance Benchmarking - Generalized\n",
    "\n",
    "This notebook provides comprehensive benchmarking of trained models.\n",
    "\n",
    "**Configuration-driven approach:** All settings loaded from `../config/pipeline_config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration system and benchmarking utilities\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.pipeline_utils import ConfigManager, StateManager, LoggingManager\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import platform\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ONNX and optimization imports\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "    onnx_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ONNXRuntime not installed. Install with: pip install onnxruntime\")\n",
    "    onnx_available = False\n",
    "\n",
    "# Hugging Face transformers\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import pickle\n",
    "    transformers_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Transformers not installed\")\n",
    "    transformers_available = False\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Resource control imports\n",
    "import psutil\n",
    "\n",
    "# Initialize managers\n",
    "config = ConfigManager(\"../config/pipeline_config.json\")\n",
    "state = StateManager(\"../config/pipeline_state.json\")\n",
    "logger_manager = LoggingManager(config, 'benchmarking')\n",
    "logger = logger_manager.get_logger()\n",
    "\n",
    "# Apply CPU and memory limits to prevent system freeze\n",
    "MAX_CPU_THREADS = 2  # Limit to 2 cores\n",
    "os.environ['OMP_NUM_THREADS'] = str(MAX_CPU_THREADS)\n",
    "os.environ['MKL_NUM_THREADS'] = str(MAX_CPU_THREADS)\n",
    "os.environ['NUMEXPR_NUM_THREADS'] = str(MAX_CPU_THREADS)\n",
    "torch.set_num_threads(MAX_CPU_THREADS)\n",
    "\n",
    "print(f\"üîß Resource limits applied:\")\n",
    "print(f\"   üñ•Ô∏è CPU threads: {MAX_CPU_THREADS}\")\n",
    "print(f\"   üíæ Available memory: {psutil.virtual_memory().available / (1024**3):.1f}GB\")\n",
    "\n",
    "logger.info(\"üìä Starting Performance Benchmarking - Generalized Pipeline\")\n",
    "print(\"üìã Configuration loaded from ../config/pipeline_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking Data Structures and Configuration\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Structured class to hold results from a single benchmark run.\"\"\"\n",
    "    model_name: str\n",
    "    model_type: str  # 'pytorch' or 'onnx'\n",
    "    batch_size: int\n",
    "    avg_latency_ms: float\n",
    "    p50_latency_ms: float\n",
    "    p95_latency_ms: float\n",
    "    p99_latency_ms: float\n",
    "    throughput_samples_per_sec: float\n",
    "    peak_memory_mb: float\n",
    "    model_size_mb: float\n",
    "    provider: str\n",
    "    accuracy: Optional[float] = None\n",
    "    f1_score: Optional[float] = None\n",
    "    weighted_accuracy: Optional[float] = None\n",
    "    weighted_f1_score: Optional[float] = None\n",
    "    avg_confidence_correct: Optional[float] = None\n",
    "    avg_confidence_incorrect: Optional[float] = None\n",
    "    per_class_metrics: Optional[Dict] = None\n",
    "    validation_samples: Optional[int] = None\n",
    "    \n",
    "    # Enhanced metrics\n",
    "    std_latency_ms: Optional[float] = None\n",
    "    min_latency_ms: Optional[float] = None\n",
    "    max_latency_ms: Optional[float] = None\n",
    "    avg_memory_mb: Optional[float] = None\n",
    "    memory_overhead_mb: Optional[float] = None\n",
    "    efficiency_score: Optional[float] = None  # throughput / model_size_mb\n",
    "    latency_stability: Optional[float] = None  # coefficient of variation (lower is better)\n",
    "    memory_efficiency: Optional[float] = None  # throughput / peak_memory_mb\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary with flattened per-class metrics.\"\"\"\n",
    "        flat_dict = asdict(self)\n",
    "        per_class = flat_dict.pop(\"per_class_metrics\", {})\n",
    "        if per_class:\n",
    "            for class_name, metrics in per_class.items():\n",
    "                for metric_name, value in metrics.items():\n",
    "                    flat_dict[f\"{class_name}_{metric_name}\"] = value\n",
    "        \n",
    "        # Calculate derived metrics\n",
    "        if flat_dict.get('throughput_samples_per_sec') and flat_dict.get('model_size_mb'):\n",
    "            flat_dict['efficiency_score'] = flat_dict['throughput_samples_per_sec'] / flat_dict['model_size_mb']\n",
    "        if flat_dict.get('std_latency_ms') and flat_dict.get('avg_latency_ms'):\n",
    "            flat_dict['latency_stability'] = flat_dict['std_latency_ms'] / flat_dict['avg_latency_ms']  # CV\n",
    "        if flat_dict.get('throughput_samples_per_sec') and flat_dict.get('peak_memory_mb') and flat_dict['peak_memory_mb'] > 0:\n",
    "            flat_dict['memory_efficiency'] = flat_dict['throughput_samples_per_sec'] / flat_dict['peak_memory_mb']\n",
    "            \n",
    "        return flat_dict\n",
    "\n",
    "class ExecutionProviderManager:\n",
    "    \"\"\"Manages ONNX execution providers for consistent benchmarking.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_execution_providers() -> List[str]:\n",
    "        \"\"\"Get available execution providers for benchmarking (CPU only to avoid issues).\"\"\"\n",
    "        # Only use CPU provider to prevent CoreML errors and reduce resource usage\n",
    "        return ['CPUExecutionProvider'] if onnx_available else []\n",
    "\n",
    "# Load benchmarking configuration\n",
    "benchmark_config = config.get('benchmarking', {})\n",
    "models_config = config.get('models', {})\n",
    "data_config = config.get('data', {})\n",
    "\n",
    "# Resource-aware configuration - All settings loaded from config file\n",
    "FAST_MODE = benchmark_config.get('fast_mode', True)\n",
    "TARGET_MODELS = benchmark_config.get('target_models', [])  # Load from config, fallback to empty list (test all models)\n",
    "EXCLUDE_MODELS = benchmark_config.get('exclude_models', [])  # Load from config, fallback to empty list (exclude none)\n",
    "INCLUDE_ACCURACY = benchmark_config.get('include_accuracy', True)  # Enable accuracy by default for better insights\n",
    "\n",
    "print(\"üìä Benchmarking Configuration:\")\n",
    "print(f\"   ‚ö° Fast mode: {FAST_MODE}\")\n",
    "print(f\"   üéØ Target models: {TARGET_MODELS if TARGET_MODELS else 'All available models'}\")\n",
    "print(f\"   ‚ùå Excluded models: {EXCLUDE_MODELS if EXCLUDE_MODELS else 'None'}\")\n",
    "print(f\"   üîß CPU threads limited to: {MAX_CPU_THREADS}\")\n",
    "print(f\"   üìä Include accuracy: {INCLUDE_ACCURACY}\")\n",
    "\n",
    "# Performance parameters - Resource-optimized for development environment\n",
    "\n",
    "# Performance parameters - Resource-optimized for development environment\n",
    "if FAST_MODE:\n",
    "    ITERATIONS = benchmark_config.get('iterations', {}).get('fast', 15)  # Increased for better precision\n",
    "    WARMUP_ITERATIONS = benchmark_config.get('warmup_iterations', {}).get('fast', 5)  # Increased warmup\n",
    "    BATCH_SIZES = benchmark_config.get('batch_sizes', {}).get('fast', [1, 4])  # Reduced batch sizes\n",
    "    ACCURACY_SAMPLE_SIZE = benchmark_config.get('accuracy_sample_size', {}).get('fast', 500)  # Increased for better accuracy measurement\n",
    "else:\n",
    "    ITERATIONS = benchmark_config.get('iterations', {}).get('thorough', 30)  # Increased for precision\n",
    "    WARMUP_ITERATIONS = benchmark_config.get('warmup_iterations', {}).get('thorough', 10)  # More warmup\n",
    "    BATCH_SIZES = benchmark_config.get('batch_sizes', {}).get('thorough', [1, 4, 8])  # Reduced batch sizes\n",
    "    ACCURACY_SAMPLE_SIZE = benchmark_config.get('accuracy_sample_size', {}).get('thorough', 1000)  # Large sample for thorough mode\n",
    "\n",
    "print(f\"   üîÑ Iterations: {ITERATIONS} (warmup: {WARMUP_ITERATIONS})\")\n",
    "print(f\"   üìä Batch sizes: {BATCH_SIZES}\")\n",
    "print(f\"   üéØ Accuracy samples: {ACCURACY_SAMPLE_SIZE if INCLUDE_ACCURACY else 'Disabled'}\")\n",
    "\n",
    "logger.info(\"Benchmarking configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Configuration-Driven Benchmarking\n",
    "\n",
    "All benchmarking parameters are now loaded from `../config/pipeline_config.json` under the `benchmarking` section:\n",
    "\n",
    "- **`target_models`**: Specify which models to benchmark (empty list = test all available models)\n",
    "- **`exclude_models`**: Models to explicitly exclude from benchmarking  \n",
    "- **`fast_mode`**: Toggle between fast and thorough benchmarking modes\n",
    "- **`include_accuracy`**: Enable/disable accuracy evaluation during benchmarking\n",
    "- **`iterations`** & **`warmup_iterations`**: Control precision vs. speed trade-off\n",
    "- **`batch_sizes`**: Batch sizes to test for each model\n",
    "- **`accuracy_sample_size`**: Number of samples for accuracy evaluation\n",
    "\n",
    "**Validation Data**: Uses actual datasets from `../data/processed/validation.csv` or `../data/FinancialPhraseBank/all-data.csv` for realistic accuracy measurements.\n",
    "\n",
    "This approach makes it easy to customize benchmarking without code changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Discovery and Loading\n",
    "logger.info(\"üîç Discovering available models...\")\n",
    "\n",
    "# Verify prerequisites\n",
    "if not onnx_available or not transformers_available:\n",
    "    raise ImportError(\"Required libraries missing. Install with: pip install onnxruntime transformers\")\n",
    "\n",
    "# Check if ONNX conversion was completed\n",
    "if not state.is_step_complete('onnx_conversion_completed'):\n",
    "    print(\"‚ö†Ô∏è ONNX conversion not completed. ONNX benchmarks will be skipped.\")\n",
    "    onnx_conversion_completed = False\n",
    "else:\n",
    "    onnx_conversion_completed = True\n",
    "\n",
    "models_dir = Path(f\"../{models_config.get('output_dir', 'models')}\")\n",
    "print(f\"üìÇ Models directory: {models_dir}\")\n",
    "\n",
    "# Discover available models\n",
    "available_models = {}\n",
    "\n",
    "if models_dir.exists():\n",
    "    for model_path in models_dir.iterdir():\n",
    "        if not model_path.is_dir() or model_path.name.startswith('.'):\n",
    "            continue\n",
    "            \n",
    "        model_name = model_path.name\n",
    "        \n",
    "        # Check if model should be excluded\n",
    "        if any(excluded in model_name for excluded in EXCLUDE_MODELS):\n",
    "            print(f\"   ‚ùå Excluding {model_name} (in exclusion list)\")\n",
    "            continue\n",
    "        \n",
    "        # Check if model should be included based on TARGET_MODELS\n",
    "        # If TARGET_MODELS is empty, include all models (except excluded ones)\n",
    "        if TARGET_MODELS and model_name not in TARGET_MODELS:\n",
    "            print(f\"   ‚è≠Ô∏è Skipping {model_name} (not in target list)\")\n",
    "            continue\n",
    "        \n",
    "        # Check for PyTorch model files\n",
    "        config_file = model_path / \"config.json\"\n",
    "        pytorch_files = list(model_path.glob(\"*.safetensors\")) + list(model_path.glob(\"pytorch_model.bin\"))\n",
    "        \n",
    "        # Check for ONNX model files\n",
    "        onnx_dir = model_path / \"onnx\"\n",
    "        onnx_files = list(onnx_dir.glob(\"*.onnx\")) if onnx_dir.exists() else []\n",
    "        \n",
    "        if config_file.exists() and (pytorch_files or onnx_files):\n",
    "            model_info = {\n",
    "                'name': model_name,\n",
    "                'path': model_path,\n",
    "                'config_file': config_file,\n",
    "                'has_pytorch': len(pytorch_files) > 0,\n",
    "                'has_onnx': len(onnx_files) > 0,\n",
    "                'pytorch_files': pytorch_files,\n",
    "                'onnx_files': onnx_files\n",
    "            }\n",
    "            available_models[model_name] = model_info\n",
    "            \n",
    "            status = []\n",
    "            if model_info['has_pytorch']:\n",
    "                status.append(\"PyTorch\")\n",
    "            if model_info['has_onnx']:\n",
    "                status.append(\"ONNX\")\n",
    "            \n",
    "            print(f\"   ‚úÖ Found: {model_name} ({', '.join(status)})\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Invalid model directory: {model_name}\")\n",
    "\n",
    "print(f\"\\nüìä Discovery Summary:\")\n",
    "print(f\"   ü§ñ Total models found: {len(available_models)}\")\n",
    "pytorch_count = sum(1 for m in available_models.values() if m['has_pytorch'])\n",
    "onnx_count = sum(1 for m in available_models.values() if m['has_onnx'])\n",
    "print(f\"   üî• PyTorch models: {pytorch_count}\")\n",
    "print(f\"   ‚ö° ONNX models: {onnx_count}\")\n",
    "\n",
    "if len(available_models) == 0:\n",
    "    logger.error(\"No models found for benchmarking\")\n",
    "    raise RuntimeError(\"No models found. Please run model training and/or ONNX conversion first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Benchmarking Engine\n",
    "class ModelBenchmarker:\n",
    "    \"\"\"Comprehensive model benchmarking with standardized measurement.\"\"\"\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.execution_providers = ExecutionProviderManager.get_execution_providers()\n",
    "    \n",
    "    def measure_memory_usage(self) -> float:\n",
    "        \"\"\"Measure current memory usage in MB.\"\"\"\n",
    "        import psutil\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / 1024 / 1024  # Convert to MB\n",
    "    \n",
    "    def get_model_size(self, model_path: Path, model_type: str) -> float:\n",
    "        \"\"\"Get model file size in MB.\"\"\"\n",
    "        if model_type == 'pytorch':\n",
    "            model_files = list(model_path.glob(\"*.safetensors\")) + list(model_path.glob(\"pytorch_model.bin\"))\n",
    "            return sum(f.stat().st_size for f in model_files) / (1024 * 1024)\n",
    "        elif model_type == 'onnx':\n",
    "            onnx_files = list((model_path / \"onnx\").glob(\"*.onnx\"))\n",
    "            return sum(f.stat().st_size for f in onnx_files) / (1024 * 1024)\n",
    "        return 0.0\n",
    "    \n",
    "    def create_sample_batch(self, tokenizer, batch_size: int, max_length: int = 64) -> Tuple[Any, Any]:\n",
    "        \"\"\"Create a sample batch for benchmarking with reduced memory footprint.\"\"\"\n",
    "        # Use diverse sample texts for more realistic benchmarking\n",
    "        sample_texts = [\n",
    "            \"The company's quarterly earnings exceeded expectations by 15%.\",\n",
    "            \"Market volatility has increased due to geopolitical tensions.\",\n",
    "            \"The Federal Reserve announced a rate hike of 0.25 basis points.\",\n",
    "            \"Tech stocks declined following regulatory concerns.\",\n",
    "            \"Oil prices surged on supply chain disruptions.\"\n",
    "        ] * (batch_size // 5 + 1)  # Repeat to fill batch\n",
    "        \n",
    "        sample_texts = sample_texts[:batch_size]\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        inputs = tokenizer(\n",
    "            sample_texts,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt' if hasattr(tokenizer, 'model_max_length') else None\n",
    "        )\n",
    "        \n",
    "        return inputs['input_ids'], inputs['attention_mask']\n",
    "    \n",
    "    def get_validation_dataset(self):\n",
    "        \"\"\"Load the actual validation dataset from the data folder.\"\"\"\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            \n",
    "            # Load validation data from the processed dataset\n",
    "            validation_path = Path(\"../data/processed/validation.csv\")\n",
    "            \n",
    "            if not validation_path.exists():\n",
    "                # Fallback to main dataset if processed validation doesn't exist\n",
    "                self.logger.info(\"Processed validation set not found, using main dataset\")\n",
    "                validation_path = Path(\"../data/FinancialPhraseBank/all-data.csv\")\n",
    "                \n",
    "                # Load and process the main dataset\n",
    "                df = pd.read_csv(validation_path, header=None, names=['label', 'text'])\n",
    "                \n",
    "                # Convert string labels to numeric\n",
    "                label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "                df['label_numeric'] = df['label'].map(label_mapping)\n",
    "                \n",
    "                # Remove any rows with unmapped labels\n",
    "                df = df.dropna(subset=['label_numeric'])\n",
    "                \n",
    "                # Take a sample for validation (20% of data)\n",
    "                sample_size = min(1000, len(df) // 5)  # Max 1000 samples\n",
    "                df_sample = df.sample(n=sample_size, random_state=42)\n",
    "                \n",
    "                validation_texts = df_sample['text'].tolist()\n",
    "                validation_labels = df_sample['label_numeric'].astype(int).tolist()\n",
    "                \n",
    "            else:\n",
    "                # Load processed validation dataset\n",
    "                self.logger.info(f\"Loading validation dataset from {validation_path}\")\n",
    "                df = pd.read_csv(validation_path)\n",
    "                \n",
    "                # Convert string labels to numeric if needed\n",
    "                if df['label'].dtype == 'object':\n",
    "                    label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "                    df['label_numeric'] = df['label'].map(label_mapping)\n",
    "                else:\n",
    "                    df['label_numeric'] = df['label']\n",
    "                \n",
    "                # Remove any rows with unmapped labels\n",
    "                df = df.dropna(subset=['label_numeric'])\n",
    "                \n",
    "                validation_texts = df['text'].tolist()\n",
    "                validation_labels = df['label_numeric'].astype(int).tolist()\n",
    "            \n",
    "            self.logger.info(f\"Loaded {len(validation_texts)} validation samples\")\n",
    "            self.logger.info(f\"Label distribution: {pd.Series(validation_labels).value_counts().sort_index().to_dict()}\")\n",
    "            \n",
    "            return validation_texts, validation_labels\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to load validation dataset: {e}\")\n",
    "            self.logger.info(\"Falling back to synthetic validation data\")\n",
    "            \n",
    "            # Fallback to a smaller synthetic dataset if file loading fails\n",
    "            validation_texts = [\n",
    "                # Positive examples (label 2)\n",
    "                \"The company reported strong quarterly earnings with revenue up 15%.\",\n",
    "                \"Outstanding dividend yield and consistent earnings growth attract investors.\",\n",
    "                \"Record-breaking sales figures demonstrate robust business performance.\",\n",
    "                \"The acquisition strategy delivers impressive synergies and cost savings.\",\n",
    "                \n",
    "                # Negative examples (label 0)\n",
    "                \"Poor quarterly results and disappointing earnings guidance concern investors.\",\n",
    "                \"High debt levels and deteriorating credit ratings raise serious concerns.\",\n",
    "                \"The company struggles with intense competitive pressure and pricing wars.\",\n",
    "                \"Declining profit margins and rising operational costs pressure performance.\",\n",
    "                \n",
    "                # Neutral examples (label 1)\n",
    "                \"The company announced routine quarterly earnings in line with expectations.\",\n",
    "                \"Management provides standard guidance for upcoming fiscal year planning.\",\n",
    "                \"The industry shows typical seasonal patterns in demand and supply dynamics.\",\n",
    "                \"The company files quarterly reports with standard financial disclosure requirements.\"\n",
    "            ]\n",
    "            \n",
    "            validation_labels = [2, 2, 2, 2, 0, 0, 0, 0, 1, 1, 1, 1]\n",
    "            \n",
    "            return validation_texts, validation_labels\n",
    "    \n",
    "    def benchmark_pytorch_model(self, model_info: Dict, batch_size: int) -> BenchmarkResult:\n",
    "        \"\"\"Benchmark PyTorch model performance with comprehensive metrics and isolated latency measurement.\"\"\"\n",
    "        model_name = model_info['name']\n",
    "        model_path = model_info['path']\n",
    "        \n",
    "        # Measure baseline memory before loading model\n",
    "        baseline_memory = self.measure_memory_usage()\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(str(model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        # Move to appropriate device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Measure memory after loading model\n",
    "        memory_after_loading = self.measure_memory_usage()\n",
    "        memory_overhead = memory_after_loading - baseline_memory\n",
    "        \n",
    "        # Create sample inputs for benchmarking\n",
    "        input_ids, attention_mask = self.create_sample_batch(tokenizer, batch_size)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        \n",
    "        # === ENHANCED LATENCY MEASUREMENT ===\n",
    "        # Extended warmup to eliminate JIT effects and stabilize performance\n",
    "        self.logger.debug(f\"Starting extended warmup for {model_name}\")\n",
    "        with torch.no_grad():\n",
    "            # Initial warmup runs\n",
    "            for _ in range(WARMUP_ITERATIONS):\n",
    "                _ = model(input_ids, attention_mask)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "            \n",
    "            # Force garbage collection before measurement\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "            # Additional stabilization runs to eliminate any remaining JIT effects\n",
    "            for _ in range(5):\n",
    "                _ = model(input_ids, attention_mask)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "        \n",
    "        # Robust latency benchmark with outlier detection\n",
    "        self.logger.debug(f\"Starting robust latency measurement for {model_name}\")\n",
    "        latencies = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for iteration in range(ITERATIONS * 2):  # Collect more samples for outlier filtering\n",
    "                # Micro-GC every 5 iterations to prevent memory pressure spikes\n",
    "                if iteration % 5 == 0:\n",
    "                    gc.collect()\n",
    "                \n",
    "                # Clear any lingering operations\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "                \n",
    "                # High-precision timing with multiple sub-measurements\n",
    "                timings = []\n",
    "                for _ in range(3):  # Take 3 measurements per iteration\n",
    "                    start_time = time.perf_counter()\n",
    "                    outputs = model(input_ids, attention_mask)\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.synchronize()\n",
    "                    end_time = time.perf_counter()\n",
    "                    \n",
    "                    timings.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "                    del outputs\n",
    "                \n",
    "                # Use median of 3 timings to reduce noise\n",
    "                latencies.append(np.median(timings))\n",
    "        \n",
    "        # === PHASE 2: MEMORY TRACKING ===\n",
    "        # Separate memory measurement to avoid interfering with latency\n",
    "        self.logger.debug(f\"Starting memory measurement for {model_name}\")\n",
    "        memory_readings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(min(ITERATIONS // 2, 10)):  # Fewer iterations for memory tracking\n",
    "                # Clear cache and measure memory before iteration\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                memory_before = self.measure_memory_usage()\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                \n",
    "                memory_after = self.measure_memory_usage()\n",
    "                memory_readings.append(memory_after)\n",
    "                \n",
    "                del outputs\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        # Outlier removal using IQR method for more robust statistics\n",
    "        latencies = np.array(latencies)\n",
    "        Q1 = np.percentile(latencies, 25)\n",
    "        Q3 = np.percentile(latencies, 75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Filter outliers\n",
    "        filtered_latencies = latencies[(latencies >= lower_bound) & (latencies <= upper_bound)]\n",
    "        \n",
    "        # Log outlier information\n",
    "        outlier_count = len(latencies) - len(filtered_latencies)\n",
    "        if outlier_count > 0:\n",
    "            self.logger.debug(f\"Removed {outlier_count} outliers from {len(latencies)} measurements for {model_name}\")\n",
    "        \n",
    "        # Use filtered data for statistics (minimum 50% of samples must remain)\n",
    "        if len(filtered_latencies) >= len(latencies) * 0.5:\n",
    "            final_latencies = filtered_latencies\n",
    "            self.logger.debug(f\"Using {len(final_latencies)} filtered measurements for {model_name}\")\n",
    "        else:\n",
    "            final_latencies = latencies\n",
    "            self.logger.warning(f\"Too many outliers detected for {model_name}, using all {len(latencies)} measurements\")\n",
    "        \n",
    "        # Calculate comprehensive latency statistics from filtered data\n",
    "        final_latencies = np.array(final_latencies)\n",
    "        avg_latency = np.mean(final_latencies)\n",
    "        std_latency = np.std(final_latencies)\n",
    "        min_latency = np.min(final_latencies)\n",
    "        max_latency = np.max(final_latencies)\n",
    "        p50_latency = np.percentile(final_latencies, 50)\n",
    "        p95_latency = np.percentile(final_latencies, 95)\n",
    "        p99_latency = np.percentile(final_latencies, 99)\n",
    "        throughput = (batch_size * 1000) / avg_latency  # samples per second\n",
    "        \n",
    "        # Memory statistics\n",
    "        peak_memory = max(memory_readings) - baseline_memory if memory_readings else memory_overhead\n",
    "        avg_memory = (sum(memory_readings) / len(memory_readings)) - baseline_memory if memory_readings else memory_overhead\n",
    "        \n",
    "        # Get model size\n",
    "        model_size = self.get_model_size(model_path, 'pytorch')\n",
    "        \n",
    "        # === PHASE 3: ACCURACY EVALUATION (if enabled) ===\n",
    "        accuracy = None\n",
    "        f1_score_value = None\n",
    "        validation_sample_count = 0\n",
    "        \n",
    "        if INCLUDE_ACCURACY:\n",
    "            try:\n",
    "                self.logger.debug(f\"Starting accuracy evaluation for {model_name}\")\n",
    "                # Get comprehensive validation set from actual dataset\n",
    "                validation_texts, true_labels = self.get_validation_dataset()\n",
    "                \n",
    "                # Use subset if ACCURACY_SAMPLE_SIZE is smaller than full dataset\n",
    "                if len(validation_texts) > ACCURACY_SAMPLE_SIZE:\n",
    "                    # Take balanced sample from each class\n",
    "                    import random\n",
    "                    random.seed(42)  # For reproducible results\n",
    "                    \n",
    "                    # Create balanced sample by label using pandas\n",
    "                    df_val = pd.DataFrame({'text': validation_texts, 'label': true_labels})\n",
    "                    \n",
    "                    # Sample from each class\n",
    "                    samples_per_class = ACCURACY_SAMPLE_SIZE // 3\n",
    "                    sampled_data = []\n",
    "                    \n",
    "                    for label in [0, 1, 2]:  # negative, neutral, positive\n",
    "                        class_data = df_val[df_val['label'] == label]\n",
    "                        if len(class_data) > 0:\n",
    "                            sample_size = min(samples_per_class, len(class_data))\n",
    "                            sampled = class_data.sample(n=sample_size, random_state=42)\n",
    "                            sampled_data.append(sampled)\n",
    "                    \n",
    "                    # Combine samples\n",
    "                    if sampled_data:\n",
    "                        df_sampled = pd.concat(sampled_data, ignore_index=True)\n",
    "                        validation_texts = df_sampled['text'].tolist()\n",
    "                        true_labels = df_sampled['label'].tolist()\n",
    "                    else:\n",
    "                        # Fallback: just take first ACCURACY_SAMPLE_SIZE samples\n",
    "                        validation_texts = validation_texts[:ACCURACY_SAMPLE_SIZE]\n",
    "                        true_labels = true_labels[:ACCURACY_SAMPLE_SIZE]\n",
    "                \n",
    "                validation_sample_count = len(validation_texts)\n",
    "                \n",
    "                # Get predictions in batches to avoid memory issues\n",
    "                predicted_labels = []\n",
    "                batch_size_eval = 8  # Small batch size for evaluation\n",
    "                \n",
    "                for i in range(0, len(validation_texts), batch_size_eval):\n",
    "                    batch_texts = validation_texts[i:i+batch_size_eval]\n",
    "                    \n",
    "                    val_inputs = tokenizer(batch_texts, max_length=128, padding='max_length', \n",
    "                                         truncation=True, return_tensors='pt').to(device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        val_outputs = model(**val_inputs)\n",
    "                        predictions = torch.nn.functional.softmax(val_outputs.logits, dim=-1)\n",
    "                        batch_predicted = torch.argmax(predictions, dim=-1).cpu().numpy()\n",
    "                        predicted_labels.extend(batch_predicted)\n",
    "                    \n",
    "                    # Clean up batch\n",
    "                    del val_inputs, val_outputs, predictions\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                # Calculate metrics using imported functions\n",
    "                from sklearn.metrics import accuracy_score, f1_score\n",
    "                accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "                f1_score_value = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "                \n",
    "                self.logger.info(f\"Accuracy evaluation for {model_name}: {accuracy:.4f} accuracy, {f1_score_value:.4f} F1\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Accuracy evaluation failed for {model_name}: {str(e)}\")\n",
    "                accuracy = None\n",
    "                f1_score_value = None\n",
    "        \n",
    "        # === CLEANUP ===\n",
    "        del model, tokenizer, input_ids, attention_mask\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return BenchmarkResult(\n",
    "            model_name=model_name,\n",
    "            model_type='pytorch',\n",
    "            batch_size=batch_size,\n",
    "            avg_latency_ms=float(avg_latency),\n",
    "            p50_latency_ms=float(p50_latency),\n",
    "            p95_latency_ms=float(p95_latency),\n",
    "            p99_latency_ms=float(p99_latency),\n",
    "            throughput_samples_per_sec=float(throughput),\n",
    "            peak_memory_mb=float(max(0, peak_memory)),\n",
    "            model_size_mb=float(model_size),\n",
    "            provider='PyTorch',\n",
    "            accuracy=float(accuracy) if accuracy is not None else None,\n",
    "            f1_score=float(f1_score_value) if f1_score_value is not None else None,\n",
    "            std_latency_ms=float(std_latency),\n",
    "            min_latency_ms=float(min_latency),\n",
    "            max_latency_ms=float(max_latency),\n",
    "            avg_memory_mb=float(max(0, avg_memory)),\n",
    "            memory_overhead_mb=float(max(0, memory_overhead)),\n",
    "            validation_samples=validation_sample_count if INCLUDE_ACCURACY else None\n",
    "        )\n",
    "    \n",
    "    def benchmark_onnx_model(self, model_info: Dict, batch_size: int, provider: str = 'CPUExecutionProvider') -> BenchmarkResult:\n",
    "        \"\"\"Benchmark ONNX model performance with comprehensive metrics and isolated latency measurement.\"\"\"\n",
    "        model_name = model_info['name']\n",
    "        model_path = model_info['path']\n",
    "        \n",
    "        # Measure baseline memory\n",
    "        baseline_memory = self.measure_memory_usage()\n",
    "        \n",
    "        # Find ONNX model file\n",
    "        onnx_files = model_info['onnx_files']\n",
    "        if not onnx_files:\n",
    "            raise ValueError(f\"No ONNX files found for {model_name}\")\n",
    "        \n",
    "        onnx_model_path = onnx_files[0]  # Use first ONNX file\n",
    "        \n",
    "        # Load tokenizer and create ONNX session\n",
    "        tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n",
    "        session = ort.InferenceSession(str(onnx_model_path), providers=[provider])\n",
    "        \n",
    "        # Measure memory after loading\n",
    "        memory_after_loading = self.measure_memory_usage()\n",
    "        memory_overhead = memory_after_loading - baseline_memory\n",
    "        \n",
    "        # Create sample inputs for benchmarking\n",
    "        input_ids, attention_mask = self.create_sample_batch(tokenizer, batch_size)\n",
    "        \n",
    "        # Convert to numpy for ONNX\n",
    "        onnx_inputs = {\n",
    "            'input_ids': input_ids.numpy() if hasattr(input_ids, 'numpy') else input_ids,\n",
    "            'attention_mask': attention_mask.numpy() if hasattr(attention_mask, 'numpy') else attention_mask\n",
    "        }\n",
    "        \n",
    "        # === PHASE 1: ISOLATED LATENCY MEASUREMENT ===\n",
    "        # Warmup runs\n",
    "        self.logger.debug(f\"Starting warmup for {model_name} (ONNX)\")\n",
    "        for _ in range(WARMUP_ITERATIONS):\n",
    "            _ = session.run(None, onnx_inputs)\n",
    "        \n",
    "        # Pure latency benchmark - no memory tracking to avoid interference  \n",
    "        self.logger.debug(f\"Starting pure latency measurement for {model_name} (ONNX)\")\n",
    "        latencies = []\n",
    "        \n",
    "        for _ in range(ITERATIONS):\n",
    "            start_time = time.perf_counter()\n",
    "            outputs = session.run(None, onnx_inputs)\n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            latencies.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "            del outputs\n",
    "        \n",
    "        # === PHASE 2: MEMORY TRACKING ===\n",
    "        self.logger.debug(f\"Starting memory measurement for {model_name} (ONNX)\")\n",
    "        memory_readings = []\n",
    "        \n",
    "        for _ in range(min(ITERATIONS // 2, 10)):  # Fewer iterations for memory tracking\n",
    "            gc.collect()\n",
    "            memory_before = self.measure_memory_usage()\n",
    "            \n",
    "            outputs = session.run(None, onnx_inputs)\n",
    "            \n",
    "            memory_after = self.measure_memory_usage()\n",
    "            memory_readings.append(memory_after)\n",
    "            del outputs\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calculate comprehensive statistics\n",
    "        latencies = np.array(latencies)\n",
    "        avg_latency = np.mean(latencies)\n",
    "        std_latency = np.std(latencies)\n",
    "        min_latency = np.min(latencies)\n",
    "        max_latency = np.max(latencies)\n",
    "        p50_latency = np.percentile(latencies, 50)\n",
    "        p95_latency = np.percentile(latencies, 95)\n",
    "        p99_latency = np.percentile(latencies, 99)\n",
    "        throughput = (batch_size * 1000) / avg_latency  # samples per second\n",
    "        \n",
    "        # Memory statistics\n",
    "        peak_memory = max(memory_readings) - baseline_memory if memory_readings else memory_overhead\n",
    "        avg_memory = (sum(memory_readings) / len(memory_readings)) - baseline_memory if memory_readings else memory_overhead\n",
    "        \n",
    "        # Get model size\n",
    "        model_size = self.get_model_size(model_path, 'onnx')\n",
    "        \n",
    "        # === PHASE 3: ACCURACY EVALUATION (if enabled) ===\n",
    "        accuracy = None\n",
    "        f1_score_value = None\n",
    "        validation_sample_count = 0\n",
    "        \n",
    "        if INCLUDE_ACCURACY:\n",
    "            try:\n",
    "                self.logger.debug(f\"Starting accuracy evaluation for {model_name} (ONNX)\")\n",
    "                # Get comprehensive validation set from actual dataset\n",
    "                validation_texts, true_labels = self.get_validation_dataset()\n",
    "                \n",
    "                # Use subset if ACCURACY_SAMPLE_SIZE is smaller than full dataset\n",
    "                if len(validation_texts) > ACCURACY_SAMPLE_SIZE:\n",
    "                    # Take balanced sample from each class\n",
    "                    import random\n",
    "                    random.seed(42)  # For reproducible results\n",
    "                    \n",
    "                    # Create balanced sample by label using pandas\n",
    "                    df_val = pd.DataFrame({'text': validation_texts, 'label': true_labels})\n",
    "                    \n",
    "                    # Sample from each class\n",
    "                    samples_per_class = ACCURACY_SAMPLE_SIZE // 3\n",
    "                    sampled_data = []\n",
    "                    \n",
    "                    for label in [0, 1, 2]:  # negative, neutral, positive\n",
    "                        class_data = df_val[df_val['label'] == label]\n",
    "                        if len(class_data) > 0:\n",
    "                            sample_size = min(samples_per_class, len(class_data))\n",
    "                            sampled = class_data.sample(n=sample_size, random_state=42)\n",
    "                            sampled_data.append(sampled)\n",
    "                    \n",
    "                    # Combine samples\n",
    "                    if sampled_data:\n",
    "                        df_sampled = pd.concat(sampled_data, ignore_index=True)\n",
    "                        validation_texts = df_sampled['text'].tolist()\n",
    "                        true_labels = df_sampled['label'].tolist()\n",
    "                    else:\n",
    "                        # Fallback: just take first ACCURACY_SAMPLE_SIZE samples\n",
    "                        validation_texts = validation_texts[:ACCURACY_SAMPLE_SIZE]\n",
    "                        true_labels = true_labels[:ACCURACY_SAMPLE_SIZE]\n",
    "                \n",
    "                validation_sample_count = len(validation_texts)\n",
    "                \n",
    "                # Get predictions in batches\n",
    "                predicted_labels = []\n",
    "                batch_size_eval = 8\n",
    "                \n",
    "                for i in range(0, len(validation_texts), batch_size_eval):\n",
    "                    batch_texts = validation_texts[i:i+batch_size_eval]\n",
    "                    \n",
    "                    val_inputs = tokenizer(batch_texts, max_length=128, padding='max_length', \n",
    "                                         truncation=True, return_tensors='np')\n",
    "                    \n",
    "                    val_onnx_inputs = {\n",
    "                        'input_ids': val_inputs['input_ids'],\n",
    "                        'attention_mask': val_inputs['attention_mask']\n",
    "                    }\n",
    "                    \n",
    "                    val_outputs = session.run(None, val_onnx_inputs)\n",
    "                    predictions = val_outputs[0]  # logits\n",
    "                    batch_predicted = np.argmax(predictions, axis=-1)\n",
    "                    predicted_labels.extend(batch_predicted)\n",
    "                    \n",
    "                    # Clean up batch\n",
    "                    del val_inputs, val_onnx_inputs, val_outputs, predictions\n",
    "                \n",
    "                # Calculate metrics using imported functions\n",
    "                from sklearn.metrics import accuracy_score, f1_score\n",
    "                accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "                f1_score_value = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "                \n",
    "                self.logger.info(f\"Accuracy evaluation for {model_name} (ONNX): {accuracy:.4f} accuracy, {f1_score_value:.4f} F1\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Accuracy evaluation failed for {model_name} (ONNX): {str(e)}\")\n",
    "                accuracy = None\n",
    "                f1_score_value = None\n",
    "        \n",
    "        # === CLEANUP ===\n",
    "        del tokenizer, session\n",
    "        gc.collect()\n",
    "        \n",
    "        return BenchmarkResult(\n",
    "            model_name=model_name,\n",
    "            model_type='onnx',\n",
    "            batch_size=batch_size,\n",
    "            avg_latency_ms=float(avg_latency),\n",
    "            p50_latency_ms=float(p50_latency),\n",
    "            p95_latency_ms=float(p95_latency),\n",
    "            p99_latency_ms=float(p99_latency),\n",
    "            throughput_samples_per_sec=float(throughput),\n",
    "            peak_memory_mb=float(max(0, peak_memory)),\n",
    "            model_size_mb=float(model_size),\n",
    "            provider=provider,\n",
    "            accuracy=float(accuracy) if accuracy is not None else None,\n",
    "            f1_score=float(f1_score_value) if f1_score_value is not None else None,\n",
    "            std_latency_ms=float(std_latency),\n",
    "            min_latency_ms=float(min_latency),\n",
    "            max_latency_ms=float(max_latency),\n",
    "            avg_memory_mb=float(max(0, avg_memory)),\n",
    "            memory_overhead_mb=float(max(0, memory_overhead)),\n",
    "            validation_samples=validation_sample_count if INCLUDE_ACCURACY else None\n",
    "        )\n",
    "\n",
    "# Initialize benchmarker\n",
    "benchmarker = ModelBenchmarker(logger)\n",
    "print(f\"üîß Benchmarker initialized with providers: {benchmarker.execution_providers}\")\n",
    "\n",
    "logger.info(\"Benchmarking engine ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Benchmarking Execution\n",
    "logger.info(\"üöÄ Starting performance benchmarking...\")\n",
    "\n",
    "# Resource monitoring and optimization\n",
    "import psutil\n",
    "initial_memory = psutil.Process().memory_info().rss / 1024 / 1024\n",
    "logger.info(f\"   üß† Initial memory usage: {initial_memory:.1f} MB\")\n",
    "logger.info(f\"   üéØ Testing {len(available_models)} models with CPU threads limited to {MAX_CPU_THREADS}\")\n",
    "\n",
    "all_results = []\n",
    "benchmark_summary = {\n",
    "    'benchmark_timestamp': datetime.now().isoformat(),\n",
    "    'configuration': {\n",
    "        'fast_mode': FAST_MODE,\n",
    "        'iterations': ITERATIONS,\n",
    "        'warmup_iterations': WARMUP_ITERATIONS,\n",
    "        'batch_sizes': BATCH_SIZES,\n",
    "        'include_accuracy': INCLUDE_ACCURACY,\n",
    "        'accuracy_sample_size': ACCURACY_SAMPLE_SIZE if INCLUDE_ACCURACY else None\n",
    "    },\n",
    "    'system_info': {\n",
    "        'platform': platform.platform(),\n",
    "        'python_version': platform.python_version(),\n",
    "        'pytorch_version': torch.__version__ if 'torch' in globals() else None,\n",
    "        'onnxruntime_version': ort.__version__ if onnx_available else None,\n",
    "        'available_providers': benchmarker.execution_providers if onnx_available else []\n",
    "    },\n",
    "    'models_benchmarked': 0,\n",
    "    'total_benchmarks': 0\n",
    "}\n",
    "\n",
    "print(f\"\\n‚ö° Performance Benchmarking:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "total_benchmarks = 0\n",
    "for model_name, model_info in available_models.items():\n",
    "    for batch_size in BATCH_SIZES:\n",
    "        if model_info['has_pytorch']:\n",
    "            total_benchmarks += 1\n",
    "        if model_info['has_onnx']:\n",
    "            total_benchmarks += len(benchmarker.execution_providers)\n",
    "\n",
    "print(f\"üìä Total benchmarks to run: {total_benchmarks}\")\n",
    "print(f\"‚è±Ô∏è Estimated time: {total_benchmarks * ITERATIONS * 0.1:.1f} seconds\")\n",
    "\n",
    "current_benchmark = 0\n",
    "successful_benchmarks = 0\n",
    "\n",
    "for model_name, model_info in available_models.items():\n",
    "    print(f\"\\nü§ñ Benchmarking {model_name}:\")\n",
    "    print(f\"   üìÅ Path: {model_info['path']}\")\n",
    "    \n",
    "    model_results = []\n",
    "    \n",
    "    for batch_size in BATCH_SIZES:\n",
    "        print(f\"\\n   üìä Batch size: {batch_size}\")\n",
    "        \n",
    "        # Benchmark PyTorch model\n",
    "        if model_info['has_pytorch']:\n",
    "            current_benchmark += 1\n",
    "            print(f\"   üî• PyTorch [{current_benchmark}/{total_benchmarks}]\", end=\" ... \")\n",
    "            \n",
    "            try:\n",
    "                result = benchmarker.benchmark_pytorch_model(model_info, batch_size)\n",
    "                model_results.append(result)\n",
    "                all_results.append(result)\n",
    "                successful_benchmarks += 1\n",
    "                \n",
    "                print(f\"‚úÖ {result.avg_latency_ms:.2f}ms avg, {result.throughput_samples_per_sec:.1f} samples/sec\")\n",
    "                logger.info(f\"PyTorch benchmark completed for {model_name} (batch={batch_size})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed: {str(e)}\")\n",
    "                logger.error(f\"PyTorch benchmark failed for {model_name}: {str(e)}\")\n",
    "        \n",
    "        # Benchmark ONNX model with different providers\n",
    "        if model_info['has_onnx']:\n",
    "            for provider in benchmarker.execution_providers:\n",
    "                current_benchmark += 1\n",
    "                provider_name = provider.replace('ExecutionProvider', '')\n",
    "                print(f\"   ‚ö° ONNX-{provider_name} [{current_benchmark}/{total_benchmarks}]\", end=\" ... \")\n",
    "                \n",
    "                try:\n",
    "                    result = benchmarker.benchmark_onnx_model(model_info, batch_size, provider)\n",
    "                    model_results.append(result)\n",
    "                    all_results.append(result)\n",
    "                    successful_benchmarks += 1\n",
    "                    \n",
    "                    print(f\"‚úÖ {result.avg_latency_ms:.2f}ms avg, {result.throughput_samples_per_sec:.1f} samples/sec\")\n",
    "                    logger.info(f\"ONNX-{provider_name} benchmark completed for {model_name} (batch={batch_size})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Failed: {str(e)}\")\n",
    "                    logger.error(f\"ONNX-{provider_name} benchmark failed for {model_name}: {str(e)}\")\n",
    "    \n",
    "    # Display model summary\n",
    "    if model_results:\n",
    "        best_latency = min(r.avg_latency_ms for r in model_results)\n",
    "        best_throughput = max(r.throughput_samples_per_sec for r in model_results)\n",
    "        \n",
    "        best_latency_config = next(r for r in model_results if r.avg_latency_ms == best_latency)\n",
    "        best_throughput_config = next(r for r in model_results if r.throughput_samples_per_sec == best_throughput)\n",
    "        \n",
    "        print(f\"   üìà Best latency: {best_latency:.2f}ms ({best_latency_config.model_type}-{best_latency_config.provider}, batch={best_latency_config.batch_size})\")\n",
    "        print(f\"   üöÄ Best throughput: {best_throughput:.1f} samples/sec ({best_throughput_config.model_type}-{best_throughput_config.provider}, batch={best_throughput_config.batch_size})\")\n",
    "\n",
    "# Update summary\n",
    "benchmark_summary['models_benchmarked'] = len(available_models)\n",
    "benchmark_summary['total_benchmarks'] = total_benchmarks\n",
    "benchmark_summary['successful_benchmarks'] = successful_benchmarks\n",
    "benchmark_summary['failed_benchmarks'] = total_benchmarks - successful_benchmarks\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚ö° Performance Benchmarking Summary:\")\n",
    "print(f\"   ü§ñ Models benchmarked: {benchmark_summary['models_benchmarked']}\")\n",
    "print(f\"   ‚úÖ Successful benchmarks: {successful_benchmarks}\")\n",
    "print(f\"   ‚ùå Failed benchmarks: {total_benchmarks - successful_benchmarks}\")\n",
    "\n",
    "if successful_benchmarks == 0:\n",
    "    logger.error(\"No benchmarks completed successfully\")\n",
    "    raise RuntimeError(\"All benchmarks failed\")\n",
    "\n",
    "logger.info(f\"Performance benchmarking completed: {successful_benchmarks}/{total_benchmarks} successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Analysis and Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"\\nüìä Results Analysis:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Create DataFrame from results with comprehensive metrics\n",
    "results_data = []\n",
    "for result in all_results:\n",
    "    results_data.append({\n",
    "        'model_name': result.model_name,\n",
    "        'model_type': result.model_type,\n",
    "        'provider': result.provider,\n",
    "        'batch_size': result.batch_size,\n",
    "        'avg_latency_ms': result.avg_latency_ms,\n",
    "        'std_latency_ms': result.std_latency_ms,\n",
    "        'min_latency_ms': result.min_latency_ms,\n",
    "        'max_latency_ms': result.max_latency_ms,\n",
    "        'p50_latency_ms': result.p50_latency_ms,\n",
    "        'p95_latency_ms': result.p95_latency_ms,\n",
    "        'p99_latency_ms': result.p99_latency_ms,\n",
    "        'throughput_samples_per_sec': result.throughput_samples_per_sec,\n",
    "        'peak_memory_mb': result.peak_memory_mb,\n",
    "        'avg_memory_mb': result.avg_memory_mb,\n",
    "        'memory_overhead_mb': result.memory_overhead_mb,\n",
    "        'model_size_mb': result.model_size_mb,\n",
    "        'accuracy': result.accuracy,\n",
    "        'f1_score': result.f1_score,\n",
    "        'validation_samples': result.validation_samples,\n",
    "        # Calculated efficiency metrics\n",
    "        'efficiency_score': result.throughput_samples_per_sec / result.model_size_mb if result.model_size_mb > 0 else None,\n",
    "        'latency_stability': result.std_latency_ms / result.avg_latency_ms if result.avg_latency_ms > 0 and result.std_latency_ms else None,\n",
    "        'memory_efficiency': result.throughput_samples_per_sec / result.peak_memory_mb if result.peak_memory_mb > 0 else None\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "print(f\"üìã Created results DataFrame with {len(df_results)} entries\")\n",
    "\n",
    "# Display best performers with enhanced analysis\n",
    "print(f\"\\nüèÜ Best Performers:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Best latency\n",
    "best_latency_row = df_results.loc[df_results['avg_latency_ms'].idxmin()]\n",
    "print(f\"‚ö° Lowest Latency: {best_latency_row['avg_latency_ms']:.2f}ms\")\n",
    "print(f\"   üìä Model: {best_latency_row['model_name']}\")\n",
    "print(f\"   üîß Config: {best_latency_row['model_type']}-{best_latency_row['provider']}\")\n",
    "print(f\"   üì¶ Batch: {best_latency_row['batch_size']}\")\n",
    "print(f\"   üöÄ Throughput: {best_latency_row['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "print(f\"   üíæ Memory: {best_latency_row['peak_memory_mb']:.1f}MB\")\n",
    "print(f\"   üìè Model Size: {best_latency_row['model_size_mb']:.1f}MB\")\n",
    "\n",
    "# Best throughput\n",
    "best_throughput_row = df_results.loc[df_results['throughput_samples_per_sec'].idxmax()]\n",
    "print(f\"\\nüöÄ Highest Throughput: {best_throughput_row['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "print(f\"   üìä Model: {best_throughput_row['model_name']}\")\n",
    "print(f\"   üîß Config: {best_throughput_row['model_type']}-{best_throughput_row['provider']}\")\n",
    "print(f\"   üì¶ Batch: {best_throughput_row['batch_size']}\")\n",
    "print(f\"   ‚ö° Latency: {best_throughput_row['avg_latency_ms']:.2f}ms\")\n",
    "print(f\"   üíæ Memory: {best_throughput_row['peak_memory_mb']:.1f}MB\")\n",
    "print(f\"   üìè Model Size: {best_throughput_row['model_size_mb']:.1f}MB\")\n",
    "\n",
    "# Best efficiency (throughput per MB of model)\n",
    "df_results['efficiency'] = df_results['throughput_samples_per_sec'] / df_results['model_size_mb']\n",
    "best_efficiency_row = df_results.loc[df_results['efficiency'].idxmax()]\n",
    "print(f\"\\n‚öôÔ∏è Best Efficiency: {best_efficiency_row['efficiency']:.1f} samples/sec/MB\")\n",
    "print(f\"   üìä Model: {best_efficiency_row['model_name']}\")\n",
    "print(f\"   üîß Config: {best_efficiency_row['model_type']}-{best_efficiency_row['provider']}\")\n",
    "print(f\"   üì¶ Batch: {best_efficiency_row['batch_size']}\")\n",
    "print(f\"   üöÄ Throughput: {best_efficiency_row['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "print(f\"   üìè Model Size: {best_efficiency_row['model_size_mb']:.1f}MB\")\n",
    "\n",
    "# Additional best performer metrics\n",
    "# Best accuracy (if available)\n",
    "if df_results['accuracy'].notna().any():\n",
    "    best_accuracy_row = df_results.loc[df_results['accuracy'].idxmax()]\n",
    "    print(f\"\\nüéØ Highest Accuracy: {best_accuracy_row['accuracy']:.4f}\")\n",
    "    print(f\"   üìä Model: {best_accuracy_row['model_name']}\")\n",
    "    print(f\"   üîß Config: {best_accuracy_row['model_type']}-{best_accuracy_row['provider']}\")\n",
    "    print(f\"   üì¶ Batch: {best_accuracy_row['batch_size']}\")\n",
    "    print(f\"   ‚ö° Latency: {best_accuracy_row['avg_latency_ms']:.2f}ms\")\n",
    "    if best_accuracy_row['f1_score']:\n",
    "        print(f\"   üéØ F1-Score: {best_accuracy_row['f1_score']:.4f}\")\n",
    "\n",
    "# Best stability (lowest coefficient of variation)\n",
    "if df_results['latency_stability'].notna().any():\n",
    "    best_stability_row = df_results.loc[df_results['latency_stability'].idxmin()]\n",
    "    print(f\"\\nüìä Most Stable Latency: {best_stability_row['latency_stability']:.4f} CV\")\n",
    "    print(f\"   üìä Model: {best_stability_row['model_name']}\")\n",
    "    print(f\"   üîß Config: {best_stability_row['model_type']}-{best_stability_row['provider']}\")\n",
    "    print(f\"   üì¶ Batch: {best_stability_row['batch_size']}\")\n",
    "    print(f\"   ‚ö° Latency: {best_stability_row['avg_latency_ms']:.2f}ms ¬±{best_stability_row['std_latency_ms']:.2f}ms\")\n",
    "\n",
    "# Best memory efficiency\n",
    "if df_results['memory_efficiency'].notna().any():\n",
    "    best_mem_eff_row = df_results.loc[df_results['memory_efficiency'].idxmax()]\n",
    "    print(f\"\\nüíæ Best Memory Efficiency: {best_mem_eff_row['memory_efficiency']:.1f} samples/sec/MB\")\n",
    "    print(f\"   üìä Model: {best_mem_eff_row['model_name']}\")\n",
    "    print(f\"   üîß Config: {best_mem_eff_row['model_type']}-{best_mem_eff_row['provider']}\")\n",
    "    print(f\"   üì¶ Batch: {best_mem_eff_row['batch_size']}\")\n",
    "    print(f\"   üíæ Memory: {best_mem_eff_row['peak_memory_mb']:.1f}MB\")\n",
    "\n",
    "# Best accuracy (if available)\n",
    "if df_results['accuracy'].notna().any():\n",
    "    best_accuracy_row = df_results.loc[df_results['accuracy'].idxmax()]\n",
    "    print(f\"\\nüéØ Highest Accuracy: {best_accuracy_row['accuracy']:.4f}\")\n",
    "    print(f\"   Model: {best_accuracy_row['model_name']} ({best_accuracy_row['model_type']}-{best_accuracy_row['provider']})\")\n",
    "    print(f\"   Batch: {best_accuracy_row['batch_size']}, Latency: {best_accuracy_row['avg_latency_ms']:.2f}ms\")\n",
    "\n",
    "# Performance by model type with enhanced metrics\n",
    "print(f\"\\nüìä Performance by Model Type:\")\n",
    "print(f\"{'='*60}\")\n",
    "type_summary = df_results.groupby('model_type').agg({\n",
    "    'avg_latency_ms': ['mean', 'min', 'max', 'std'],\n",
    "    'throughput_samples_per_sec': ['mean', 'min', 'max', 'std'],\n",
    "    'peak_memory_mb': ['mean', 'min', 'max'],\n",
    "    'model_size_mb': ['mean', 'min', 'max'],\n",
    "    'accuracy': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "for model_type in type_summary.index:\n",
    "    print(f\"\\nüî• {model_type.upper()}:\")\n",
    "    print(f\"   ‚ö° Latency: {type_summary.loc[model_type, ('avg_latency_ms', 'mean')]:.2f}ms ¬±{type_summary.loc[model_type, ('avg_latency_ms', 'std')]:.2f}ms\")\n",
    "    print(f\"      Range: {type_summary.loc[model_type, ('avg_latency_ms', 'min')]:.2f}ms - {type_summary.loc[model_type, ('avg_latency_ms', 'max')]:.2f}ms\")\n",
    "    print(f\"   üöÄ Throughput: {type_summary.loc[model_type, ('throughput_samples_per_sec', 'mean')]:.1f} ¬±{type_summary.loc[model_type, ('throughput_samples_per_sec', 'std')]:.1f} samples/sec\")\n",
    "    print(f\"      Range: {type_summary.loc[model_type, ('throughput_samples_per_sec', 'min')]:.1f} - {type_summary.loc[model_type, ('throughput_samples_per_sec', 'max')]:.1f} samples/sec\")\n",
    "    print(f\"   üíæ Memory: {type_summary.loc[model_type, ('peak_memory_mb', 'mean')]:.1f}MB avg ({type_summary.loc[model_type, ('peak_memory_mb', 'min')]:.1f}-{type_summary.loc[model_type, ('peak_memory_mb', 'max')]:.1f}MB)\")\n",
    "    print(f\"   üìè Model Size: {type_summary.loc[model_type, ('model_size_mb', 'mean')]:.1f}MB avg ({type_summary.loc[model_type, ('model_size_mb', 'min')]:.1f}-{type_summary.loc[model_type, ('model_size_mb', 'max')]:.1f}MB)\")\n",
    "    if not pd.isna(type_summary.loc[model_type, ('accuracy', 'mean')]):\n",
    "        print(f\"   üéØ Accuracy: {type_summary.loc[model_type, ('accuracy', 'mean')]:.4f}\")\n",
    "\n",
    "# Model-specific analysis\n",
    "print(f\"\\nü§ñ Per-Model Analysis:\")\n",
    "print(f\"{'='*60}\")\n",
    "model_summary = df_results.groupby('model_name').agg({\n",
    "    'avg_latency_ms': 'min',\n",
    "    'throughput_samples_per_sec': 'max',\n",
    "    'peak_memory_mb': 'mean',\n",
    "    'model_size_mb': 'first'\n",
    "}).round(2)\n",
    "\n",
    "for model_name in model_summary.index:\n",
    "    print(f\"\\nüìä {model_name}:\")\n",
    "    print(f\"   ‚ö° Best Latency: {model_summary.loc[model_name, 'avg_latency_ms']:.2f}ms\")\n",
    "    print(f\"   üöÄ Best Throughput: {model_summary.loc[model_name, 'throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "    print(f\"   üíæ Avg Memory: {model_summary.loc[model_name, 'peak_memory_mb']:.1f}MB\")\n",
    "    print(f\"   üìè Model Size: {model_summary.loc[model_name, 'model_size_mb']:.1f}MB\")\n",
    "    \n",
    "    # Find best configuration for this model\n",
    "    model_data = df_results[df_results['model_name'] == model_name]\n",
    "    best_config = model_data.loc[model_data['avg_latency_ms'].idxmin()]\n",
    "    print(f\"   üîß Best Config: {best_config['model_type']}-{best_config['provider']} (batch={best_config['batch_size']})\")\n",
    "\n",
    "# Create visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Performance Benchmarking Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Latency comparison\n",
    "ax1 = axes[0, 0]\n",
    "df_pivot_latency = df_results.pivot_table(values='avg_latency_ms', index='model_name', \n",
    "                                          columns=['model_type', 'provider'], aggfunc='mean')\n",
    "df_pivot_latency.plot(kind='bar', ax=ax1, rot=45)\n",
    "ax1.set_title('Average Latency by Model')\n",
    "ax1.set_ylabel('Latency (ms)')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 2. Throughput comparison  \n",
    "ax2 = axes[0, 1]\n",
    "df_pivot_throughput = df_results.pivot_table(values='throughput_samples_per_sec', index='model_name',\n",
    "                                             columns=['model_type', 'provider'], aggfunc='mean')\n",
    "df_pivot_throughput.plot(kind='bar', ax=ax2, rot=45)\n",
    "ax2.set_title('Throughput by Model')\n",
    "ax2.set_ylabel('Samples/sec')\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 3. Batch size impact on latency\n",
    "ax3 = axes[1, 0]\n",
    "for model_type in df_results['model_type'].unique():\n",
    "    subset = df_results[df_results['model_type'] == model_type]\n",
    "    batch_latency = subset.groupby('batch_size')['avg_latency_ms'].mean()\n",
    "    ax3.plot(batch_latency.index, batch_latency.values, marker='o', label=model_type)\n",
    "ax3.set_title('Batch Size Impact on Latency')\n",
    "ax3.set_xlabel('Batch Size')\n",
    "ax3.set_ylabel('Average Latency (ms)')\n",
    "ax3.legend()\n",
    "ax3.set_xscale('log')\n",
    "\n",
    "# 4. Memory usage vs performance\n",
    "ax4 = axes[1, 1]\n",
    "scatter = ax4.scatter(df_results['peak_memory_mb'], df_results['avg_latency_ms'], \n",
    "                      c=df_results['throughput_samples_per_sec'], cmap='viridis', alpha=0.7)\n",
    "ax4.set_title('Memory vs Latency (colored by throughput)')\n",
    "ax4.set_xlabel('Peak Memory Usage (MB)')\n",
    "ax4.set_ylabel('Average Latency (ms)')\n",
    "plt.colorbar(scatter, ax=ax4, label='Throughput (samples/sec)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Results visualization completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"\\nüíæ Saving Results:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path(\"../results\")  # Fixed: use Path directly instead of undefined base_path\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate filename with timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "base_filename = f\"benchmark_results_generalized_{timestamp}\"\n",
    "\n",
    "# 1. Save CSV results\n",
    "csv_path = results_dir / f\"{base_filename}.csv\"\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"üìä CSV results saved: {csv_path}\")\n",
    "\n",
    "# 2. Save detailed JSON results\n",
    "json_results = {\n",
    "    'benchmark_summary': benchmark_summary,\n",
    "    'detailed_results': [result.__dict__ for result in all_results],\n",
    "    'performance_summary': {\n",
    "        'best_latency': {\n",
    "            'value': float(best_latency_row['avg_latency_ms']),\n",
    "            'model': best_latency_row['model_name'],\n",
    "            'config': f\"{best_latency_row['model_type']}-{best_latency_row['provider']}\",\n",
    "            'batch_size': int(best_latency_row['batch_size'])\n",
    "        },\n",
    "        'best_throughput': {\n",
    "            'value': float(best_throughput_row['throughput_samples_per_sec']),\n",
    "            'model': best_throughput_row['model_name'],\n",
    "            'config': f\"{best_throughput_row['model_type']}-{best_throughput_row['provider']}\",\n",
    "            'batch_size': int(best_throughput_row['batch_size'])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add accuracy summary if available\n",
    "if df_results['accuracy'].notna().any():\n",
    "    json_results['performance_summary']['best_accuracy'] = {\n",
    "        'value': float(best_accuracy_row['accuracy']),\n",
    "        'model': best_accuracy_row['model_name'],\n",
    "        'config': f\"{best_accuracy_row['model_type']}-{best_accuracy_row['provider']}\",\n",
    "        'batch_size': int(best_accuracy_row['batch_size'])\n",
    "    }\n",
    "\n",
    "json_path = results_dir / f\"{base_filename}.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(json_results, f, indent=2, default=str)\n",
    "print(f\"üìã JSON results saved: {json_path}\")\n",
    "\n",
    "# 3. Save performance summary report\n",
    "report_path = results_dir / f\"{base_filename}_summary.txt\"\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"Performance Benchmarking Summary Report\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Configuration: {json_results['benchmark_summary']['configuration']}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Best Performers:\\n\")\n",
    "    f.write(\"-\" * 20 + \"\\n\")\n",
    "    f.write(f\"Lowest Latency: {best_latency_row['avg_latency_ms']:.2f}ms\\n\")\n",
    "    f.write(f\"  Model: {best_latency_row['model_name']} ({best_latency_row['model_type']}-{best_latency_row['provider']})\\n\")\n",
    "    f.write(f\"  Batch: {best_latency_row['batch_size']}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Highest Throughput: {best_throughput_row['throughput_samples_per_sec']:.1f} samples/sec\\n\")\n",
    "    f.write(f\"  Model: {best_throughput_row['model_name']} ({best_throughput_row['model_type']}-{best_throughput_row['provider']})\\n\")\n",
    "    f.write(f\"  Batch: {best_throughput_row['batch_size']}\\n\\n\")\n",
    "    \n",
    "    if df_results['accuracy'].notna().any():\n",
    "        f.write(f\"Highest Accuracy: {best_accuracy_row['accuracy']:.4f}\\n\")\n",
    "        f.write(f\"  Model: {best_accuracy_row['model_name']} ({best_accuracy_row['model_type']}-{best_accuracy_row['provider']})\\n\")\n",
    "        f.write(f\"  Batch: {best_accuracy_row['batch_size']}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Performance by Model Type:\\n\")\n",
    "    f.write(\"-\" * 30 + \"\\n\")\n",
    "    for model_type in type_summary.index:\n",
    "        f.write(f\"{model_type.upper()}:\\n\")\n",
    "        f.write(f\"  Avg Latency: {type_summary.loc[model_type, ('avg_latency_ms', 'mean')]:.2f}ms\\n\")\n",
    "        f.write(f\"  Avg Throughput: {type_summary.loc[model_type, ('throughput_samples_per_sec', 'mean')]:.1f} samples/sec\\n\")\n",
    "        if not pd.isna(type_summary.loc[model_type, ('accuracy', 'mean')]):\n",
    "            f.write(f\"  Avg Accuracy: {type_summary.loc[model_type, ('accuracy', 'mean')]:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"üìÑ Summary report saved: {report_path}\")\n",
    "\n",
    "# 4. Save plot\n",
    "plot_path = results_dir / f\"{base_filename}_plots.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"üìà Plots saved: {plot_path}\")\n",
    "\n",
    "# 5. Update state manager with latest results\n",
    "try:\n",
    "    state.save_state('benchmark_results', {\n",
    "        'latest_run': {\n",
    "            'timestamp': timestamp,\n",
    "            'csv_path': str(csv_path),\n",
    "            'json_path': str(json_path),\n",
    "            'report_path': str(report_path),\n",
    "            'plot_path': str(plot_path)\n",
    "        },\n",
    "        'summary': json_results['performance_summary'],\n",
    "        'models_benchmarked': list(available_models.keys()),\n",
    "        'successful_benchmarks': successful_benchmarks,\n",
    "        'total_benchmarks': total_benchmarks\n",
    "    })\n",
    "    print(f\"üíæ State updated with benchmark results\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Failed to update state: {e}\")\n",
    "\n",
    "# Display completion summary\n",
    "print(f\"\\nüéâ Benchmarking Complete!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"üìä Results available in: {results_dir}\")\n",
    "print(f\"üìà CSV: {csv_path.name}\")\n",
    "print(f\"üìã JSON: {json_path.name}\")\n",
    "print(f\"üìÑ Report: {report_path.name}\")\n",
    "print(f\"üñºÔ∏è Plots: {plot_path.name}\")\n",
    "\n",
    "logger.info(f\"Comprehensive benchmarking completed successfully. Results saved to {results_dir}\")\n",
    "print(f\"\\n‚úÖ All benchmarking tasks completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
