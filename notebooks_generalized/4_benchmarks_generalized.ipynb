{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Performance Benchmarking - Generalized\n",
    "\n",
    "This notebook provides comprehensive benchmarking of trained models.\n",
    "\n",
    "**Configuration-driven approach:** All settings loaded from `../config/pipeline_config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration system and benchmarking utilities\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.pipeline_utils import ConfigManager, StateManager, LoggingManager\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import gc\n",
    "import platform\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ONNX and optimization imports\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "    onnx_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è ONNXRuntime not installed. Install with: pip install onnxruntime\")\n",
    "    onnx_available = False\n",
    "\n",
    "# Hugging Face transformers\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import pickle\n",
    "    transformers_available = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Transformers not installed\")\n",
    "    transformers_available = False\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize managers\n",
    "config = ConfigManager(\"../config/pipeline_config.json\")\n",
    "state = StateManager(\"../config/pipeline_state.json\")\n",
    "logger_manager = LoggingManager(config, 'benchmarking')\n",
    "logger = logger_manager.get_logger()\n",
    "\n",
    "logger.info(\"üìä Starting Performance Benchmarking - Generalized Pipeline\")\n",
    "print(\"üìã Configuration loaded from ../config/pipeline_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking Data Structures and Configuration\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Structured class to hold results from a single benchmark run.\"\"\"\n",
    "    model_name: str\n",
    "    model_type: str  # 'pytorch' or 'onnx'\n",
    "    batch_size: int\n",
    "    avg_latency_ms: float\n",
    "    p50_latency_ms: float\n",
    "    p95_latency_ms: float\n",
    "    p99_latency_ms: float\n",
    "    throughput_samples_per_sec: float\n",
    "    peak_memory_mb: float\n",
    "    model_size_mb: float\n",
    "    provider: str\n",
    "    accuracy: Optional[float] = None\n",
    "    f1_score: Optional[float] = None\n",
    "    weighted_accuracy: Optional[float] = None\n",
    "    weighted_f1_score: Optional[float] = None\n",
    "    avg_confidence_correct: Optional[float] = None\n",
    "    avg_confidence_incorrect: Optional[float] = None\n",
    "    per_class_metrics: Optional[Dict] = None\n",
    "    validation_samples: Optional[int] = None\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary with flattened per-class metrics.\"\"\"\n",
    "        flat_dict = asdict(self)\n",
    "        per_class = flat_dict.pop(\"per_class_metrics\", {})\n",
    "        if per_class:\n",
    "            for class_name, metrics in per_class.items():\n",
    "                for metric_name, value in metrics.items():\n",
    "                    flat_dict[f\"{class_name}_{metric_name}\"] = value\n",
    "        return flat_dict\n",
    "\n",
    "class ExecutionProviderManager:\n",
    "    \"\"\"Manages ONNX execution providers for consistent benchmarking.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_execution_providers() -> List[str]:\n",
    "        \"\"\"Get available execution providers for benchmarking.\"\"\"\n",
    "        available_providers = ort.get_available_providers() if onnx_available else []\n",
    "        \n",
    "        # Prioritize providers for consistent benchmarking\n",
    "        preferred_providers = ['CPUExecutionProvider']  # Start with CPU for consistency\n",
    "        \n",
    "        # Add platform-specific optimizations if available\n",
    "        if platform.system() == \"Darwin\" and \"CoreMLExecutionProvider\" in available_providers:\n",
    "            preferred_providers.insert(0, 'CoreMLExecutionProvider')\n",
    "        elif \"CUDAExecutionProvider\" in available_providers:\n",
    "            preferred_providers.insert(0, 'CUDAExecutionProvider')\n",
    "        \n",
    "        return [p for p in preferred_providers if p in available_providers]\n",
    "\n",
    "# Load benchmarking configuration\n",
    "benchmark_config = config.get('benchmarking', {})\n",
    "models_config = config.get('models', {})\n",
    "data_config = config.get('data', {})\n",
    "\n",
    "print(\"üìä Benchmarking Configuration:\")\n",
    "print(f\"   ‚ö° Fast mode: {benchmark_config.get('fast_mode', True)}\")\n",
    "print(f\"   üéØ Include accuracy: {benchmark_config.get('include_accuracy', True)}\")\n",
    "print(f\"   üîÑ Target models: {benchmark_config.get('target_models') or 'All available'}\")\n",
    "\n",
    "# Set benchmark parameters based on configuration\n",
    "FAST_MODE = benchmark_config.get('fast_mode', True)\n",
    "INCLUDE_ACCURACY = benchmark_config.get('include_accuracy', True)\n",
    "TARGET_MODELS = benchmark_config.get('target_models')  # None means all models\n",
    "\n",
    "# Performance parameters\n",
    "if FAST_MODE:\n",
    "    ITERATIONS = benchmark_config.get('iterations', {}).get('fast', 15)\n",
    "    WARMUP_ITERATIONS = benchmark_config.get('warmup_iterations', {}).get('fast', 5)\n",
    "    BATCH_SIZES = benchmark_config.get('batch_sizes', {}).get('fast', [1, 8])\n",
    "    ACCURACY_SAMPLE_SIZE = benchmark_config.get('accuracy_sample_size', {}).get('fast', 300)\n",
    "else:\n",
    "    ITERATIONS = benchmark_config.get('iterations', {}).get('thorough', 30)\n",
    "    WARMUP_ITERATIONS = benchmark_config.get('warmup_iterations', {}).get('thorough', 10)\n",
    "    BATCH_SIZES = benchmark_config.get('batch_sizes', {}).get('thorough', [1, 4, 8, 16])\n",
    "    ACCURACY_SAMPLE_SIZE = benchmark_config.get('accuracy_sample_size', {}).get('thorough', 500)\n",
    "\n",
    "print(f\"   üîÑ Iterations: {ITERATIONS} (warmup: {WARMUP_ITERATIONS})\")\n",
    "print(f\"   üìä Batch sizes: {BATCH_SIZES}\")\n",
    "print(f\"   üéØ Accuracy samples: {ACCURACY_SAMPLE_SIZE if INCLUDE_ACCURACY else 'Disabled'}\")\n",
    "\n",
    "logger.info(\"Benchmarking configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Discovery and Loading\n",
    "logger.info(\"üîç Discovering available models...\")\n",
    "\n",
    "# Verify prerequisites\n",
    "if not onnx_available or not transformers_available:\n",
    "    raise ImportError(\"Required libraries missing. Install with: pip install onnxruntime transformers\")\n",
    "\n",
    "# Check if ONNX conversion was completed\n",
    "if not state.is_step_complete('onnx_conversion_completed'):\n",
    "    print(\"‚ö†Ô∏è ONNX conversion not completed. ONNX benchmarks will be skipped.\")\n",
    "    onnx_conversion_completed = False\n",
    "else:\n",
    "    onnx_conversion_completed = True\n",
    "\n",
    "models_dir = Path(f\"../{models_config.get('output_dir', 'models')}\")\n",
    "print(f\"üìÇ Models directory: {models_dir}\")\n",
    "\n",
    "# Discover available models\n",
    "available_models = {}\n",
    "\n",
    "if models_dir.exists():\n",
    "    for model_path in models_dir.iterdir():\n",
    "        if not model_path.is_dir() or model_path.name.startswith('.'):\n",
    "            continue\n",
    "            \n",
    "        model_name = model_path.name\n",
    "        \n",
    "        # Check if model should be included based on TARGET_MODELS\n",
    "        if TARGET_MODELS and model_name not in TARGET_MODELS:\n",
    "            print(f\"   ‚è≠Ô∏è Skipping {model_name} (not in target list)\")\n",
    "            continue\n",
    "        \n",
    "        # Check for PyTorch model files\n",
    "        config_file = model_path / \"config.json\"\n",
    "        pytorch_files = list(model_path.glob(\"*.safetensors\")) + list(model_path.glob(\"pytorch_model.bin\"))\n",
    "        \n",
    "        # Check for ONNX model files\n",
    "        onnx_dir = model_path / \"onnx\"\n",
    "        onnx_files = list(onnx_dir.glob(\"*.onnx\")) if onnx_dir.exists() else []\n",
    "        \n",
    "        if config_file.exists() and (pytorch_files or onnx_files):\n",
    "            model_info = {\n",
    "                'name': model_name,\n",
    "                'path': model_path,\n",
    "                'config_file': config_file,\n",
    "                'has_pytorch': len(pytorch_files) > 0,\n",
    "                'has_onnx': len(onnx_files) > 0,\n",
    "                'pytorch_files': pytorch_files,\n",
    "                'onnx_files': onnx_files\n",
    "            }\n",
    "            available_models[model_name] = model_info\n",
    "            \n",
    "            status = []\n",
    "            if model_info['has_pytorch']:\n",
    "                status.append(\"PyTorch\")\n",
    "            if model_info['has_onnx']:\n",
    "                status.append(\"ONNX\")\n",
    "            \n",
    "            print(f\"   ‚úÖ Found: {model_name} ({', '.join(status)})\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Invalid model directory: {model_name}\")\n",
    "\n",
    "print(f\"\\nüìä Discovery Summary:\")\n",
    "print(f\"   ü§ñ Total models found: {len(available_models)}\")\n",
    "pytorch_count = sum(1 for m in available_models.values() if m['has_pytorch'])\n",
    "onnx_count = sum(1 for m in available_models.values() if m['has_onnx'])\n",
    "print(f\"   üî• PyTorch models: {pytorch_count}\")\n",
    "print(f\"   ‚ö° ONNX models: {onnx_count}\")\n",
    "\n",
    "if len(available_models) == 0:\n",
    "    logger.error(\"No models found for benchmarking\")\n",
    "    raise RuntimeError(\"No models found. Please run model training and/or ONNX conversion first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Benchmarking Engine\n",
    "class ModelBenchmarker:\n",
    "    \"\"\"Comprehensive model benchmarking with standardized measurement.\"\"\"\n",
    "    \n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.execution_providers = ExecutionProviderManager.get_execution_providers()\n",
    "    \n",
    "    def measure_memory_usage(self) -> float:\n",
    "        \"\"\"Measure current memory usage in MB.\"\"\"\n",
    "        import psutil\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / 1024 / 1024  # Convert to MB\n",
    "    \n",
    "    def get_model_size(self, model_path: Path, model_type: str) -> float:\n",
    "        \"\"\"Get model file size in MB.\"\"\"\n",
    "        if model_type == 'pytorch':\n",
    "            model_files = list(model_path.glob(\"*.safetensors\")) + list(model_path.glob(\"pytorch_model.bin\"))\n",
    "            return sum(f.stat().st_size for f in model_files) / (1024 * 1024)\n",
    "        elif model_type == 'onnx':\n",
    "            onnx_files = list((model_path / \"onnx\").glob(\"*.onnx\"))\n",
    "            return sum(f.stat().st_size for f in onnx_files) / (1024 * 1024)\n",
    "        return 0.0\n",
    "    \n",
    "    def create_sample_batch(self, tokenizer, batch_size: int, max_length: int = 128) -> Tuple[Any, Any]:\n",
    "        \"\"\"Create a sample batch for benchmarking.\"\"\"\n",
    "        # Use diverse sample texts for more realistic benchmarking\n",
    "        sample_texts = [\n",
    "            \"The company's quarterly earnings exceeded expectations by 15%.\",\n",
    "            \"Market volatility has increased due to geopolitical tensions.\",\n",
    "            \"The Federal Reserve announced a rate hike of 0.25 basis points.\",\n",
    "            \"Tech stocks declined following regulatory concerns.\",\n",
    "            \"Oil prices surged on supply chain disruptions.\"\n",
    "        ] * (batch_size // 5 + 1)  # Repeat to fill batch\n",
    "        \n",
    "        sample_texts = sample_texts[:batch_size]\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        inputs = tokenizer(\n",
    "            sample_texts,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt' if hasattr(tokenizer, 'model_max_length') else None\n",
    "        )\n",
    "        \n",
    "        return inputs['input_ids'], inputs['attention_mask']\n",
    "    \n",
    "    def benchmark_pytorch_model(self, model_info: Dict, batch_size: int) -> BenchmarkResult:\n",
    "        \"\"\"Benchmark PyTorch model performance.\"\"\"\n",
    "        model_name = model_info['name']\n",
    "        model_path = model_info['path']\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(str(model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        # Move to appropriate device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Create sample inputs\n",
    "        input_ids, attention_mask = self.create_sample_batch(tokenizer, batch_size)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        \n",
    "        # Warmup runs\n",
    "        with torch.no_grad():\n",
    "            for _ in range(WARMUP_ITERATIONS):\n",
    "                _ = model(input_ids, attention_mask)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark runs\n",
    "        latencies = []\n",
    "        memory_before = self.measure_memory_usage()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(ITERATIONS):\n",
    "                start_time = time.perf_counter()\n",
    "                _ = model(input_ids, attention_mask)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "                end_time = time.perf_counter()\n",
    "                latencies.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "        \n",
    "        memory_after = self.measure_memory_usage()\n",
    "        peak_memory = memory_after - memory_before\n",
    "        \n",
    "        # Calculate statistics\n",
    "        latencies = np.array(latencies)\n",
    "        avg_latency = np.mean(latencies)\n",
    "        p50_latency = np.percentile(latencies, 50)\n",
    "        p95_latency = np.percentile(latencies, 95)\n",
    "        p99_latency = np.percentile(latencies, 99)\n",
    "        throughput = (batch_size * 1000) / avg_latency  # samples per second\n",
    "        \n",
    "        # Get model size\n",
    "        model_size = self.get_model_size(model_path, 'pytorch')\n",
    "        \n",
    "        # Clean up\n",
    "        del model, tokenizer, input_ids, attention_mask\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        gc.collect()\n",
    "        \n",
    "        return BenchmarkResult(\n",
    "            model_name=model_name,\n",
    "            model_type='pytorch',\n",
    "            batch_size=batch_size,\n",
    "            avg_latency_ms=float(avg_latency),\n",
    "            p50_latency_ms=float(p50_latency),\n",
    "            p95_latency_ms=float(p95_latency),\n",
    "            p99_latency_ms=float(p99_latency),\n",
    "            throughput_samples_per_sec=float(throughput),\n",
    "            peak_memory_mb=float(max(0, peak_memory)),\n",
    "            model_size_mb=float(model_size),\n",
    "            provider='PyTorch'\n",
    "        )\n",
    "    \n",
    "    def benchmark_onnx_model(self, model_info: Dict, batch_size: int, provider: str = 'CPUExecutionProvider') -> BenchmarkResult:\n",
    "        \"\"\"Benchmark ONNX model performance.\"\"\"\n",
    "        model_name = model_info['name']\n",
    "        model_path = model_info['path']\n",
    "        \n",
    "        # Find ONNX model file\n",
    "        onnx_files = model_info['onnx_files']\n",
    "        if not onnx_files:\n",
    "            raise ValueError(f\"No ONNX files found for {model_name}\")\n",
    "        \n",
    "        onnx_model_path = onnx_files[0]  # Use first ONNX file\n",
    "        \n",
    "        # Load tokenizer and create ONNX session\n",
    "        tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n",
    "        session = ort.InferenceSession(str(onnx_model_path), providers=[provider])\n",
    "        \n",
    "        # Create sample inputs\n",
    "        input_ids, attention_mask = self.create_sample_batch(tokenizer, batch_size)\n",
    "        \n",
    "        # Convert to numpy for ONNX\n",
    "        onnx_inputs = {\n",
    "            'input_ids': input_ids.numpy() if hasattr(input_ids, 'numpy') else input_ids,\n",
    "            'attention_mask': attention_mask.numpy() if hasattr(attention_mask, 'numpy') else attention_mask\n",
    "        }\n",
    "        \n",
    "        # Warmup runs\n",
    "        for _ in range(WARMUP_ITERATIONS):\n",
    "            _ = session.run(None, onnx_inputs)\n",
    "        \n",
    "        # Benchmark runs\n",
    "        latencies = []\n",
    "        memory_before = self.measure_memory_usage()\n",
    "        \n",
    "        for _ in range(ITERATIONS):\n",
    "            start_time = time.perf_counter()\n",
    "            _ = session.run(None, onnx_inputs)\n",
    "            end_time = time.perf_counter()\n",
    "            latencies.append((end_time - start_time) * 1000)  # Convert to ms\n",
    "        \n",
    "        memory_after = self.measure_memory_usage()\n",
    "        peak_memory = memory_after - memory_before\n",
    "        \n",
    "        # Calculate statistics\n",
    "        latencies = np.array(latencies)\n",
    "        avg_latency = np.mean(latencies)\n",
    "        p50_latency = np.percentile(latencies, 50)\n",
    "        p95_latency = np.percentile(latencies, 95)\n",
    "        p99_latency = np.percentile(latencies, 99)\n",
    "        throughput = (batch_size * 1000) / avg_latency  # samples per second\n",
    "        \n",
    "        # Get model size\n",
    "        model_size = self.get_model_size(model_path, 'onnx')\n",
    "        \n",
    "        # Clean up\n",
    "        del tokenizer, session\n",
    "        gc.collect()\n",
    "        \n",
    "        return BenchmarkResult(\n",
    "            model_name=model_name,\n",
    "            model_type='onnx',\n",
    "            batch_size=batch_size,\n",
    "            avg_latency_ms=float(avg_latency),\n",
    "            p50_latency_ms=float(p50_latency),\n",
    "            p95_latency_ms=float(p95_latency),\n",
    "            p99_latency_ms=float(p99_latency),\n",
    "            throughput_samples_per_sec=float(throughput),\n",
    "            peak_memory_mb=float(max(0, peak_memory)),\n",
    "            model_size_mb=float(model_size),\n",
    "            provider=provider\n",
    "        )\n",
    "\n",
    "# Initialize benchmarker\n",
    "benchmarker = ModelBenchmarker(logger)\n",
    "print(f\"üîß Benchmarker initialized with providers: {benchmarker.execution_providers}\")\n",
    "\n",
    "logger.info(\"Benchmarking engine ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Benchmarking Execution\n",
    "logger.info(\"üöÄ Starting performance benchmarking...\")\n",
    "\n",
    "all_results = []\n",
    "benchmark_summary = {\n",
    "    'benchmark_timestamp': datetime.now().isoformat(),\n",
    "    'configuration': {\n",
    "        'fast_mode': FAST_MODE,\n",
    "        'iterations': ITERATIONS,\n",
    "        'warmup_iterations': WARMUP_ITERATIONS,\n",
    "        'batch_sizes': BATCH_SIZES,\n",
    "        'include_accuracy': INCLUDE_ACCURACY,\n",
    "        'accuracy_sample_size': ACCURACY_SAMPLE_SIZE if INCLUDE_ACCURACY else None\n",
    "    },\n",
    "    'system_info': {\n",
    "        'platform': platform.platform(),\n",
    "        'python_version': platform.python_version(),\n",
    "        'pytorch_version': torch.__version__ if 'torch' in globals() else None,\n",
    "        'onnxruntime_version': ort.__version__ if onnx_available else None,\n",
    "        'available_providers': benchmarker.execution_providers if onnx_available else []\n",
    "    },\n",
    "    'models_benchmarked': 0,\n",
    "    'total_benchmarks': 0\n",
    "}\n",
    "\n",
    "print(f\"\\n‚ö° Performance Benchmarking:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "total_benchmarks = 0\n",
    "for model_name, model_info in available_models.items():\n",
    "    for batch_size in BATCH_SIZES:\n",
    "        if model_info['has_pytorch']:\n",
    "            total_benchmarks += 1\n",
    "        if model_info['has_onnx']:\n",
    "            total_benchmarks += len(benchmarker.execution_providers)\n",
    "\n",
    "print(f\"üìä Total benchmarks to run: {total_benchmarks}\")\n",
    "print(f\"‚è±Ô∏è Estimated time: {total_benchmarks * ITERATIONS * 0.1:.1f} seconds\")\n",
    "\n",
    "current_benchmark = 0\n",
    "successful_benchmarks = 0\n",
    "\n",
    "for model_name, model_info in available_models.items():\n",
    "    print(f\"\\nü§ñ Benchmarking {model_name}:\")\n",
    "    print(f\"   üìÅ Path: {model_info['path']}\")\n",
    "    \n",
    "    model_results = []\n",
    "    \n",
    "    for batch_size in BATCH_SIZES:\n",
    "        print(f\"\\n   üìä Batch size: {batch_size}\")\n",
    "        \n",
    "        # Benchmark PyTorch model\n",
    "        if model_info['has_pytorch']:\n",
    "            current_benchmark += 1\n",
    "            print(f\"   üî• PyTorch [{current_benchmark}/{total_benchmarks}]\", end=\" ... \")\n",
    "            \n",
    "            try:\n",
    "                result = benchmarker.benchmark_pytorch_model(model_info, batch_size)\n",
    "                model_results.append(result)\n",
    "                all_results.append(result)\n",
    "                successful_benchmarks += 1\n",
    "                \n",
    "                print(f\"‚úÖ {result.avg_latency_ms:.2f}ms avg, {result.throughput_samples_per_sec:.1f} samples/sec\")\n",
    "                logger.info(f\"PyTorch benchmark completed for {model_name} (batch={batch_size})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed: {str(e)}\")\n",
    "                logger.error(f\"PyTorch benchmark failed for {model_name}: {str(e)}\")\n",
    "        \n",
    "        # Benchmark ONNX model with different providers\n",
    "        if model_info['has_onnx']:\n",
    "            for provider in benchmarker.execution_providers:\n",
    "                current_benchmark += 1\n",
    "                provider_name = provider.replace('ExecutionProvider', '')\n",
    "                print(f\"   ‚ö° ONNX-{provider_name} [{current_benchmark}/{total_benchmarks}]\", end=\" ... \")\n",
    "                \n",
    "                try:\n",
    "                    result = benchmarker.benchmark_onnx_model(model_info, batch_size, provider)\n",
    "                    model_results.append(result)\n",
    "                    all_results.append(result)\n",
    "                    successful_benchmarks += 1\n",
    "                    \n",
    "                    print(f\"‚úÖ {result.avg_latency_ms:.2f}ms avg, {result.throughput_samples_per_sec:.1f} samples/sec\")\n",
    "                    logger.info(f\"ONNX-{provider_name} benchmark completed for {model_name} (batch={batch_size})\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Failed: {str(e)}\")\n",
    "                    logger.error(f\"ONNX-{provider_name} benchmark failed for {model_name}: {str(e)}\")\n",
    "    \n",
    "    # Display model summary\n",
    "    if model_results:\n",
    "        best_latency = min(r.avg_latency_ms for r in model_results)\n",
    "        best_throughput = max(r.throughput_samples_per_sec for r in model_results)\n",
    "        \n",
    "        best_latency_config = next(r for r in model_results if r.avg_latency_ms == best_latency)\n",
    "        best_throughput_config = next(r for r in model_results if r.throughput_samples_per_sec == best_throughput)\n",
    "        \n",
    "        print(f\"   üìà Best latency: {best_latency:.2f}ms ({best_latency_config.model_type}-{best_latency_config.provider}, batch={best_latency_config.batch_size})\")\n",
    "        print(f\"   üöÄ Best throughput: {best_throughput:.1f} samples/sec ({best_throughput_config.model_type}-{best_throughput_config.provider}, batch={best_throughput_config.batch_size})\")\n",
    "\n",
    "# Update summary\n",
    "benchmark_summary['models_benchmarked'] = len(available_models)\n",
    "benchmark_summary['total_benchmarks'] = total_benchmarks\n",
    "benchmark_summary['successful_benchmarks'] = successful_benchmarks\n",
    "benchmark_summary['failed_benchmarks'] = total_benchmarks - successful_benchmarks\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚ö° Performance Benchmarking Summary:\")\n",
    "print(f\"   ü§ñ Models benchmarked: {benchmark_summary['models_benchmarked']}\")\n",
    "print(f\"   ‚úÖ Successful benchmarks: {successful_benchmarks}\")\n",
    "print(f\"   ‚ùå Failed benchmarks: {total_benchmarks - successful_benchmarks}\")\n",
    "\n",
    "if successful_benchmarks == 0:\n",
    "    logger.error(\"No benchmarks completed successfully\")\n",
    "    raise RuntimeError(\"All benchmarks failed\")\n",
    "\n",
    "logger.info(f\"Performance benchmarking completed: {successful_benchmarks}/{total_benchmarks} successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results Analysis and Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"\\nüìä Results Analysis:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_data = []\n",
    "for result in all_results:\n",
    "    results_data.append({\n",
    "        'model_name': result.model_name,\n",
    "        'model_type': result.model_type,\n",
    "        'provider': result.provider,\n",
    "        'batch_size': result.batch_size,\n",
    "        'avg_latency_ms': result.avg_latency_ms,\n",
    "        'p50_latency_ms': result.p50_latency_ms,\n",
    "        'p95_latency_ms': result.p95_latency_ms,\n",
    "        'p99_latency_ms': result.p99_latency_ms,\n",
    "        'throughput_samples_per_sec': result.throughput_samples_per_sec,\n",
    "        'memory_usage_mb': result.memory_usage_mb,\n",
    "        'accuracy_score': result.accuracy_score\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "print(f\"üìã Created results DataFrame with {len(df_results)} entries\")\n",
    "\n",
    "# Display best performers\n",
    "print(f\"\\nüèÜ Best Performers:\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Best latency\n",
    "best_latency_row = df_results.loc[df_results['avg_latency_ms'].idxmin()]\n",
    "print(f\"‚ö° Lowest Latency: {best_latency_row['avg_latency_ms']:.2f}ms\")\n",
    "print(f\"   Model: {best_latency_row['model_name']} ({best_latency_row['model_type']}-{best_latency_row['provider']})\")\n",
    "print(f\"   Batch: {best_latency_row['batch_size']}, Throughput: {best_latency_row['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "\n",
    "# Best throughput\n",
    "best_throughput_row = df_results.loc[df_results['throughput_samples_per_sec'].idxmax()]\n",
    "print(f\"\\nüöÄ Highest Throughput: {best_throughput_row['throughput_samples_per_sec']:.1f} samples/sec\")\n",
    "print(f\"   Model: {best_throughput_row['model_name']} ({best_throughput_row['model_type']}-{best_throughput_row['provider']})\")\n",
    "print(f\"   Batch: {best_throughput_row['batch_size']}, Latency: {best_throughput_row['avg_latency_ms']:.2f}ms\")\n",
    "\n",
    "# Best accuracy (if available)\n",
    "if df_results['accuracy_score'].notna().any():\n",
    "    best_accuracy_row = df_results.loc[df_results['accuracy_score'].idxmax()]\n",
    "    print(f\"\\nüéØ Highest Accuracy: {best_accuracy_row['accuracy_score']:.4f}\")\n",
    "    print(f\"   Model: {best_accuracy_row['model_name']} ({best_accuracy_row['model_type']}-{best_accuracy_row['provider']})\")\n",
    "    print(f\"   Batch: {best_accuracy_row['batch_size']}, Latency: {best_accuracy_row['avg_latency_ms']:.2f}ms\")\n",
    "\n",
    "# Performance by model type\n",
    "print(f\"\\nüìä Performance by Model Type:\")\n",
    "print(f\"{'='*50}\")\n",
    "type_summary = df_results.groupby('model_type').agg({\n",
    "    'avg_latency_ms': ['mean', 'min', 'max'],\n",
    "    'throughput_samples_per_sec': ['mean', 'min', 'max'],\n",
    "    'accuracy_score': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "for model_type in type_summary.index:\n",
    "    print(f\"{model_type.upper()}:\")\n",
    "    print(f\"   Latency: {type_summary.loc[model_type, ('avg_latency_ms', 'mean')]:.2f}ms avg \"\n",
    "          f\"({type_summary.loc[model_type, ('avg_latency_ms', 'min')]:.2f}-{type_summary.loc[model_type, ('avg_latency_ms', 'max')]:.2f}ms)\")\n",
    "    print(f\"   Throughput: {type_summary.loc[model_type, ('throughput_samples_per_sec', 'mean')]:.1f} samples/sec avg \"\n",
    "          f\"({type_summary.loc[model_type, ('throughput_samples_per_sec', 'min')]:.1f}-{type_summary.loc[model_type, ('throughput_samples_per_sec', 'max')]:.1f})\")\n",
    "    if not pd.isna(type_summary.loc[model_type, ('accuracy_score', 'mean')]):\n",
    "        print(f\"   Accuracy: {type_summary.loc[model_type, ('accuracy_score', 'mean')]:.4f}\")\n",
    "\n",
    "# Create visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Performance Benchmarking Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Latency comparison\n",
    "ax1 = axes[0, 0]\n",
    "df_pivot_latency = df_results.pivot_table(values='avg_latency_ms', index='model_name', \n",
    "                                          columns=['model_type', 'provider'], aggfunc='mean')\n",
    "df_pivot_latency.plot(kind='bar', ax=ax1, rot=45)\n",
    "ax1.set_title('Average Latency by Model')\n",
    "ax1.set_ylabel('Latency (ms)')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 2. Throughput comparison  \n",
    "ax2 = axes[0, 1]\n",
    "df_pivot_throughput = df_results.pivot_table(values='throughput_samples_per_sec', index='model_name',\n",
    "                                             columns=['model_type', 'provider'], aggfunc='mean')\n",
    "df_pivot_throughput.plot(kind='bar', ax=ax2, rot=45)\n",
    "ax2.set_title('Throughput by Model')\n",
    "ax2.set_ylabel('Samples/sec')\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 3. Batch size impact on latency\n",
    "ax3 = axes[1, 0]\n",
    "for model_type in df_results['model_type'].unique():\n",
    "    subset = df_results[df_results['model_type'] == model_type]\n",
    "    batch_latency = subset.groupby('batch_size')['avg_latency_ms'].mean()\n",
    "    ax3.plot(batch_latency.index, batch_latency.values, marker='o', label=model_type)\n",
    "ax3.set_title('Batch Size Impact on Latency')\n",
    "ax3.set_xlabel('Batch Size')\n",
    "ax3.set_ylabel('Average Latency (ms)')\n",
    "ax3.legend()\n",
    "ax3.set_xscale('log')\n",
    "\n",
    "# 4. Memory usage vs performance\n",
    "ax4 = axes[1, 1]\n",
    "scatter = ax4.scatter(df_results['memory_usage_mb'], df_results['avg_latency_ms'], \n",
    "                      c=df_results['throughput_samples_per_sec'], cmap='viridis', alpha=0.7)\n",
    "ax4.set_title('Memory vs Latency (colored by throughput)')\n",
    "ax4.set_xlabel('Memory Usage (MB)')\n",
    "ax4.set_ylabel('Average Latency (ms)')\n",
    "plt.colorbar(scatter, ax=ax4, label='Throughput (samples/sec)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Results visualization completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"\\nüíæ Saving Results:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Create results directory\n",
    "results_dir = base_path / 'results'\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate filename with timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "base_filename = f\"benchmark_results_generalized_{timestamp}\"\n",
    "\n",
    "# 1. Save CSV results\n",
    "csv_path = results_dir / f\"{base_filename}.csv\"\n",
    "df_results.to_csv(csv_path, index=False)\n",
    "print(f\"üìä CSV results saved: {csv_path}\")\n",
    "\n",
    "# 2. Save detailed JSON results\n",
    "json_results = {\n",
    "    'benchmark_summary': benchmark_summary,\n",
    "    'detailed_results': [result.__dict__ for result in all_results],\n",
    "    'performance_summary': {\n",
    "        'best_latency': {\n",
    "            'value': float(best_latency_row['avg_latency_ms']),\n",
    "            'model': best_latency_row['model_name'],\n",
    "            'config': f\"{best_latency_row['model_type']}-{best_latency_row['provider']}\",\n",
    "            'batch_size': int(best_latency_row['batch_size'])\n",
    "        },\n",
    "        'best_throughput': {\n",
    "            'value': float(best_throughput_row['throughput_samples_per_sec']),\n",
    "            'model': best_throughput_row['model_name'],\n",
    "            'config': f\"{best_throughput_row['model_type']}-{best_throughput_row['provider']}\",\n",
    "            'batch_size': int(best_throughput_row['batch_size'])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add accuracy summary if available\n",
    "if df_results['accuracy_score'].notna().any():\n",
    "    json_results['performance_summary']['best_accuracy'] = {\n",
    "        'value': float(best_accuracy_row['accuracy_score']),\n",
    "        'model': best_accuracy_row['model_name'],\n",
    "        'config': f\"{best_accuracy_row['model_type']}-{best_accuracy_row['provider']}\",\n",
    "        'batch_size': int(best_accuracy_row['batch_size'])\n",
    "    }\n",
    "\n",
    "json_path = results_dir / f\"{base_filename}.json\"\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(json_results, f, indent=2, default=str)\n",
    "print(f\"üìã JSON results saved: {json_path}\")\n",
    "\n",
    "# 3. Save performance summary report\n",
    "report_path = results_dir / f\"{base_filename}_summary.txt\"\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"Performance Benchmarking Summary Report\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Configuration: {json_results['benchmark_summary']['configuration']}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Best Performers:\\n\")\n",
    "    f.write(\"-\" * 20 + \"\\n\")\n",
    "    f.write(f\"Lowest Latency: {best_latency_row['avg_latency_ms']:.2f}ms\\n\")\n",
    "    f.write(f\"  Model: {best_latency_row['model_name']} ({best_latency_row['model_type']}-{best_latency_row['provider']})\\n\")\n",
    "    f.write(f\"  Batch: {best_latency_row['batch_size']}\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Highest Throughput: {best_throughput_row['throughput_samples_per_sec']:.1f} samples/sec\\n\")\n",
    "    f.write(f\"  Model: {best_throughput_row['model_name']} ({best_throughput_row['model_type']}-{best_throughput_row['provider']})\\n\")\n",
    "    f.write(f\"  Batch: {best_throughput_row['batch_size']}\\n\\n\")\n",
    "    \n",
    "    if df_results['accuracy_score'].notna().any():\n",
    "        f.write(f\"Highest Accuracy: {best_accuracy_row['accuracy_score']:.4f}\\n\")\n",
    "        f.write(f\"  Model: {best_accuracy_row['model_name']} ({best_accuracy_row['model_type']}-{best_accuracy_row['provider']})\\n\")\n",
    "        f.write(f\"  Batch: {best_accuracy_row['batch_size']}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Performance by Model Type:\\n\")\n",
    "    f.write(\"-\" * 30 + \"\\n\")\n",
    "    for model_type in type_summary.index:\n",
    "        f.write(f\"{model_type.upper()}:\\n\")\n",
    "        f.write(f\"  Avg Latency: {type_summary.loc[model_type, ('avg_latency_ms', 'mean')]:.2f}ms\\n\")\n",
    "        f.write(f\"  Avg Throughput: {type_summary.loc[model_type, ('throughput_samples_per_sec', 'mean')]:.1f} samples/sec\\n\")\n",
    "        if not pd.isna(type_summary.loc[model_type, ('accuracy_score', 'mean')]):\n",
    "            f.write(f\"  Avg Accuracy: {type_summary.loc[model_type, ('accuracy_score', 'mean')]:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"üìÑ Summary report saved: {report_path}\")\n",
    "\n",
    "# 4. Save plot\n",
    "plot_path = results_dir / f\"{base_filename}_plots.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"üìà Plots saved: {plot_path}\")\n",
    "\n",
    "# 5. Update state manager with latest results\n",
    "try:\n",
    "    state_manager.save_state('benchmark_results', {\n",
    "        'latest_run': {\n",
    "            'timestamp': timestamp,\n",
    "            'csv_path': str(csv_path),\n",
    "            'json_path': str(json_path),\n",
    "            'report_path': str(report_path),\n",
    "            'plot_path': str(plot_path)\n",
    "        },\n",
    "        'summary': json_results['performance_summary'],\n",
    "        'models_benchmarked': list(available_models.keys()),\n",
    "        'successful_benchmarks': successful_benchmarks,\n",
    "        'total_benchmarks': total_benchmarks\n",
    "    })\n",
    "    print(f\"üíæ State updated with benchmark results\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Failed to update state: {e}\")\n",
    "\n",
    "# Display completion summary\n",
    "print(f\"\\nüéâ Benchmarking Complete!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"üìä Results available in: {results_dir}\")\n",
    "print(f\"üìà CSV: {csv_path.name}\")\n",
    "print(f\"üìã JSON: {json_path.name}\")\n",
    "print(f\"üìÑ Report: {report_path.name}\")\n",
    "print(f\"üñºÔ∏è Plots: {plot_path.name}\")\n",
    "\n",
    "logger.info(f\"Comprehensive benchmarking completed successfully. Results saved to {results_dir}\")\n",
    "print(f\"\\n‚úÖ All benchmarking tasks completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
