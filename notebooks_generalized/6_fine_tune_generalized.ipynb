{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Comparative Fine-Tuning: Explainability-Driven vs Standard Approaches\n",
    "\n",
    "## Overview\n",
    "This notebook provides a comprehensive comparison of different fine-tuning methodologies for financial NLP models. We compare four distinct approaches to demonstrate the value of explainability-driven optimization.\n",
    "\n",
    "### üéØ Fine-Tuning Methods Compared\n",
    "\n",
    "1. **üîÑ Baseline (No Fine-Tuning)**: Pre-trained models as-is\n",
    "2. **üìà Standard Fine-Tuning**: Traditional uniform fine-tuning approach\n",
    "3. **üß† Explainability-Driven**: SHAP/LIME-guided targeted fine-tuning\n",
    "4. **üîÄ Hybrid Approach**: Combined standard + explainability refinement\n",
    "\n",
    "### üìä Evaluation Dimensions\n",
    "\n",
    "- **Performance**: Accuracy, F1, Precision, Recall\n",
    "- **Explainability**: SHAP coherence, attention focus, decision boundary stability\n",
    "- **Efficiency**: Training time, convergence speed, computational cost\n",
    "- **Robustness**: Confidence distribution, mistake pattern analysis\n",
    "\n",
    "### üî¨ Academic Value\n",
    "\n",
    "This comparative analysis provides:\n",
    "- **Controlled Experiments**: Same models, data, different approaches\n",
    "- **Statistical Validation**: Significance testing across methods\n",
    "- **Ablation Studies**: Understanding which explainability insights matter most\n",
    "- **Trade-off Analysis**: Performance vs interpretability vs efficiency\n",
    "\n",
    "### üéì Research Applications\n",
    "\n",
    "Perfect for demonstrating:\n",
    "- Novel explainability-driven fine-tuning methodology\n",
    "- Quantitative evidence of explainability impact on performance\n",
    "- Systematic comparison framework for future research\n",
    "- Domain-specific insights for financial NLP\n",
    "\n",
    "**Configuration-driven approach:** All settings loaded from `../config/pipeline_config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration system and comprehensive libraries for comparative analysis\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.pipeline_utils import ConfigManager, StateManager, LoggingManager\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Optional, Tuple, Any, Union\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Model and tokenizer for fine-tuning\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Explainability libraries\n",
    "print(\"üîç Importing explainability libraries...\")\n",
    "try:\n",
    "    import shap\n",
    "    shap_available = True\n",
    "    print(\"‚úÖ SHAP available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è SHAP not available. Install with: pip install shap\")\n",
    "    shap_available = False\n",
    "\n",
    "try:\n",
    "    from lime.lime_text import LimeTextExplainer\n",
    "    lime_available = True\n",
    "    print(\"‚úÖ LIME available\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è LIME not available. Install with: pip install lime\")\n",
    "    lime_available = False\n",
    "\n",
    "# Visualization and interactivity\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Initialize managers\n",
    "config = ConfigManager(\"../config/pipeline_config.json\")\n",
    "state = StateManager(\"../config/pipeline_state.json\")\n",
    "logger_manager = LoggingManager(config, 'comparative_fine_tuning')\n",
    "logger = logger_manager.get_logger()\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"\udcc2 Models directory: {config.get('models', {}).get('output_dir', 'models')}\")\n",
    "print(f\"üìä Data directory: {config.get('data', {}).get('processed_data_dir', 'data/processed')}\")\n",
    "print(\"üî¨ Starting Comparative Fine-Tuning Analysis\")\n",
    "\n",
    "logger.info(\"üî¨ Starting Comparative Fine-Tuning Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparativeFineTuningFramework:\n",
    "    \"\"\"\n",
    "    Comprehensive framework for comparing different fine-tuning approaches.\n",
    "    Implements four distinct methodologies:\n",
    "    1. Baseline (minimal fine-tuning)\n",
    "    2. Standard (conventional fine-tuning)\n",
    "    3. Explainability-Driven (guided by SHAP/LIME insights)\n",
    "    4. Hybrid (combined approach)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, logger, data_dir=\"data\", models_dir=\"models\"):\n",
    "        self.config = config\n",
    "        self.logger = logger\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.models_dir = Path(models_dir)\n",
    "        \n",
    "        # Results storage\n",
    "        self.results = {\n",
    "            'baseline': {},\n",
    "            'standard': {},\n",
    "            'explainability': {},\n",
    "            'hybrid': {}\n",
    "        }\n",
    "        \n",
    "        # Explainability cache\n",
    "        self.explainability_cache = {}\n",
    "        \n",
    "        # Initialize explainers if available\n",
    "        self.shap_available = shap_available\n",
    "        self.lime_available = lime_available\n",
    "        \n",
    "        if self.lime_available:\n",
    "            self.lime_explainer = LimeTextExplainer(class_names=['negative', 'neutral', 'positive'])\n",
    "            \n",
    "        print(\"üî¨ ComparativeFineTuningFramework initialized\")\n",
    "        print(f\"   üìä Explainability tools: SHAP={self.shap_available}, LIME={self.lime_available}\")\n",
    "        self.logger.info(\"ComparativeFineTuningFramework initialized\")\n",
    "    \n",
    "    def load_data(self, dataset_name=\"FinancialPhraseBank\"):\n",
    "        \"\"\"Load and prepare data for comparative fine-tuning.\"\"\"\n",
    "        print(f\"üìÇ Loading dataset: {dataset_name}\")\n",
    "        \n",
    "        data_path = self.data_dir / dataset_name\n",
    "        \n",
    "        if dataset_name == \"FinancialPhraseBank\":\n",
    "            # Load all-data.csv\n",
    "            file_path = data_path / \"all-data.csv\"\n",
    "            if file_path.exists():\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"   ‚úÖ Loaded {len(df)} samples from {file_path}\")\n",
    "                \n",
    "                # Prepare train/test split\n",
    "                from sklearn.model_selection import train_test_split\n",
    "                train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "                \n",
    "                self.train_data = train_df\n",
    "                self.test_data = test_df\n",
    "                \n",
    "                print(f\"   üìä Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "                print(f\"   üè∑Ô∏è Classes: {sorted(df['label'].unique())}\")\n",
    "                \n",
    "                self.logger.info(f\"Data loaded: Train={len(train_df)}, Test={len(test_df)}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"   ‚ùå File not found: {file_path}\")\n",
    "                return False\n",
    "        \n",
    "        else:\n",
    "            # Try to load from standard train/test files\n",
    "            train_path = data_path / \"train.csv\"\n",
    "            test_path = data_path / \"test.csv\"\n",
    "            \n",
    "            if train_path.exists() and test_path.exists():\n",
    "                self.train_data = pd.read_csv(train_path)\n",
    "                self.test_data = pd.read_csv(test_path)\n",
    "                \n",
    "                print(f\"   ‚úÖ Loaded train: {len(self.train_data)}, test: {len(self.test_data)}\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"   ‚ùå Train/test files not found in {data_path}\")\n",
    "                return False\n",
    "    \n",
    "    def get_explainability_insights(self, model_name, sample_texts, sample_labels, n_samples=50):\n",
    "        \"\"\"Generate explainability insights for a model using SHAP and LIME.\"\"\"\n",
    "        cache_key = f\"{model_name}_{len(sample_texts)}\"\n",
    "        \n",
    "        if cache_key in self.explainability_cache:\n",
    "            print(f\"   üîÑ Using cached explainability insights for {model_name}\")\n",
    "            return self.explainability_cache[cache_key]\n",
    "        \n",
    "        print(f\"üîç Generating explainability insights for {model_name}\")\n",
    "        insights = {}\n",
    "        \n",
    "        # Load model for analysis\n",
    "        try:\n",
    "            model_path = self.models_dir / model_name\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "            model.eval()\n",
    "            \n",
    "            # Ensure pad_token is set\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Sample data for analysis\n",
    "            sample_indices = np.random.choice(len(sample_texts), min(n_samples, len(sample_texts)), replace=False)\n",
    "            sample_texts_subset = [sample_texts[i] for i in sample_indices]\n",
    "            sample_labels_subset = [sample_labels[i] for i in sample_indices]\n",
    "            \n",
    "            insights['difficult_samples'] = []\n",
    "            insights['feature_importance'] = {}\n",
    "            insights['token_patterns'] = {}\n",
    "            \n",
    "            # Simple prediction-based analysis (lightweight version)\n",
    "            with torch.no_grad():\n",
    "                for i, text in enumerate(sample_texts_subset[:10]):  # Analyze first 10 samples\n",
    "                    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "                    outputs = model(**inputs)\n",
    "                    probs = torch.softmax(outputs.logits, dim=-1)\n",
    "                    predicted_class = torch.argmax(probs, dim=-1).item()\n",
    "                    confidence = torch.max(probs).item()\n",
    "                    \n",
    "                    # Identify low-confidence predictions as \"difficult\"\n",
    "                    if confidence < 0.7:\n",
    "                        insights['difficult_samples'].append({\n",
    "                            'text': text,\n",
    "                            'true_label': sample_labels_subset[i],\n",
    "                            'predicted_label': predicted_class,\n",
    "                            'confidence': confidence\n",
    "                        })\n",
    "            \n",
    "            # Basic token analysis\n",
    "            all_tokens = []\n",
    "            for text in sample_texts_subset[:20]:\n",
    "                tokens = tokenizer.tokenize(text)\n",
    "                all_tokens.extend(tokens)\n",
    "            \n",
    "            token_counts = Counter(all_tokens)\n",
    "            insights['token_patterns']['most_common'] = token_counts.most_common(20)\n",
    "            \n",
    "            print(f\"   ‚úÖ Generated insights: {len(insights['difficult_samples'])} difficult samples identified\")\n",
    "            \n",
    "            # Cache results\n",
    "            self.explainability_cache[cache_key] = insights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error generating explainability insights: {e}\")\n",
    "            insights = {'error': str(e)}\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def baseline_fine_tuning(self, model_name, training_args_override=None):\n",
    "        \"\"\"\n",
    "        Baseline approach: Minimal fine-tuning with default parameters.\n",
    "        \"\"\"\n",
    "        print(f\"üèÅ Starting Baseline Fine-Tuning for {model_name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Load model and tokenizer\n",
    "            model_path = self.models_dir / model_name\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "            \n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Minimal training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=f\"./results/{model_name}_baseline\",\n",
    "                num_train_epochs=1,  # Minimal training\n",
    "                per_device_train_batch_size=8,\n",
    "                per_device_eval_batch_size=8,\n",
    "                learning_rate=5e-5,\n",
    "                warmup_steps=10,\n",
    "                logging_dir=f\"./logs/{model_name}_baseline\",\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=100,\n",
    "                save_strategy=\"steps\",\n",
    "                save_steps=200,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_accuracy\",\n",
    "                greater_is_better=True,\n",
    "            )\n",
    "            \n",
    "            if training_args_override:\n",
    "                for key, value in training_args_override.items():\n",
    "                    setattr(training_args, key, value)\n",
    "            \n",
    "            # Prepare datasets\n",
    "            def tokenize_function(examples):\n",
    "                return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)\n",
    "            \n",
    "            train_dataset = Dataset.from_pandas(self.train_data)\n",
    "            train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "            \n",
    "            test_dataset = Dataset.from_pandas(self.test_data)\n",
    "            test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "            \n",
    "            # Data collator\n",
    "            data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "            \n",
    "            # Compute metrics function\n",
    "            def compute_metrics(eval_pred):\n",
    "                predictions, labels = eval_pred\n",
    "                predictions = np.argmax(predictions, axis=1)\n",
    "                return {'accuracy': accuracy_score(labels, predictions)}\n",
    "            \n",
    "            # Initialize trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=test_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "            )\n",
    "            \n",
    "            # Train\n",
    "            trainer.train()\n",
    "            \n",
    "            # Evaluate\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            # Store results\n",
    "            end_time = time.time()\n",
    "            self.results['baseline'][model_name] = {\n",
    "                'training_time': end_time - start_time,\n",
    "                'eval_accuracy': eval_results['eval_accuracy'],\n",
    "                'eval_loss': eval_results['eval_loss'],\n",
    "                'training_args': training_args.to_dict(),\n",
    "                'approach': 'baseline'\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚úÖ Baseline complete - Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "            print(f\"   ‚è±Ô∏è Training time: {end_time - start_time:.2f}s\")\n",
    "            \n",
    "            return self.results['baseline'][model_name]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Baseline fine-tuning failed: {e}\")\n",
    "            self.logger.error(f\"Baseline fine-tuning failed for {model_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def standard_fine_tuning(self, model_name, training_args_override=None):\n",
    "        \"\"\"\n",
    "        Standard approach: Conventional fine-tuning with best practices.\n",
    "        \"\"\"\n",
    "        print(f\"üîß Starting Standard Fine-Tuning for {model_name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Load model and tokenizer\n",
    "            model_path = self.models_dir / model_name\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "            \n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Standard training arguments with best practices\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=f\"./results/{model_name}_standard\",\n",
    "                num_train_epochs=3,\n",
    "                per_device_train_batch_size=16,\n",
    "                per_device_eval_batch_size=16,\n",
    "                learning_rate=2e-5,\n",
    "                warmup_steps=500,\n",
    "                weight_decay=0.01,\n",
    "                logging_dir=f\"./logs/{model_name}_standard\",\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=200,\n",
    "                save_strategy=\"steps\",\n",
    "                save_steps=400,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_accuracy\",\n",
    "                greater_is_better=True,\n",
    "                fp16=torch.cuda.is_available(),\n",
    "            )\n",
    "            \n",
    "            if training_args_override:\n",
    "                for key, value in training_args_override.items():\n",
    "                    setattr(training_args, key, value)\n",
    "            \n",
    "            # Prepare datasets\n",
    "            def tokenize_function(examples):\n",
    "                return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)\n",
    "            \n",
    "            train_dataset = Dataset.from_pandas(self.train_data)\n",
    "            train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "            \n",
    "            test_dataset = Dataset.from_pandas(self.test_data)\n",
    "            test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "            \n",
    "            # Data collator\n",
    "            data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "            \n",
    "            # Compute metrics function\n",
    "            def compute_metrics(eval_pred):\n",
    "                predictions, labels = eval_pred\n",
    "                predictions = np.argmax(predictions, axis=1)\n",
    "                return {'accuracy': accuracy_score(labels, predictions)}\n",
    "            \n",
    "            # Initialize trainer with early stopping\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=test_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "            )\n",
    "            \n",
    "            # Train\n",
    "            trainer.train()\n",
    "            \n",
    "            # Evaluate\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            # Store results\n",
    "            end_time = time.time()\n",
    "            self.results['standard'][model_name] = {\n",
    "                'training_time': end_time - start_time,\n",
    "                'eval_accuracy': eval_results['eval_accuracy'],\n",
    "                'eval_loss': eval_results['eval_loss'],\n",
    "                'training_args': training_args.to_dict(),\n",
    "                'approach': 'standard'\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚úÖ Standard complete - Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "            print(f\"   ‚è±Ô∏è Training time: {end_time - start_time:.2f}s\")\n",
    "            \n",
    "            return self.results['standard'][model_name]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Standard fine-tuning failed: {e}\")\n",
    "            self.logger.error(f\"Standard fine-tuning failed for {model_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def explainability_driven_fine_tuning(self, model_name, training_args_override=None):\n",
    "        \"\"\"\n",
    "        Explainability-driven approach: Use insights to guide fine-tuning.\n",
    "        \"\"\"\n",
    "        print(f\"üîç Starting Explainability-Driven Fine-Tuning for {model_name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Generate explainability insights first\n",
    "            sample_texts = self.train_data['text'].tolist()\n",
    "            sample_labels = self.train_data['label'].tolist()\n",
    "            insights = self.get_explainability_insights(model_name, sample_texts, sample_labels)\n",
    "            \n",
    "            # Load model and tokenizer\n",
    "            model_path = self.models_dir / model_name\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "            \n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Adjust training based on insights\n",
    "            base_lr = 2e-5\n",
    "            adjusted_lr = base_lr\n",
    "            \n",
    "            # If we have difficult samples, increase learning rate slightly\n",
    "            if 'difficult_samples' in insights and len(insights['difficult_samples']) > 0:\n",
    "                difficult_ratio = len(insights['difficult_samples']) / min(50, len(sample_texts))\n",
    "                if difficult_ratio > 0.3:  # High difficulty\n",
    "                    adjusted_lr = base_lr * 1.5\n",
    "                    print(f\"   üìà Increased learning rate to {adjusted_lr} due to difficult samples\")\n",
    "            \n",
    "            # Explainability-informed training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=f\"./results/{model_name}_explainability\",\n",
    "                num_train_epochs=4,  # More epochs for difficult cases\n",
    "                per_device_train_batch_size=12,\n",
    "                per_device_eval_batch_size=12,\n",
    "                learning_rate=adjusted_lr,\n",
    "                warmup_steps=300,\n",
    "                weight_decay=0.01,\n",
    "                logging_dir=f\"./logs/{model_name}_explainability\",\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=150,\n",
    "                save_strategy=\"steps\",\n",
    "                save_steps=300,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_accuracy\",\n",
    "                greater_is_better=True,\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                logging_steps=50,\n",
    "            )\n",
    "            \n",
    "            if training_args_override:\n",
    "                for key, value in training_args_override.items():\n",
    "                    setattr(training_args, key, value)\n",
    "            \n",
    "            # Focus on difficult samples by creating weighted dataset\n",
    "            train_df = self.train_data.copy()\n",
    "            \n",
    "            # If we have difficult samples, create a focused dataset\n",
    "            if 'difficult_samples' in insights and len(insights['difficult_samples']) > 0:\n",
    "                # Add difficult samples to training data with higher frequency\n",
    "                difficult_texts = [sample['text'] for sample in insights['difficult_samples']]\n",
    "                difficult_labels = [sample['true_label'] for sample in insights['difficult_samples']]\n",
    "                \n",
    "                # Create additional training samples from difficult cases\n",
    "                additional_df = pd.DataFrame({\n",
    "                    'text': difficult_texts * 2,  # Duplicate difficult samples\n",
    "                    'label': difficult_labels * 2\n",
    "                })\n",
    "                \n",
    "                train_df = pd.concat([train_df, additional_df], ignore_index=True)\n",
    "                print(f\"   üìä Enhanced training with {len(additional_df)} additional difficult samples\")\n",
    "            \n",
    "            # Prepare datasets\n",
    "            def tokenize_function(examples):\n",
    "                return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)\n",
    "            \n",
    "            train_dataset = Dataset.from_pandas(train_df)\n",
    "            train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "            \n",
    "            test_dataset = Dataset.from_pandas(self.test_data)\n",
    "            test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "            \n",
    "            # Data collator\n",
    "            data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "            \n",
    "            # Compute metrics function\n",
    "            def compute_metrics(eval_pred):\n",
    "                predictions, labels = eval_pred\n",
    "                predictions = np.argmax(predictions, axis=1)\n",
    "                return {'accuracy': accuracy_score(labels, predictions)}\n",
    "            \n",
    "            # Initialize trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=test_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "            )\n",
    "            \n",
    "            # Train\n",
    "            trainer.train()\n",
    "            \n",
    "            # Evaluate\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            # Store results\n",
    "            end_time = time.time()\n",
    "            self.results['explainability'][model_name] = {\n",
    "                'training_time': end_time - start_time,\n",
    "                'eval_accuracy': eval_results['eval_accuracy'],\n",
    "                'eval_loss': eval_results['eval_loss'],\n",
    "                'training_args': training_args.to_dict(),\n",
    "                'explainability_insights': insights,\n",
    "                'approach': 'explainability_driven'\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚úÖ Explainability-driven complete - Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "            print(f\"   ‚è±Ô∏è Training time: {end_time - start_time:.2f}s\")\n",
    "            \n",
    "            return self.results['explainability'][model_name]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Explainability-driven fine-tuning failed: {e}\")\n",
    "            self.logger.error(f\"Explainability-driven fine-tuning failed for {model_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def hybrid_fine_tuning(self, model_name, training_args_override=None):\n",
    "        \"\"\"\n",
    "        Hybrid approach: Combine standard and explainability-driven methods.\n",
    "        \"\"\"\n",
    "        print(f\"üîÄ Starting Hybrid Fine-Tuning for {model_name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Get insights but use them more conservatively\n",
    "            sample_texts = self.train_data['text'].tolist()\n",
    "            sample_labels = self.train_data['label'].tolist()\n",
    "            insights = self.get_explainability_insights(model_name, sample_texts, sample_labels)\n",
    "            \n",
    "            # Load model and tokenizer\n",
    "            model_path = self.models_dir / model_name\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "            \n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Balanced approach - moderate adjustments\n",
    "            base_lr = 2e-5\n",
    "            adjusted_lr = base_lr\n",
    "            \n",
    "            if 'difficult_samples' in insights and len(insights['difficult_samples']) > 0:\n",
    "                difficult_ratio = len(insights['difficult_samples']) / min(50, len(sample_texts))\n",
    "                if difficult_ratio > 0.4:\n",
    "                    adjusted_lr = base_lr * 1.2  # Modest increase\n",
    "                    print(f\"   üìä Moderately adjusted learning rate to {adjusted_lr}\")\n",
    "            \n",
    "            # Hybrid training arguments\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=f\"./results/{model_name}_hybrid\",\n",
    "                num_train_epochs=3,  # Standard epochs\n",
    "                per_device_train_batch_size=14,  # Between standard and explainability\n",
    "                per_device_eval_batch_size=14,\n",
    "                learning_rate=adjusted_lr,\n",
    "                warmup_steps=400,  # Between standard and explainability\n",
    "                weight_decay=0.01,\n",
    "                logging_dir=f\"./logs/{model_name}_hybrid\",\n",
    "                evaluation_strategy=\"steps\",\n",
    "                eval_steps=175,  # Between standard and explainability\n",
    "                save_strategy=\"steps\",\n",
    "                save_steps=350,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_accuracy\",\n",
    "                greater_is_better=True,\n",
    "                fp16=torch.cuda.is_available(),\n",
    "                logging_steps=75,\n",
    "            )\n",
    "            \n",
    "            if training_args_override:\n",
    "                for key, value in training_args_override.items():\n",
    "                    setattr(training_args, key, value)\n",
    "            \n",
    "            # Moderate focus on difficult samples\n",
    "            train_df = self.train_data.copy()\n",
    "            \n",
    "            if 'difficult_samples' in insights and len(insights['difficult_samples']) > 0:\n",
    "                # Add fewer duplicates than pure explainability approach\n",
    "                difficult_texts = [sample['text'] for sample in insights['difficult_samples']]\n",
    "                difficult_labels = [sample['true_label'] for sample in insights['difficult_samples']]\n",
    "                \n",
    "                additional_df = pd.DataFrame({\n",
    "                    'text': difficult_texts,  # Single duplication\n",
    "                    'label': difficult_labels\n",
    "                })\n",
    "                \n",
    "                train_df = pd.concat([train_df, additional_df], ignore_index=True)\n",
    "                print(f\"   üìä Added {len(additional_df)} additional samples (hybrid approach)\")\n",
    "            \n",
    "            # Prepare datasets\n",
    "            def tokenize_function(examples):\n",
    "                return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)\n",
    "            \n",
    "            train_dataset = Dataset.from_pandas(train_df)\n",
    "            train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "            \n",
    "            test_dataset = Dataset.from_pandas(self.test_data)\n",
    "            test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "            \n",
    "            # Data collator\n",
    "            data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "            \n",
    "            # Compute metrics function\n",
    "            def compute_metrics(eval_pred):\n",
    "                predictions, labels = eval_pred\n",
    "                predictions = np.argmax(predictions, axis=1)\n",
    "                return {'accuracy': accuracy_score(labels, predictions)}\n",
    "            \n",
    "            # Initialize trainer\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=test_dataset,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics,\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "            )\n",
    "            \n",
    "            # Train\n",
    "            trainer.train()\n",
    "            \n",
    "            # Evaluate\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            # Store results\n",
    "            end_time = time.time()\n",
    "            self.results['hybrid'][model_name] = {\n",
    "                'training_time': end_time - start_time,\n",
    "                'eval_accuracy': eval_results['eval_accuracy'],\n",
    "                'eval_loss': eval_results['eval_loss'],\n",
    "                'training_args': training_args.to_dict(),\n",
    "                'explainability_insights': insights,\n",
    "                'approach': 'hybrid'\n",
    "            }\n",
    "            \n",
    "            print(f\"   ‚úÖ Hybrid complete - Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "            print(f\"   ‚è±Ô∏è Training time: {end_time - start_time:.2f}s\")\n",
    "            \n",
    "            return self.results['hybrid'][model_name]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Hybrid fine-tuning failed: {e}\")\n",
    "            self.logger.error(f\"Hybrid fine-tuning failed for {model_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def run_comparative_analysis(self, model_names, approaches=['baseline', 'standard', 'explainability', 'hybrid']):\n",
    "        \"\"\"\n",
    "        Run comparative analysis across all specified approaches and models.\n",
    "        \"\"\"\n",
    "        print(\"üî¨ Starting Comprehensive Comparative Analysis\")\n",
    "        print(f\"   üìã Models: {model_names}\")\n",
    "        print(f\"   üîß Approaches: {approaches}\")\n",
    "        \n",
    "        total_experiments = len(model_names) * len(approaches)\n",
    "        completed = 0\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            print(f\"\\nü§ñ Analyzing model: {model_name}\")\n",
    "            \n",
    "            for approach in approaches:\n",
    "                print(f\"\\n   üîÑ Running {approach} approach...\")\n",
    "                completed += 1\n",
    "                print(f\"   üìä Progress: {completed}/{total_experiments}\")\n",
    "                \n",
    "                try:\n",
    "                    if approach == 'baseline':\n",
    "                        result = self.baseline_fine_tuning(model_name)\n",
    "                    elif approach == 'standard':\n",
    "                        result = self.standard_fine_tuning(model_name)\n",
    "                    elif approach == 'explainability':\n",
    "                        result = self.explainability_driven_fine_tuning(model_name)\n",
    "                    elif approach == 'hybrid':\n",
    "                        result = self.hybrid_fine_tuning(model_name)\n",
    "                    \n",
    "                    if result:\n",
    "                        print(f\"      ‚úÖ {approach} completed successfully\")\n",
    "                    else:\n",
    "                        print(f\"      ‚ùå {approach} failed\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"      ‚ö†Ô∏è {approach} encountered error: {e}\")\n",
    "                    self.logger.error(f\"{approach} approach failed for {model_name}: {e}\")\n",
    "        \n",
    "        print(f\"\\nüéâ Comparative analysis complete!\")\n",
    "        print(f\"   ‚úÖ Completed: {completed}/{total_experiments} experiments\")\n",
    "        \n",
    "        return self.generate_comparison_report()\n",
    "    \n",
    "    def generate_comparison_report(self):\n",
    "        \"\"\"Generate comprehensive comparison report.\"\"\"\n",
    "        print(\"\\nüìä Generating Comprehensive Comparison Report\")\n",
    "        \n",
    "        report = {\n",
    "            'summary': {},\n",
    "            'detailed_results': self.results,\n",
    "            'statistical_analysis': {},\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        # Collect all results for analysis\n",
    "        all_results = []\n",
    "        for approach, models in self.results.items():\n",
    "            for model_name, result in models.items():\n",
    "                if result:  # Skip failed experiments\n",
    "                    all_results.append({\n",
    "                        'approach': approach,\n",
    "                        'model': model_name,\n",
    "                        'accuracy': result['eval_accuracy'],\n",
    "                        'training_time': result['training_time'],\n",
    "                        'loss': result['eval_loss']\n",
    "                    })\n",
    "        \n",
    "        if not all_results:\n",
    "            print(\"   ‚ö†Ô∏è No successful experiments found\")\n",
    "            return report\n",
    "        \n",
    "        df_results = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Summary statistics\n",
    "        summary_stats = df_results.groupby('approach').agg({\n",
    "            'accuracy': ['mean', 'std', 'max', 'min'],\n",
    "            'training_time': ['mean', 'std'],\n",
    "            'loss': ['mean', 'std']\n",
    "        }).round(4)\n",
    "        \n",
    "        report['summary']['statistics'] = summary_stats.to_dict()\n",
    "        \n",
    "        # Best performing approach\n",
    "        best_approach = df_results.loc[df_results['accuracy'].idxmax(), 'approach']\n",
    "        best_accuracy = df_results['accuracy'].max()\n",
    "        \n",
    "        report['summary']['best_approach'] = best_approach\n",
    "        report['summary']['best_accuracy'] = best_accuracy\n",
    "        \n",
    "        # Statistical significance testing\n",
    "        approaches = df_results['approach'].unique()\n",
    "        if len(approaches) > 1:\n",
    "            print(\"   üßÆ Computing statistical significance...\")\n",
    "            \n",
    "            significance_results = {}\n",
    "            for i, approach1 in enumerate(approaches):\n",
    "                for approach2 in approaches[i+1:]:\n",
    "                    acc1 = df_results[df_results['approach'] == approach1]['accuracy']\n",
    "                    acc2 = df_results[df_results['approach'] == approach2]['accuracy']\n",
    "                    \n",
    "                    if len(acc1) > 1 and len(acc2) > 1:\n",
    "                        stat, p_value = stats.ttest_ind(acc1, acc2)\n",
    "                        significance_results[f\"{approach1}_vs_{approach2}\"] = {\n",
    "                            'statistic': stat,\n",
    "                            'p_value': p_value,\n",
    "                            'significant': p_value < 0.05\n",
    "                        }\n",
    "            \n",
    "            report['statistical_analysis']['significance_tests'] = significance_results\n",
    "        \n",
    "        # Efficiency analysis\n",
    "        efficiency_scores = []\n",
    "        for _, row in df_results.iterrows():\n",
    "            # Efficiency = Accuracy / (Training Time / 60)  # Accuracy per minute\n",
    "            efficiency = row['accuracy'] / max(row['training_time'] / 60, 0.1)\n",
    "            efficiency_scores.append({\n",
    "                'approach': row['approach'],\n",
    "                'model': row['model'],\n",
    "                'efficiency': efficiency\n",
    "            })\n",
    "        \n",
    "        df_efficiency = pd.DataFrame(efficiency_scores)\n",
    "        most_efficient = df_efficiency.loc[df_efficiency['efficiency'].idxmax()]\n",
    "        \n",
    "        report['summary']['most_efficient_approach'] = most_efficient['approach']\n",
    "        report['summary']['best_efficiency_score'] = most_efficient['efficiency']\n",
    "        \n",
    "        # Generate recommendations\n",
    "        recommendations = []\n",
    "        \n",
    "        if best_approach == 'explainability':\n",
    "            recommendations.append(\"üîç Explainability-driven approach shows superior performance. Consider integrating explainability insights into standard practice.\")\n",
    "        elif best_approach == 'hybrid':\n",
    "            recommendations.append(\"üîÄ Hybrid approach balances performance and efficiency well. Recommended for production use.\")\n",
    "        elif best_approach == 'standard':\n",
    "            recommendations.append(\"üîß Standard fine-tuning remains competitive. Explainability overhead may not justify improvements.\")\n",
    "        else:\n",
    "            recommendations.append(\"üèÅ Baseline approach performed best. Consider if more complex approaches are necessary.\")\n",
    "        \n",
    "        # Efficiency recommendation\n",
    "        if most_efficient['approach'] != best_approach:\n",
    "            recommendations.append(f\"‚ö° For time-constrained scenarios, consider {most_efficient['approach']} approach for best efficiency.\")\n",
    "        \n",
    "        # Model-specific insights\n",
    "        model_performance = df_results.groupby('model')['accuracy'].mean().sort_values(ascending=False)\n",
    "        best_model = model_performance.index[0]\n",
    "        recommendations.append(f\"ü§ñ {best_model} shows consistently strong performance across approaches.\")\n",
    "        \n",
    "        report['recommendations'] = recommendations\n",
    "        \n",
    "        # Save report\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        report_path = f\"comparative_analysis_report_{timestamp}.json\"\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"   üíæ Report saved to: {report_path}\")\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"\\nüìà COMPARATIVE ANALYSIS RESULTS\")\n",
    "        print(f\"=\" * 50)\n",
    "        print(f\"üèÜ Best Approach: {best_approach} (Accuracy: {best_accuracy:.4f})\")\n",
    "        print(f\"‚ö° Most Efficient: {most_efficient['approach']} (Score: {most_efficient['efficiency']:.2f})\")\n",
    "        print(f\"ü§ñ Best Model: {best_model}\")\n",
    "        print(f\"\\nüìã Key Recommendations:\")\n",
    "        for i, rec in enumerate(recommendations, 1):\n",
    "            print(f\"   {i}. {rec}\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "print(\"‚úÖ ComparativeFineTuningFramework class defined\")\n",
    "\n",
    "# Initialize the framework\n",
    "framework = ComparativeFineTuningFramework(config, logger)\n",
    "print(\"üî¨ Framework ready for comparative analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparativeFineTuningDashboard:\n",
    "    \"\"\"\n",
    "    Interactive dashboard for comparative fine-tuning experiments.\n",
    "    Provides GUI controls for running experiments and visualizing results.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, framework):\n",
    "        self.framework = framework\n",
    "        self.current_results = {}\n",
    "        \n",
    "        # Available models (will be populated from models directory)\n",
    "        self.available_models = self.get_available_models()\n",
    "        \n",
    "        # Create widgets\n",
    "        self.create_widgets()\n",
    "        \n",
    "        print(\"üéõÔ∏è ComparativeFineTuningDashboard initialized\")\n",
    "    \n",
    "    def get_available_models(self):\n",
    "        \"\"\"Get list of available models from the models directory.\"\"\"\n",
    "        models_dir = Path(self.framework.models_dir)\n",
    "        available = []\n",
    "        \n",
    "        if models_dir.exists():\n",
    "            for item in models_dir.iterdir():\n",
    "                if item.is_dir() and not item.name.startswith('.'):\n",
    "                    # Check if it's a valid model directory (has config.json)\n",
    "                    if (item / \"config.json\").exists():\n",
    "                        available.append(item.name)\n",
    "        \n",
    "        return sorted(available) if available else [\"distilbert-financial-sentiment\", \"finbert-tone-financial-sentiment\"]\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        \"\"\"Create interactive widgets for the dashboard.\"\"\"\n",
    "        \n",
    "        # Model selection\n",
    "        self.model_selector = widgets.SelectMultiple(\n",
    "            options=self.available_models,\n",
    "            value=[self.available_models[0]] if self.available_models else [],\n",
    "            description='Models:',\n",
    "            disabled=False,\n",
    "            layout=widgets.Layout(width='400px', height='100px')\n",
    "        )\n",
    "        \n",
    "        # Approach selection\n",
    "        self.approach_selector = widgets.SelectMultiple(\n",
    "            options=['baseline', 'standard', 'explainability', 'hybrid'],\n",
    "            value=['standard', 'explainability'],\n",
    "            description='Approaches:',\n",
    "            disabled=False,\n",
    "            layout=widgets.Layout(width='400px', height='100px')\n",
    "        )\n",
    "        \n",
    "        # Dataset selection\n",
    "        self.dataset_selector = widgets.Dropdown(\n",
    "            options=['FinancialPhraseBank', 'FinancialClassification', 'FinancialAuditor'],\n",
    "            value='FinancialPhraseBank',\n",
    "            description='Dataset:',\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        # Quick test mode\n",
    "        self.quick_test_mode = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Quick Test Mode (reduced epochs)',\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        # Run button\n",
    "        self.run_button = widgets.Button(\n",
    "            description='üöÄ Run Comparative Analysis',\n",
    "            disabled=False,\n",
    "            button_style='success',\n",
    "            layout=widgets.Layout(width='300px', height='40px')\n",
    "        )\n",
    "        \n",
    "        # Results visualization button\n",
    "        self.visualize_button = widgets.Button(\n",
    "            description='üìä Visualize Results',\n",
    "            disabled=True,\n",
    "            button_style='info',\n",
    "            layout=widgets.Layout(width='200px', height='40px')\n",
    "        )\n",
    "        \n",
    "        # Progress output\n",
    "        self.progress_output = widgets.Output()\n",
    "        \n",
    "        # Results output\n",
    "        self.results_output = widgets.Output()\n",
    "        \n",
    "        # Bind button clicks\n",
    "        self.run_button.on_click(self.run_analysis)\n",
    "        self.visualize_button.on_click(self.visualize_results)\n",
    "    \n",
    "    def display(self):\n",
    "        \"\"\"Display the dashboard interface.\"\"\"\n",
    "        \n",
    "        # Configuration section\n",
    "        config_box = widgets.VBox([\n",
    "            widgets.HTML(value=\"<h3>üî¨ Comparative Fine-Tuning Configuration</h3>\"),\n",
    "            widgets.HBox([\n",
    "                widgets.VBox([\n",
    "                    self.model_selector,\n",
    "                    self.dataset_selector\n",
    "                ]),\n",
    "                widgets.VBox([\n",
    "                    self.approach_selector,\n",
    "                    self.quick_test_mode\n",
    "                ])\n",
    "            ]),\n",
    "            widgets.HBox([self.run_button, self.visualize_button])\n",
    "        ])\\n        \n",
    "        # Output section\n",
    "        output_box = widgets.VBox([\n",
    "            widgets.HTML(value=\"<h3>üìä Experiment Progress</h3>\"),\n",
    "            self.progress_output,\n",
    "            widgets.HTML(value=\"<h3>üìà Results Summary</h3>\"),\n",
    "            self.results_output\n",
    "        ])\n",
    "        \n",
    "        # Main dashboard\n",
    "        dashboard = widgets.VBox([\n",
    "            config_box,\n",
    "            widgets.HTML(value=\"<hr>\"),\n",
    "            output_box\n",
    "        ])\n",
    "        \n",
    "        display(dashboard)\n",
    "        \n",
    "        # Show instructions\n",
    "        with self.progress_output:\n",
    "            print(\"üî¨ Welcome to Comparative Fine-Tuning Analysis!\")\n",
    "            print(\"‚îå\" + \"‚îÄ\" * 50 + \"‚îê\")\n",
    "            print(\"‚îÇ Instructions:                                  ‚îÇ\")\n",
    "            print(\"‚îÇ 1. Select models to compare                    ‚îÇ\") \n",
    "            print(\"‚îÇ 2. Choose fine-tuning approaches              ‚îÇ\")\n",
    "            print(\"‚îÇ 3. Select dataset                             ‚îÇ\")\n",
    "            print(\"‚îÇ 4. Enable Quick Test for faster experiments   ‚îÇ\")\n",
    "            print(\"‚îÇ 5. Click 'Run Comparative Analysis'           ‚îÇ\")\n",
    "            print(\"‚îî\" + \"‚îÄ\" * 50 + \"‚îò\")\n",
    "            print(\"\\nüìã Available Models:\")\n",
    "            for i, model in enumerate(self.available_models, 1):\n",
    "                print(f\"   {i}. {model}\")\n",
    "    \n",
    "    def run_analysis(self, button):\n",
    "        \"\"\"Run the comparative fine-tuning analysis.\"\"\"\n",
    "        \n",
    "        # Clear outputs\n",
    "        self.progress_output.clear_output()\n",
    "        self.results_output.clear_output()\n",
    "        \n",
    "        # Get selections\n",
    "        selected_models = list(self.model_selector.value)\n",
    "        selected_approaches = list(self.approach_selector.value)\n",
    "        selected_dataset = self.dataset_selector.value\n",
    "        quick_mode = self.quick_test_mode.value\n",
    "        \n",
    "        # Validation\n",
    "        if not selected_models:\n",
    "            with self.progress_output:\n",
    "                print(\"‚ùå Please select at least one model\")\n",
    "            return\n",
    "        \n",
    "        if not selected_approaches:\n",
    "            with self.progress_output:\n",
    "                print(\"‚ùå Please select at least one approach\")\n",
    "            return\n",
    "        \n",
    "        # Disable run button during execution\n",
    "        self.run_button.disabled = True\n",
    "        self.run_button.description = \"‚è≥ Running...\"\n",
    "        \n",
    "        try:\n",
    "            with self.progress_output:\n",
    "                print(f\"üöÄ Starting Comparative Analysis\")\n",
    "                print(f\"   üìã Models: {', '.join(selected_models)}\")\n",
    "                print(f\"   üîß Approaches: {', '.join(selected_approaches)}\")\n",
    "                print(f\"   üìä Dataset: {selected_dataset}\")\n",
    "                print(f\"   ‚ö° Quick Mode: {'Yes' if quick_mode else 'No'}\")\n",
    "                print(\"‚îÄ\" * 60)\n",
    "            \n",
    "            # Load data\n",
    "            if not self.framework.load_data(selected_dataset):\n",
    "                with self.progress_output:\n",
    "                    print(\"‚ùå Failed to load dataset\")\n",
    "                return\n",
    "            \n",
    "            # Apply quick mode adjustments if enabled\n",
    "            training_overrides = {}\n",
    "            if quick_mode:\n",
    "                training_overrides = {\n",
    "                    'num_train_epochs': 1,\n",
    "                    'eval_steps': 50,\n",
    "                    'save_steps': 100,\n",
    "                    'warmup_steps': 50\n",
    "                }\n",
    "                with self.progress_output:\n",
    "                    print(\"‚ö° Quick mode enabled - using reduced training parameters\")\n",
    "            \n",
    "            # Run comparative analysis with progress updates\n",
    "            def progress_callback(message):\n",
    "                with self.progress_output:\n",
    "                    print(message)\n",
    "            \n",
    "            # Custom run with progress feedback\n",
    "            total_experiments = len(selected_models) * len(selected_approaches)\n",
    "            completed = 0\n",
    "            \n",
    "            for model_name in selected_models:\n",
    "                with self.progress_output:\n",
    "                    print(f\"\\nü§ñ Processing model: {model_name}\")\n",
    "                \n",
    "                for approach in selected_approaches:\n",
    "                    completed += 1\n",
    "                    \n",
    "                    with self.progress_output:\n",
    "                        print(f\"   üîÑ Running {approach} approach... ({completed}/{total_experiments})\")\n",
    "                    \n",
    "                    try:\n",
    "                        if approach == 'baseline':\n",
    "                            result = self.framework.baseline_fine_tuning(model_name, training_overrides)\n",
    "                        elif approach == 'standard':\n",
    "                            result = self.framework.standard_fine_tuning(model_name, training_overrides)\n",
    "                        elif approach == 'explainability':\n",
    "                            result = self.framework.explainability_driven_fine_tuning(model_name, training_overrides)\n",
    "                        elif approach == 'hybrid':\n",
    "                            result = self.framework.hybrid_fine_tuning(model_name, training_overrides)\n",
    "                        \n",
    "                        if result:\n",
    "                            with self.progress_output:\n",
    "                                print(f\"      ‚úÖ Completed - Accuracy: {result['eval_accuracy']:.4f}\")\n",
    "                        else:\n",
    "                            with self.progress_output:\n",
    "                                print(f\"      ‚ùå Failed\")\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        with self.progress_output:\n",
    "                            print(f\"      ‚ö†Ô∏è Error: {str(e)[:100]}...\")\n",
    "            \n",
    "            # Generate report\n",
    "            with self.progress_output:\n",
    "                print(f\"\\nüìä Generating comprehensive report...\")\n",
    "            \n",
    "            report = self.framework.generate_comparison_report()\n",
    "            self.current_results = report\n",
    "            \n",
    "            # Display summary results\n",
    "            self.display_results_summary(report)\n",
    "            \n",
    "            # Enable visualization button\n",
    "            self.visualize_button.disabled = False\n",
    "            \n",
    "            with self.progress_output:\n",
    "                print(f\"\\nüéâ Analysis complete! Check results below.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            with self.progress_output:\n",
    "                print(f\"‚ùå Analysis failed: {e}\")\n",
    "            \n",
    "        finally:\n",
    "            # Re-enable run button\n",
    "            self.run_button.disabled = False\n",
    "            self.run_button.description = \"üöÄ Run Comparative Analysis\"\n",
    "    \n",
    "    def display_results_summary(self, report):\n",
    "        \"\"\"Display a summary of the results.\"\"\"\n",
    "        \n",
    "        with self.results_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            print(\"üìä COMPARATIVE ANALYSIS RESULTS\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            if 'best_approach' in report['summary']:\n",
    "                print(f\"üèÜ Best Approach: {report['summary']['best_approach']}\")\n",
    "                print(f\"   üìà Accuracy: {report['summary']['best_accuracy']:.4f}\")\n",
    "            \n",
    "            if 'most_efficient_approach' in report['summary']:\n",
    "                print(f\"‚ö° Most Efficient: {report['summary']['most_efficient_approach']}\")\n",
    "                print(f\"   üî¢ Efficiency Score: {report['summary']['best_efficiency_score']:.2f}\")\n",
    "            \n",
    "            print(f\"\\nüìã Key Recommendations:\")\n",
    "            for i, rec in enumerate(report.get('recommendations', []), 1):\n",
    "                print(f\"   {i}. {rec}\")\n",
    "            \n",
    "            # Statistics table\n",
    "            if 'statistics' in report['summary']:\n",
    "                print(f\"\\nüìä Performance Statistics:\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                stats_data = report['summary']['statistics']\n",
    "                if 'accuracy' in stats_data:\n",
    "                    print(\"Approach        | Avg Acc | Std Dev | Best    | Worst   |\")\n",
    "                    print(\"-\" * 60)\n",
    "                    \n",
    "                    for approach in stats_data['accuracy']['mean'].keys():\n",
    "                        avg_acc = stats_data['accuracy']['mean'][approach]\n",
    "                        std_acc = stats_data['accuracy']['std'][approach] if not pd.isna(stats_data['accuracy']['std'][approach]) else 0\n",
    "                        max_acc = stats_data['accuracy']['max'][approach]\n",
    "                        min_acc = stats_data['accuracy']['min'][approach]\n",
    "                        \n",
    "                        print(f\"{approach:<15} | {avg_acc:.4f} | {std_acc:.4f}  | {max_acc:.4f} | {min_acc:.4f} |\")\n",
    "    \n",
    "    def visualize_results(self, button):\n",
    "        \"\"\"Create visualizations of the comparative results.\"\"\"\n",
    "        \n",
    "        if not self.current_results or not self.current_results['detailed_results']:\n",
    "            with self.results_output:\n",
    "                print(\"‚ùå No results to visualize. Please run analysis first.\")\n",
    "            return\n",
    "        \n",
    "        # Prepare data for visualization\n",
    "        all_results = []\n",
    "        for approach, models in self.current_results['detailed_results'].items():\n",
    "            for model_name, result in models.items():\n",
    "                if result:\n",
    "                    all_results.append({\n",
    "                        'Approach': approach.title(),\n",
    "                        'Model': model_name,\n",
    "                        'Accuracy': result['eval_accuracy'],\n",
    "                        'Training Time (s)': result['training_time'],\n",
    "                        'Loss': result['eval_loss']\n",
    "                    })\n",
    "        \n",
    "        if not all_results:\n",
    "            with self.results_output:\n",
    "                print(\"‚ùå No successful experiments to visualize\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(all_results)\n",
    "        \n",
    "        # Clear output and create plots\n",
    "        with self.results_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Set up the plotting style\n",
    "            plt.style.use('seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else 'default')\n",
    "            \n",
    "            # Create subplots\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            fig.suptitle('üî¨ Comparative Fine-Tuning Analysis Results', fontsize=16, fontweight='bold')\n",
    "            \n",
    "            # 1. Accuracy comparison\n",
    "            ax1 = axes[0, 0]\n",
    "            sns.barplot(data=df, x='Approach', y='Accuracy', ax=ax1, palette='viridis')\n",
    "            ax1.set_title('üìà Accuracy by Approach', fontweight='bold')\n",
    "            ax1.set_ylabel('Accuracy')\n",
    "            ax1.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for i, v in enumerate(df.groupby('Approach')['Accuracy'].mean()):\n",
    "                ax1.text(i, v + 0.001, f'{v:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            # 2. Training time comparison\n",
    "            ax2 = axes[0, 1]\n",
    "            sns.barplot(data=df, x='Approach', y='Training Time (s)', ax=ax2, palette='plasma')\n",
    "            ax2.set_title('‚è±Ô∏è Training Time by Approach', fontweight='bold')\n",
    "            ax2.set_ylabel('Training Time (seconds)')\n",
    "            ax2.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # 3. Model comparison\n",
    "            ax3 = axes[1, 0]\n",
    "            sns.boxplot(data=df, x='Model', y='Accuracy', ax=ax3, palette='Set2')\n",
    "            ax3.set_title('ü§ñ Accuracy Distribution by Model', fontweight='bold')\n",
    "            ax3.set_ylabel('Accuracy')\n",
    "            ax3.tick_params(axis='x', rotation=45)\\n            \n",
    "            # 4. Efficiency scatter plot (Accuracy vs Time)\n",
    "            ax4 = axes[1, 1]\n",
    "            for approach in df['Approach'].unique():\n",
    "                approach_data = df[df['Approach'] == approach]\n",
    "                ax4.scatter(approach_data['Training Time (s)'], approach_data['Accuracy'], \n",
    "                          label=approach, s=100, alpha=0.7)\n",
    "            \n",
    "            ax4.set_xlabel('Training Time (seconds)')\n",
    "            ax4.set_ylabel('Accuracy')\n",
    "            ax4.set_title('‚ö° Efficiency Analysis (Accuracy vs Time)', fontweight='bold')\n",
    "            ax4.legend()\n",
    "            ax4.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Performance summary table\n",
    "            print(\"\\nüìä Detailed Performance Summary\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            summary_table = df.groupby('Approach').agg({\n",
    "                'Accuracy': ['mean', 'std', 'min', 'max'],\n",
    "                'Training Time (s)': ['mean', 'std'],\n",
    "                'Loss': ['mean', 'std']\n",
    "            }).round(4)\n",
    "            \n",
    "            print(summary_table.to_string())\n",
    "            \n",
    "            # Statistical significance if available\n",
    "            if 'significance_tests' in self.current_results.get('statistical_analysis', {}):\n",
    "                print(\"\\nüßÆ Statistical Significance Tests\")\n",
    "                print(\"-\" * 50)\n",
    "                \n",
    "                for comparison, test_result in self.current_results['statistical_analysis']['significance_tests'].items():\n",
    "                    significance = \"‚úÖ Significant\" if test_result['significant'] else \"‚ùå Not Significant\"\n",
    "                    print(f\"{comparison}: p-value = {test_result['p_value']:.4f} - {significance}\")\n",
    "\n",
    "print(\"‚úÖ ComparativeFineTuningDashboard class defined\")\n",
    "\n",
    "# Create the interactive dashboard\n",
    "dashboard = ComparativeFineTuningDashboard(framework)\n",
    "print(\"üéõÔ∏è Interactive dashboard ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Interactive Comparative Fine-Tuning Dashboard\n",
    "\n",
    "Use the dashboard below to run comprehensive comparative fine-tuning experiments. The dashboard allows you to:\n",
    "\n",
    "- **Select Multiple Models**: Compare different pre-trained models simultaneously\n",
    "- **Choose Approaches**: Run baseline, standard, explainability-driven, and hybrid fine-tuning\n",
    "- **Configure Experiments**: Select datasets and enable quick test mode\n",
    "- **Real-time Progress**: Monitor experiment progress with detailed logging\n",
    "- **Comprehensive Results**: Get statistical analysis and recommendations\n",
    "- **Interactive Visualizations**: Generate plots and performance comparisons\n",
    "\n",
    "### üìã Methodology Overview\n",
    "\n",
    "**üèÅ Baseline Approach**: Minimal fine-tuning with default parameters to establish a performance floor.\n",
    "\n",
    "**üîß Standard Approach**: Conventional fine-tuning with established best practices and early stopping.\n",
    "\n",
    "**üîç Explainability-Driven Approach**: Uses SHAP/LIME insights to identify difficult samples and adjust training accordingly.\n",
    "\n",
    "**üîÄ Hybrid Approach**: Combines standard and explainability-driven methods for balanced performance.\n",
    "\n",
    "### üìä Evaluation Dimensions\n",
    "\n",
    "- **Performance**: Accuracy, loss, and error analysis\n",
    "- **Efficiency**: Training time and resource utilization  \n",
    "- **Robustness**: Statistical significance and consistency\n",
    "- **Explainability**: Insight generation and model interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the interactive comparative fine-tuning dashboard\n",
    "dashboard.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Alternative: Programmatic Usage\n",
    "\n",
    "If you prefer to run experiments programmatically instead of using the interactive dashboard, you can use the framework directly:\n",
    "\n",
    "### Example: Compare All Approaches on Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic usage example - compare multiple models and approaches\n",
    "\"\"\"\n",
    "# Example 1: Full comparative analysis\n",
    "models_to_compare = [\n",
    "    'distilbert-financial-sentiment',\n",
    "    'finbert-tone-financial-sentiment',\n",
    "    'all-MiniLM-L6-v2-financial-sentiment'\n",
    "]\n",
    "\n",
    "approaches_to_test = ['baseline', 'standard', 'explainability', 'hybrid']\n",
    "\n",
    "# Load data\n",
    "framework.load_data('FinancialPhraseBank')\n",
    "\n",
    "# Run comprehensive analysis\n",
    "results = framework.run_comparative_analysis(\n",
    "    model_names=models_to_compare,\n",
    "    approaches=approaches_to_test\n",
    ")\n",
    "\n",
    "print(\"üìä Comparative analysis complete!\")\n",
    "print(f\"üèÜ Best performing approach: {results['summary']['best_approach']}\")\n",
    "\"\"\"\n",
    "\n",
    "# Example 2: Single model, multiple approaches\n",
    "\"\"\"\n",
    "# For focused analysis on one model\n",
    "model_name = 'distilbert-financial-sentiment'\n",
    "\n",
    "framework.load_data('FinancialPhraseBank')\n",
    "\n",
    "# Test different approaches\n",
    "baseline_result = framework.baseline_fine_tuning(model_name)\n",
    "standard_result = framework.standard_fine_tuning(model_name)\n",
    "explainability_result = framework.explainability_driven_fine_tuning(model_name)\n",
    "hybrid_result = framework.hybrid_fine_tuning(model_name)\n",
    "\n",
    "# Compare results\n",
    "results = [\n",
    "    ('Baseline', baseline_result['eval_accuracy'] if baseline_result else 0),\n",
    "    ('Standard', standard_result['eval_accuracy'] if standard_result else 0),\n",
    "    ('Explainability', explainability_result['eval_accuracy'] if explainability_result else 0),\n",
    "    ('Hybrid', hybrid_result['eval_accuracy'] if hybrid_result else 0)\n",
    "]\n",
    "\n",
    "print(\"üìà Single Model Comparison Results:\")\n",
    "for approach, accuracy in sorted(results, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"   {approach}: {accuracy:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "# Example 3: Quick test mode for rapid prototyping\n",
    "\"\"\"\n",
    "# Quick testing with reduced parameters\n",
    "quick_training_args = {\n",
    "    'num_train_epochs': 1,\n",
    "    'eval_steps': 50,\n",
    "    'save_steps': 100\n",
    "}\n",
    "\n",
    "framework.load_data('FinancialPhraseBank')\n",
    "\n",
    "# Test with quick parameters\n",
    "quick_result = framework.explainability_driven_fine_tuning(\n",
    "    'distilbert-financial-sentiment',\n",
    "    training_args_override=quick_training_args\n",
    ")\n",
    "\n",
    "print(f\"‚ö° Quick test result: {quick_result['eval_accuracy']:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìã Programmatic usage examples ready (uncomment to use)\")\n",
    "print(\"üí° Tip: Use the interactive dashboard above for easier experimentation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Academic Research Integration\n",
    "\n",
    "This comparative framework is designed to support academic research and paper writing. Here's how to leverage the results:\n",
    "\n",
    "### üî¨ Experimental Design\n",
    "\n",
    "The framework implements a **controlled experimental design** with:\n",
    "- **Independent Variables**: Fine-tuning approach (baseline, standard, explainability-driven, hybrid)\n",
    "- **Dependent Variables**: Accuracy, training time, loss, efficiency metrics\n",
    "- **Controls**: Same dataset splits, random seeds, model architectures\n",
    "- **Replication**: Multiple models tested with same approaches\n",
    "\n",
    "### üìä Statistical Validation\n",
    "\n",
    "The framework automatically computes:\n",
    "- **Descriptive Statistics**: Mean, standard deviation, min/max for each approach\n",
    "- **Statistical Significance**: T-tests between approach pairs (p < 0.05)\n",
    "- **Effect Size**: Practical significance of performance differences\n",
    "- **Confidence Intervals**: Reliability of performance estimates\n",
    "\n",
    "### üìà Key Research Questions Addressed\n",
    "\n",
    "1. **RQ1**: Does explainability-driven fine-tuning improve model performance compared to standard approaches?\n",
    "2. **RQ2**: What is the computational overhead of integrating explainability methods into fine-tuning?\n",
    "3. **RQ3**: Which combination of explainability insights and training parameters yields optimal results?\n",
    "4. **RQ4**: How does the effectiveness of different approaches vary across model architectures?\n",
    "\n",
    "### üìë Paper Sections Supported\n",
    "\n",
    "- **Methodology**: Detailed implementation of each fine-tuning approach\n",
    "- **Experimental Setup**: Controlled comparison framework\n",
    "- **Results**: Comprehensive performance analysis with statistical validation\n",
    "- **Discussion**: Insights from explainability-driven optimization\n",
    "- **Ablation Studies**: Component-wise analysis of hybrid approaches\n",
    "\n",
    "### üíæ Reproducibility\n",
    "\n",
    "All experiments generate:\n",
    "- **Configuration Files**: Complete training arguments and hyperparameters\n",
    "- **Random Seeds**: Fixed for reproducible results\n",
    "- **Detailed Logs**: Step-by-step training progress\n",
    "- **Raw Results**: JSON format for further analysis\n",
    "- **Statistical Reports**: Ready for publication tables\n",
    "\n",
    "### üîç Novel Contributions\n",
    "\n",
    "This framework contributes:\n",
    "1. **Systematic Integration** of explainability methods into fine-tuning workflows\n",
    "2. **Comparative Analysis** of traditional vs. explainability-driven approaches  \n",
    "3. **Efficiency Metrics** balancing performance and computational cost\n",
    "4. **Academic Validation** with statistical significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultsAnalyzer:\n",
    "    \"\"\"\n",
    "    Utility class for analyzing and exporting comparative fine-tuning results\n",
    "    for academic research and publication.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, framework):\n",
    "        self.framework = framework\n",
    "        self.results = framework.results\n",
    "    \n",
    "    def export_results_to_csv(self, filename=\"comparative_results.csv\"):\n",
    "        \"\"\"Export results to CSV format for statistical analysis.\"\"\"\n",
    "        \n",
    "        all_results = []\n",
    "        for approach, models in self.results.items():\n",
    "            for model_name, result in models.items():\n",
    "                if result:\n",
    "                    all_results.append({\n",
    "                        'Approach': approach,\n",
    "                        'Model': model_name,\n",
    "                        'Accuracy': result['eval_accuracy'],\n",
    "                        'Loss': result['eval_loss'],\n",
    "                        'Training_Time_s': result['training_time'],\n",
    "                        'Epochs': result['training_args'].get('num_train_epochs', 'N/A'),\n",
    "                        'Learning_Rate': result['training_args'].get('learning_rate', 'N/A'),\n",
    "                        'Batch_Size': result['training_args'].get('per_device_train_batch_size', 'N/A')\n",
    "                    })\n",
    "        \n",
    "        if all_results:\n",
    "            df = pd.DataFrame(all_results)\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"üìä Results exported to {filename}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"‚ùå No results to export\")\n",
    "            return None\n",
    "    \n",
    "    def generate_latex_table(self, metric='Accuracy', round_digits=4):\n",
    "        \"\"\"Generate LaTeX table for academic papers.\"\"\"\n",
    "        \n",
    "        df = self.export_results_to_csv()\n",
    "        if df is None:\n",
    "            return None\n",
    "        \n",
    "        # Create pivot table\n",
    "        pivot = df.pivot_table(values=metric, index='Model', columns='Approach', aggfunc='mean')\n",
    "        \n",
    "        # Generate LaTeX\n",
    "        latex_code = \"\\\\begin{table}[h]\\n\"\n",
    "        latex_code += \"\\\\centering\\n\"\n",
    "        latex_code += f\"\\\\caption{{Comparative Fine-Tuning Results: {metric}}}\\n\"\n",
    "        latex_code += \"\\\\label{tab:comparative_results}\\n\"\n",
    "        \n",
    "        # Table structure\n",
    "        num_cols = len(pivot.columns) + 1\n",
    "        latex_code += f\"\\\\begin{{tabular}}{{'|l' + '|c' * (num_cols-1) + '|'}}\\n\"\n",
    "        latex_code += \"\\\\hline\\n\"\n",
    "        \n",
    "        # Header\n",
    "        latex_code += \"Model & \" + \" & \".join(pivot.columns) + \" \\\\\\\\\\n\"\n",
    "        latex_code += \"\\\\hline\\n\"\n",
    "        \n",
    "        # Data rows\n",
    "        for model in pivot.index:\n",
    "            row = [model.replace('_', '\\\\_')]\n",
    "            for approach in pivot.columns:\n",
    "                value = pivot.loc[model, approach]\n",
    "                if pd.isna(value):\n",
    "                    row.append(\"N/A\")\n",
    "                else:\n",
    "                    row.append(f\"{value:.{round_digits}f}\")\n",
    "            latex_code += \" & \".join(row) + \" \\\\\\\\\\n\"\n",
    "        \n",
    "        latex_code += \"\\\\hline\\n\"\n",
    "        latex_code += \"\\\\end{tabular}\\n\"\n",
    "        latex_code += \"\\\\end{table}\\n\"\n",
    "        \n",
    "        print(\"üìÑ LaTeX table generated:\")\n",
    "        print(latex_code)\n",
    "        \n",
    "        # Save to file\n",
    "        with open(\"comparative_results_table.tex\", \"w\") as f:\n",
    "            f.write(latex_code)\n",
    "        \n",
    "        return latex_code\n",
    "    \n",
    "    def statistical_analysis_report(self):\n",
    "        \"\"\"Generate comprehensive statistical analysis report.\"\"\"\n",
    "        \n",
    "        df = self.export_results_to_csv()\n",
    "        if df is None:\n",
    "            return None\n",
    "        \n",
    "        print(\"üìä STATISTICAL ANALYSIS REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Descriptive statistics\n",
    "        print(\"\\n1. DESCRIPTIVE STATISTICS\")\n",
    "        print(\"-\" * 30)\n",
    "        desc_stats = df.groupby('Approach')['Accuracy'].describe()\n",
    "        print(desc_stats.round(4))\n",
    "        \n",
    "        # ANOVA test\n",
    "        print(\"\\n2. ANALYSIS OF VARIANCE (ANOVA)\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        approaches = df['Approach'].unique()\n",
    "        if len(approaches) > 2:\n",
    "            groups = [df[df['Approach'] == approach]['Accuracy'].values for approach in approaches]\n",
    "            \n",
    "            try:\n",
    "                f_stat, p_value = stats.f_oneway(*groups)\n",
    "                print(f\"F-statistic: {f_stat:.4f}\")\n",
    "                print(f\"p-value: {p_value:.6f}\")\n",
    "                print(f\"Significant: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "            except Exception as e:\n",
    "                print(f\"ANOVA failed: {e}\")\n",
    "        \n",
    "        # Pairwise t-tests\n",
    "        print(\"\\n3. PAIRWISE T-TESTS\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for i, approach1 in enumerate(approaches):\n",
    "            for approach2 in approaches[i+1:]:\n",
    "                group1 = df[df['Approach'] == approach1]['Accuracy']\n",
    "                group2 = df[df['Approach'] == approach2]['Accuracy']\n",
    "                \n",
    "                if len(group1) > 1 and len(group2) > 1:\n",
    "                    try:\n",
    "                        t_stat, p_val = stats.ttest_ind(group1, group2)\n",
    "                        significance = \"***\" if p_val < 0.001 else \"**\" if p_val < 0.01 else \"*\" if p_val < 0.05 else \"ns\"\n",
    "                        print(f\"{approach1} vs {approach2}: t={t_stat:.4f}, p={p_val:.6f} {significance}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"{approach1} vs {approach2}: Test failed - {e}\")\n",
    "        \n",
    "        # Effect sizes (Cohen's d)\n",
    "        print(\"\\n4. EFFECT SIZES (COHEN'S d)\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        def cohens_d(group1, group2):\n",
    "            n1, n2 = len(group1), len(group2)\n",
    "            pooled_std = np.sqrt(((n1-1)*group1.std()**2 + (n2-1)*group2.std()**2) / (n1+n2-2))\n",
    "            return (group1.mean() - group2.mean()) / pooled_std\n",
    "        \n",
    "        for i, approach1 in enumerate(approaches):\n",
    "            for approach2 in approaches[i+1:]:\n",
    "                group1 = df[df['Approach'] == approach1]['Accuracy']\n",
    "                group2 = df[df['Approach'] == approach2]['Accuracy']\n",
    "                \n",
    "                if len(group1) > 1 and len(group2) > 1:\n",
    "                    try:\n",
    "                        d = cohens_d(group1, group2)\n",
    "                        magnitude = \"Large\" if abs(d) > 0.8 else \"Medium\" if abs(d) > 0.5 else \"Small\"\n",
    "                        print(f\"{approach1} vs {approach2}: d={d:.4f} ({magnitude})\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"{approach1} vs {approach2}: Effect size calculation failed - {e}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üìù Legend: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")\n",
    "        \n",
    "        return desc_stats\n",
    "    \n",
    "    def create_publication_plots(self):\n",
    "        \"\"\"Create publication-ready plots.\"\"\"\n",
    "        \n",
    "        df = self.export_results_to_csv()\n",
    "        if df is None:\n",
    "            return\n",
    "        \n",
    "        # Set publication style\n",
    "        plt.rcParams.update({\n",
    "            'font.size': 12,\n",
    "            'font.family': 'serif',\n",
    "            'axes.linewidth': 1.5,\n",
    "            'axes.spines.top': False,\n",
    "            'axes.spines.right': False,\n",
    "            'xtick.major.size': 5,\n",
    "            'ytick.major.size': 5,\n",
    "            'legend.frameon': False\n",
    "        })\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Plot 1: Box plot of accuracy by approach\n",
    "        sns.boxplot(data=df, x='Approach', y='Accuracy', ax=axes[0], palette='Set2')\n",
    "        axes[0].set_title('(A) Accuracy Distribution by Fine-tuning Approach', fontweight='bold')\n",
    "        axes[0].set_xlabel('Fine-tuning Approach')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot 2: Training efficiency scatter\n",
    "        sns.scatterplot(data=df, x='Training_Time_s', y='Accuracy', \n",
    "                       hue='Approach', s=100, alpha=0.8, ax=axes[1])\n",
    "        axes[1].set_title('(B) Training Efficiency Analysis', fontweight='bold')\n",
    "        axes[1].set_xlabel('Training Time (seconds)')\n",
    "        axes[1].set_ylabel('Accuracy')\n",
    "        axes[1].legend(title='Approach', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # Plot 3: Model comparison\n",
    "        model_means = df.groupby(['Model', 'Approach'])['Accuracy'].mean().unstack()\n",
    "        model_means.plot(kind='bar', ax=axes[2], width=0.8)\n",
    "        axes[2].set_title('(C) Model Performance Comparison', fontweight='bold')\n",
    "        axes[2].set_xlabel('Model')\n",
    "        axes[2].set_ylabel('Mean Accuracy')\n",
    "        axes[2].tick_params(axis='x', rotation=45)\n",
    "        axes[2].legend(title='Approach', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('comparative_analysis_publication.png', dpi=300, bbox_inches='tight')\n",
    "        plt.savefig('comparative_analysis_publication.pdf', bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"üìä Publication plots saved as PNG and PDF\")\n",
    "    \n",
    "    def export_for_paper(self):\n",
    "        \"\"\"Export all materials needed for academic paper.\"\"\"\n",
    "        \n",
    "        print(\"üìÑ Exporting materials for academic paper...\")\n",
    "        \n",
    "        # Export CSV data\n",
    "        df = self.export_results_to_csv(\"paper_results.csv\")\n",
    "        \n",
    "        # Generate LaTeX table\n",
    "        self.generate_latex_table()\n",
    "        \n",
    "        # Statistical analysis\n",
    "        stats_report = self.statistical_analysis_report()\n",
    "        \n",
    "        # Publication plots\n",
    "        self.create_publication_plots()\n",
    "        \n",
    "        # Export raw results as JSON\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        raw_results_file = f\"raw_results_{timestamp}.json\"\n",
    "        \n",
    "        with open(raw_results_file, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Academic materials exported:\")\n",
    "        print(f\"   üìä CSV Data: paper_results.csv\")\n",
    "        print(f\"   üìÑ LaTeX Table: comparative_results_table.tex\")\n",
    "        print(f\"   üñºÔ∏è Publication Plots: comparative_analysis_publication.png/pdf\")\n",
    "        print(f\"   üìÅ Raw Results: {raw_results_file}\")\n",
    "        \n",
    "        return {\n",
    "            'csv_data': df,\n",
    "            'latex_table': 'comparative_results_table.tex',\n",
    "            'plots': 'comparative_analysis_publication.png',\n",
    "            'raw_results': raw_results_file\n",
    "        }\n",
    "\n",
    "# Create results analyzer\n",
    "analyzer = ResultsAnalyzer(framework)\n",
    "print(\"üìä ResultsAnalyzer ready for academic export!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary & Next Steps\n",
    "\n",
    "### üî¨ What This Notebook Provides\n",
    "\n",
    "**‚úÖ Comprehensive Comparative Framework**: Four distinct fine-tuning approaches (baseline, standard, explainability-driven, hybrid) with systematic evaluation.\n",
    "\n",
    "**‚úÖ Interactive Dashboard**: User-friendly interface for running experiments with real-time progress monitoring and results visualization.\n",
    "\n",
    "**‚úÖ Academic Research Support**: Statistical analysis, publication-ready plots, LaTeX table generation, and reproducible experimental design.\n",
    "\n",
    "**‚úÖ Explainability Integration**: Novel methodology using SHAP/LIME insights to guide fine-tuning optimization decisions.\n",
    "\n",
    "### üöÄ Usage Workflow\n",
    "\n",
    "1. **üîß Setup**: Run the import and initialization cells to prepare the framework\n",
    "2. **üéõÔ∏è Interactive Mode**: Use the dashboard for guided experimentation \n",
    "3. **üìä Analysis**: Run comparative experiments across multiple models and approaches\n",
    "4. **üìà Visualization**: Generate comprehensive plots and statistical reports\n",
    "5. **üìÑ Export**: Create publication-ready materials for academic papers\n",
    "\n",
    "### üîÆ Next Steps for Research\n",
    "\n",
    "**üìù Paper Writing**: Use the generated statistical reports, LaTeX tables, and publication plots to support your academic paper on explainability-driven fine-tuning.\n",
    "\n",
    "**üîç Deeper Analysis**: Investigate which specific explainability insights (difficult samples, feature importance, etc.) contribute most to performance improvements.\n",
    "\n",
    "**üèóÔ∏è Framework Extension**: Add more explainability methods (Integrated Gradients, Attention visualization) or fine-tuning techniques (LoRA, AdaLoRA).\n",
    "\n",
    "**üìä Broader Evaluation**: Test on additional datasets, model architectures, and domains to validate the generalizability of explainability-driven approaches.\n",
    "\n",
    "### üí° Key Research Contributions\n",
    "\n",
    "This framework enables you to demonstrate:\n",
    "\n",
    "- **Novel Methodology**: Systematic integration of explainability methods into fine-tuning workflows\n",
    "- **Empirical Validation**: Controlled experiments with statistical significance testing\n",
    "- **Practical Impact**: Balance between performance improvement and computational efficiency\n",
    "- **Reproducible Science**: Complete experimental pipeline with detailed logging and exports\n",
    "\n",
    "### üéâ Ready for Research!\n",
    "\n",
    "Your comparative fine-tuning framework is now complete and ready to support your academic research. The combination of rigorous methodology, comprehensive analysis, and academic integration tools provides a solid foundation for investigating explainability-driven optimization in transformer fine-tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
