{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔄 ONNX Conversion Pipeline - Generalized\n",
    "\n",
    "This notebook converts trained PyTorch models to ONNX format for optimized inference.\n",
    "\n",
    "**Key Features:**\n",
    "- Automatic model discovery and conversion\n",
    "- Configuration-driven optimization settings\n",
    "- Quantization and optimization support\n",
    "- Validation of converted models\n",
    "- Comprehensive conversion reporting\n",
    "\n",
    "**Configuration-driven approach:** All settings loaded from `../config/pipeline_config.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 13:31:46,899 - pipeline.onnx_conversion - INFO - 🔄 Starting ONNX Conversion - Generalized Pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Configuration loaded from ../config/pipeline_config.json\n"
     ]
    }
   ],
   "source": [
    "# Import configuration system and utilities\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.pipeline_utils import ConfigManager, StateManager, LoggingManager\n",
    "import torch\n",
    "import torch.onnx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# ONNX and optimization imports\n",
    "try:\n",
    "    import onnx\n",
    "    import onnxruntime as ort\n",
    "    from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType\n",
    "    onnx_available = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ ONNX or ONNXRuntime not installed. Install with: pip install onnx onnxruntime\")\n",
    "    onnx_available = False\n",
    "\n",
    "# Hugging Face transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Suppress excessive logging\n",
    "logging.getLogger(\"onnxruntime\").setLevel(logging.ERROR)\n",
    "\n",
    "# Initialize managers\n",
    "config = ConfigManager(\"../config/pipeline_config.json\")\n",
    "state = StateManager(\"../config/pipeline_state.json\")\n",
    "logger_manager = LoggingManager(config, 'onnx_conversion')\n",
    "logger = logger_manager.get_logger()\n",
    "\n",
    "logger.info(\"🔄 Starting ONNX Conversion - Generalized Pipeline\")\n",
    "print(\"📋 Configuration loaded from ../config/pipeline_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 13:31:46,925 - pipeline.onnx_conversion - INFO - 🔍 Checking ONNX conversion prerequisites...\n",
      "2025-08-09 13:31:46,941 - pipeline.onnx_conversion - INFO - Successfully discovered 8 trained models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prerequisites verification passed\n",
      "🔄 ONNX Configuration:\n",
      "   📊 Enabled: True\n",
      "   🔧 Opset version: 11\n",
      "   ⚡ Optimization level: all\n",
      "   🔍 Validate conversion: True\n",
      "\n",
      "📂 Discovering Trained Models:\n",
      "   📁 Models directory: ../models\n",
      "   ✅ Found: tinybert-financial-classifier-fine-tuned\n",
      "   ✅ Found: all-MiniLM-L6-v2-financial-sentiment\n",
      "   ✅ Found: distilbert-financial-sentiment\n",
      "   ✅ Found: finbert-tone-financial-sentiment\n",
      "   ✅ Found: SmolLM2-360M-Instruct-financial-sentiment\n",
      "   ✅ Found: tinybert-financial-classifier\n",
      "   ✅ Found: tinybert-financial-classifier-pruned\n",
      "   ✅ Found: mobilebert-uncased-financial-sentiment\n",
      "   📊 Total models found: 8\n"
     ]
    }
   ],
   "source": [
    "# Verify prerequisites and discover models\n",
    "logger.info(\"🔍 Checking ONNX conversion prerequisites...\")\n",
    "\n",
    "if not onnx_available:\n",
    "    raise ImportError(\"ONNX libraries are required. Install with: pip install onnx onnxruntime\")\n",
    "\n",
    "# Verify model training was completed\n",
    "if not state.is_step_complete('model_training_completed'):\n",
    "    logger.error(\"Model training step not completed. Please run 2_train_models_generalized.ipynb first.\")\n",
    "    raise RuntimeError(\"Model training required. Run 2_train_models_generalized.ipynb first.\")\n",
    "\n",
    "print(\"✅ Prerequisites verification passed\")\n",
    "\n",
    "# Load ONNX configuration\n",
    "onnx_config = config.get('onnx_conversion', {})\n",
    "models_config = config.get('models', {})\n",
    "\n",
    "print(f\"🔄 ONNX Configuration:\")\n",
    "print(f\"   📊 Enabled: {onnx_config.get('enabled', True)}\")\n",
    "print(f\"   🔧 Opset version: {onnx_config.get('opset_version', 11)}\")\n",
    "print(f\"   ⚡ Optimization level: {onnx_config.get('optimization_level', 'all')}\")\n",
    "print(f\"   🔍 Validate conversion: {onnx_config.get('validate_conversion', True)}\")\n",
    "\n",
    "# Discover trained models\n",
    "models_dir = Path(f\"../{models_config.get('output_dir', 'models')}\")\n",
    "print(f\"\\n📂 Discovering Trained Models:\")\n",
    "print(f\"   📁 Models directory: {models_dir}\")\n",
    "\n",
    "available_models = []\n",
    "if models_dir.exists():\n",
    "    for model_path in models_dir.iterdir():\n",
    "        if model_path.is_dir() and not model_path.name.startswith('.'):\n",
    "            # Check if it's a valid model directory\n",
    "            config_file = model_path / \"config.json\"\n",
    "            model_files = list(model_path.glob(\"*.safetensors\")) + list(model_path.glob(\"pytorch_model.bin\"))\n",
    "            \n",
    "            if config_file.exists() and model_files:\n",
    "                available_models.append({\n",
    "                    'name': model_path.name,\n",
    "                    'path': model_path,\n",
    "                    'config_file': config_file,\n",
    "                    'model_files': model_files\n",
    "                })\n",
    "                print(f\"   ✅ Found: {model_path.name}\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ Invalid model directory: {model_path.name}\")\n",
    "\n",
    "print(f\"   📊 Total models found: {len(available_models)}\")\n",
    "\n",
    "if len(available_models) == 0:\n",
    "    logger.error(\"No trained models found\")\n",
    "    raise RuntimeError(\"No trained models found. Please run 2_train_models_generalized.ipynb first.\")\n",
    "\n",
    "logger.info(f\"Successfully discovered {len(available_models)} trained models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 ONNX Converter initialized\n"
     ]
    }
   ],
   "source": [
    "# ONNX Conversion Utilities\n",
    "class ONNXConverter:\n",
    "    \"\"\"Handles ONNX conversion for transformer models\"\"\"\n",
    "    \n",
    "    def __init__(self, onnx_config: Dict, logger):\n",
    "        self.onnx_config = onnx_config\n",
    "        self.logger = logger\n",
    "        self.opset_version = onnx_config.get('opset_version', 11)\n",
    "        self.optimization_level = onnx_config.get('optimization_level', 'all')\n",
    "        self.dynamic_axes = onnx_config.get('dynamic_axes', {\n",
    "            'input_ids': {'0': 'batch_size'},\n",
    "            'attention_mask': {'0': 'batch_size'},\n",
    "            'logits': {'0': 'batch_size'}\n",
    "        })\n",
    "        \n",
    "    def create_sample_inputs(self, tokenizer, max_length: int = 128) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Create sample inputs for ONNX export\"\"\"\n",
    "        sample_text = \"This is a sample financial text for model conversion.\"\n",
    "        \n",
    "        # Tokenize sample text\n",
    "        inputs = tokenizer(\n",
    "            sample_text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return inputs['input_ids'], inputs['attention_mask']\n",
    "    \n",
    "    def convert_to_onnx(self, model, tokenizer, model_name: str, output_path: Path) -> bool:\n",
    "        \"\"\"Convert PyTorch model to ONNX format\"\"\"\n",
    "        try:\n",
    "            # Set model to evaluation mode\n",
    "            model.eval()\n",
    "            \n",
    "            # Create sample inputs\n",
    "            input_ids, attention_mask = self.create_sample_inputs(tokenizer)\n",
    "            \n",
    "            # Define input and output names\n",
    "            input_names = ['input_ids', 'attention_mask']\n",
    "            output_names = ['logits']\n",
    "            \n",
    "            self.logger.info(f\"Converting {model_name} to ONNX...\")\n",
    "            \n",
    "            # Export to ONNX\n",
    "            torch.onnx.export(\n",
    "                model,\n",
    "                (input_ids, attention_mask),\n",
    "                str(output_path),\n",
    "                input_names=input_names,\n",
    "                output_names=output_names,\n",
    "                dynamic_axes=self.dynamic_axes,\n",
    "                opset_version=self.opset_version,\n",
    "                do_constant_folding=True,\n",
    "                export_params=True,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to convert {model_name}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def validate_onnx_model(self, onnx_path: Path, tokenizer, original_model) -> Dict:\n",
    "        \"\"\"Validate ONNX model by comparing outputs with original\"\"\"\n",
    "        try:\n",
    "            # Load ONNX model\n",
    "            ort_session = ort.InferenceSession(str(onnx_path))\n",
    "            \n",
    "            # Create sample inputs\n",
    "            input_ids, attention_mask = self.create_sample_inputs(tokenizer)\n",
    "            \n",
    "            # Get original PyTorch outputs\n",
    "            with torch.no_grad():\n",
    "                original_outputs = original_model(input_ids, attention_mask)\n",
    "                original_logits = original_outputs.logits.numpy()\n",
    "            \n",
    "            # Get ONNX outputs\n",
    "            onnx_inputs = {\n",
    "                'input_ids': input_ids.numpy(),\n",
    "                'attention_mask': attention_mask.numpy()\n",
    "            }\n",
    "            onnx_outputs = ort_session.run(None, onnx_inputs)\n",
    "            onnx_logits = onnx_outputs[0]\n",
    "            \n",
    "            # Compare outputs\n",
    "            max_diff = np.max(np.abs(original_logits - onnx_logits))\n",
    "            mean_diff = np.mean(np.abs(original_logits - onnx_logits))\n",
    "            \n",
    "            # Check if outputs are similar (tolerance for floating point differences)\n",
    "            is_valid = max_diff < 1e-4\n",
    "            \n",
    "            return {\n",
    "                'valid': is_valid,\n",
    "                'max_diff': float(max_diff),\n",
    "                'mean_diff': float(mean_diff),\n",
    "                'original_shape': original_logits.shape,\n",
    "                'onnx_shape': onnx_logits.shape\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Validation failed: {str(e)}\")\n",
    "            return {'valid': False, 'error': str(e)}\n",
    "    \n",
    "    def optimize_onnx_model(self, onnx_path: Path) -> Optional[Path]:\n",
    "        \"\"\"Apply ONNX optimizations\"\"\"\n",
    "        try:\n",
    "            if self.optimization_level == 'none':\n",
    "                return None\n",
    "                \n",
    "            # Load and optimize model\n",
    "            model = onnx.load(str(onnx_path))\n",
    "            \n",
    "            # Apply basic optimizations\n",
    "            from onnxruntime.transformers import optimizer\n",
    "            optimized_model = optimizer.optimize_model(\n",
    "                str(onnx_path),\n",
    "                model_type='bert',\n",
    "                num_heads=0,  # Auto-detect\n",
    "                hidden_size=0  # Auto-detect\n",
    "            )\n",
    "            \n",
    "            # Save optimized model\n",
    "            optimized_path = onnx_path.parent / f\"{onnx_path.stem}_optimized.onnx\"\n",
    "            optimized_model.save_model_to_file(str(optimized_path))\n",
    "            \n",
    "            return optimized_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Optimization failed: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Initialize converter\n",
    "converter = ONNXConverter(onnx_config, logger)\n",
    "print(\"🔧 ONNX Converter initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 13:31:46,983 - pipeline.onnx_conversion - INFO - 🚀 Starting ONNX conversion process...\n",
      "2025-08-09 13:31:46,985 - pipeline.onnx_conversion - INFO - Starting conversion for tinybert-financial-classifier-fine-tuned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Converting Models to ONNX:\n",
      "============================================================\n",
      "\n",
      "🤖 Converting tinybert-financial-classifier-fine-tuned:\n",
      "   📁 Output path: ../models/tinybert-financial-classifier-fine-tuned/onnx/tinybert-financial-classifier-fine-tuned.onnx\n",
      "   🔄 Loading PyTorch model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 13:31:49,136 - pipeline.onnx_conversion - INFO - Converting tinybert-financial-classifier-fine-tuned to ONNX...\n"
     ]
    }
   ],
   "source": [
    "# Main ONNX Conversion Loop\n",
    "logger.info(\"🚀 Starting ONNX conversion process...\")\n",
    "\n",
    "conversion_results = {}\n",
    "successful_conversions = 0\n",
    "failed_conversions = 0\n",
    "\n",
    "print(f\"\\n🔄 Converting Models to ONNX:\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for model_info in available_models:\n",
    "    model_name = model_info['name']\n",
    "    model_path = model_info['path']\n",
    "    \n",
    "    print(f\"\\n🤖 Converting {model_name}:\")\n",
    "    logger.info(f\"Starting conversion for {model_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Create ONNX output directory\n",
    "        onnx_dir = model_path / \"onnx\"\n",
    "        onnx_dir.mkdir(exist_ok=True)\n",
    "        onnx_path = onnx_dir / f\"{model_name}.onnx\"\n",
    "        \n",
    "        print(f\"   📁 Output path: {onnx_path}\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        print(f\"   🔄 Loading PyTorch model...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(str(model_path))\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(str(model_path))\n",
    "        \n",
    "        # Convert to ONNX\n",
    "        print(f\"   ⚡ Converting to ONNX...\")\n",
    "        conversion_success = converter.convert_to_onnx(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            model_name=model_name,\n",
    "            output_path=onnx_path\n",
    "        )\n",
    "        \n",
    "        if not conversion_success:\n",
    "            print(f\"   ❌ Conversion failed\")\n",
    "            failed_conversions += 1\n",
    "            continue\n",
    "        \n",
    "        # Validate conversion\n",
    "        validation_results = {}\n",
    "        if onnx_config.get('validate_conversion', True):\n",
    "            print(f\"   🔍 Validating ONNX model...\")\n",
    "            validation_results = converter.validate_onnx_model(\n",
    "                onnx_path=onnx_path,\n",
    "                tokenizer=tokenizer,\n",
    "                original_model=model\n",
    "            )\n",
    "            \n",
    "            if validation_results.get('valid', False):\n",
    "                print(f\"   ✅ Validation passed (max diff: {validation_results['max_diff']:.2e})\")\n",
    "            else:\n",
    "                print(f\"   ⚠️ Validation issues detected\")\n",
    "        \n",
    "        # Apply optimizations\n",
    "        optimized_path = None\n",
    "        if onnx_config.get('optimization_level', 'all') != 'none':\n",
    "            print(f\"   ⚡ Applying optimizations...\")\n",
    "            try:\n",
    "                optimized_path = converter.optimize_onnx_model(onnx_path)\n",
    "                if optimized_path:\n",
    "                    print(f\"   ✅ Optimized model saved to: {optimized_path.name}\")\n",
    "                else:\n",
    "                    print(f\"   ⚠️ Optimization skipped\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Optimization failed: {str(e)}\")\n",
    "        \n",
    "        # Get model file sizes\n",
    "        original_size = sum(f.stat().st_size for f in model_info['model_files']) / (1024 * 1024)  # MB\n",
    "        onnx_size = onnx_path.stat().st_size / (1024 * 1024)  # MB\n",
    "        optimized_size = optimized_path.stat().st_size / (1024 * 1024) if optimized_path else None\n",
    "        \n",
    "        # Store results\n",
    "        conversion_results[model_name] = {\n",
    "            'conversion_successful': True,\n",
    "            'onnx_path': str(onnx_path.relative_to(Path(\"../\"))),\n",
    "            'optimized_path': str(optimized_path.relative_to(Path(\"../\"))) if optimized_path else None,\n",
    "            'validation': validation_results,\n",
    "            'file_sizes': {\n",
    "                'original_mb': round(original_size, 2),\n",
    "                'onnx_mb': round(onnx_size, 2),\n",
    "                'optimized_mb': round(optimized_size, 2) if optimized_size else None,\n",
    "                'compression_ratio': round(original_size / onnx_size, 2) if onnx_size > 0 else None\n",
    "            },\n",
    "            'model_config': {\n",
    "                'opset_version': converter.opset_version,\n",
    "                'optimization_level': converter.optimization_level,\n",
    "                'dynamic_axes': list(converter.dynamic_axes.keys())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        successful_conversions += 1\n",
    "        print(f\"   ✅ Conversion completed successfully!\")\n",
    "        print(f\"   📊 Original: {original_size:.1f}MB → ONNX: {onnx_size:.1f}MB\")\n",
    "        if optimized_size:\n",
    "            print(f\"   📊 Optimized: {optimized_size:.1f}MB\")\n",
    "        \n",
    "        # Clean up memory\n",
    "        del model, tokenizer\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        logger.info(f\"Successfully converted {model_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to convert {model_name}: {str(e)}\")\n",
    "        print(f\"   ❌ Conversion failed: {str(e)}\")\n",
    "        \n",
    "        conversion_results[model_name] = {\n",
    "            'conversion_successful': False,\n",
    "            'error': str(e)\n",
    "        }\n",
    "        failed_conversions += 1\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"🎉 ONNX Conversion completed!\")\n",
    "print(f\"   ✅ Successful: {successful_conversions} models\")\n",
    "print(f\"   ❌ Failed: {failed_conversions} models\")\n",
    "\n",
    "logger.info(f\"ONNX conversion completed: {successful_conversions} successful, {failed_conversions} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save conversion results and complete ONNX conversion step\n",
    "logger.info(\"💾 Saving ONNX conversion results...\")\n",
    "\n",
    "# Create comprehensive conversion summary\n",
    "conversion_summary = {\n",
    "    'conversion_timestamp': datetime.now().isoformat(),\n",
    "    'models_processed': len(available_models),\n",
    "    'successful_conversions': successful_conversions,\n",
    "    'failed_conversions': failed_conversions,\n",
    "    'onnx_config': onnx_config,\n",
    "    'conversion_results': conversion_results,\n",
    "    'summary_statistics': {\n",
    "        'total_original_size_mb': sum(\n",
    "            result.get('file_sizes', {}).get('original_mb', 0) \n",
    "            for result in conversion_results.values() \n",
    "            if result.get('conversion_successful', False)\n",
    "        ),\n",
    "        'total_onnx_size_mb': sum(\n",
    "            result.get('file_sizes', {}).get('onnx_mb', 0) \n",
    "            for result in conversion_results.values() \n",
    "            if result.get('conversion_successful', False)\n",
    "        ),\n",
    "        'average_compression_ratio': np.mean([\n",
    "            result.get('file_sizes', {}).get('compression_ratio', 1) \n",
    "            for result in conversion_results.values() \n",
    "            if result.get('conversion_successful', False) and \n",
    "               result.get('file_sizes', {}).get('compression_ratio') is not None\n",
    "        ]) if successful_conversions > 0 else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Update pipeline state\n",
    "state.mark_step_complete('onnx_conversion_completed', **conversion_summary)\n",
    "\n",
    "# Save conversion report\n",
    "results_dir = Path(\"../results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(results_dir / 'onnx_conversion_report.json', 'w') as f:\n",
    "    json.dump(conversion_summary, f, indent=2)\n",
    "\n",
    "# Create conversion results visualization\n",
    "if successful_conversions > 0:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Prepare data for visualization\n",
    "    successful_results = {\n",
    "        name: result for name, result in conversion_results.items() \n",
    "        if result.get('conversion_successful', False)\n",
    "    }\n",
    "    \n",
    "    if len(successful_results) > 0:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Subplot 1: File Size Comparison\n",
    "        plt.subplot(1, 3, 1)\n",
    "        model_names = list(successful_results.keys())\n",
    "        original_sizes = [successful_results[m]['file_sizes']['original_mb'] for m in model_names]\n",
    "        onnx_sizes = [successful_results[m]['file_sizes']['onnx_mb'] for m in model_names]\n",
    "        \n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, original_sizes, width, label='Original', alpha=0.8)\n",
    "        plt.bar(x + width/2, onnx_sizes, width, label='ONNX', alpha=0.8)\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Size (MB)')\n",
    "        plt.title('Model Size Comparison')\n",
    "        plt.xticks(x, [name[:15] + '...' if len(name) > 15 else name for name in model_names], rotation=45)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Subplot 2: Compression Ratios\n",
    "        plt.subplot(1, 3, 2)\n",
    "        compression_ratios = [\n",
    "            successful_results[m]['file_sizes'].get('compression_ratio', 1) \n",
    "            for m in model_names\n",
    "        ]\n",
    "        plt.bar(model_names, compression_ratios, alpha=0.8)\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Compression Ratio')\n",
    "        plt.title('ONNX Compression Ratios')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.axhline(y=1, color='r', linestyle='--', alpha=0.5, label='No Compression')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Subplot 3: Validation Results\n",
    "        if any(successful_results[m].get('validation', {}).get('max_diff') is not None for m in model_names):\n",
    "            plt.subplot(1, 3, 3)\n",
    "            max_diffs = [\n",
    "                successful_results[m].get('validation', {}).get('max_diff', 0)\n",
    "                for m in model_names\n",
    "            ]\n",
    "            plt.bar(model_names, max_diffs, alpha=0.8)\n",
    "            plt.xlabel('Models')\n",
    "            plt.ylabel('Max Output Difference')\n",
    "            plt.title('ONNX Validation Results')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(results_dir / 'onnx_conversion_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"🎉 ONNX CONVERSION COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"📝 Next Steps:\")\n",
    "print(\"1. Run 4_benchmarks_generalized.ipynb to benchmark ONNX performance\")\n",
    "print(\"2. Continue with the sequential pipeline: 5 → 6\")\n",
    "\n",
    "print(f\"\\n🔄 Conversion Summary:\")\n",
    "print(f\"   🤖 Models processed: {len(available_models)}\")\n",
    "print(f\"   ✅ Successful conversions: {successful_conversions}\")\n",
    "print(f\"   ❌ Failed conversions: {failed_conversions}\")\n",
    "\n",
    "if successful_conversions > 0:\n",
    "    total_original = conversion_summary['summary_statistics']['total_original_size_mb']\n",
    "    total_onnx = conversion_summary['summary_statistics']['total_onnx_size_mb']\n",
    "    avg_compression = conversion_summary['summary_statistics']['average_compression_ratio']\n",
    "    \n",
    "    print(f\"   📊 Total size reduction: {total_original:.1f}MB → {total_onnx:.1f}MB\")\n",
    "    print(f\"   📈 Average compression ratio: {avg_compression:.2f}x\")\n",
    "    \n",
    "    print(f\"\\n📁 ONNX Models Location:\")\n",
    "    for model_name, result in successful_results.items():\n",
    "        print(f\"   📂 {model_name}: {result['onnx_path']}\")\n",
    "\n",
    "print(f\"\\n📄 Conversion report saved to: {results_dir / 'onnx_conversion_report.json'}\")\n",
    "if successful_conversions > 0:\n",
    "    print(f\"📊 Comparison charts saved to: {results_dir / 'onnx_conversion_comparison.png'}\")\n",
    "\n",
    "logger.info(\"✅ ONNX conversion completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
