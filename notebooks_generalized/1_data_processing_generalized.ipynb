{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🔄 Data Processing Pipeline - Generalized\n",
        "\n",
        "This notebook handles data loading, preprocessing, and preparation.\n",
        "\n",
        "**Configuration-driven approach:** All settings loaded from `../config/pipeline_config.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 Configuration loaded from ../config/pipeline_config.json\n"
          ]
        }
      ],
      "source": [
        "# Import configuration system and utilities\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "from src.pipeline_utils import ConfigManager, StateManager, LoggingManager\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize managers\n",
        "config = ConfigManager(\"../config/pipeline_config.json\")\n",
        "state = StateManager(\"../config/pipeline_state.json\")\n",
        "logger_manager = LoggingManager(config, 'data_processing')\n",
        "logger = logger_manager.get_logger()\n",
        "\n",
        "print(\"📋 Configuration loaded from ../config/pipeline_config.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-08 16:29:01,928 - pipeline.data_processing - INFO - 🔍 Checking pipeline prerequisites...\n",
            "2025-08-08 16:29:01,930 - pipeline.data_processing - INFO - Data configuration loaded successfully\n",
            "2025-08-08 16:29:01,930 - pipeline.data_processing - INFO - Data configuration loaded successfully\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Setup verification passed\n",
            "📊 Data Configuration:\n",
            "   📁 Raw data path: data/FinancialPhraseBank/all-data.csv\n",
            "   📁 Processed data dir: data/processed\n",
            "   📈 Validation split: 0.1\n"
          ]
        }
      ],
      "source": [
        "# Verify setup completion and load data configuration\n",
        "logger.info(\"🔍 Checking pipeline prerequisites...\")\n",
        "\n",
        "# Verify setup was completed\n",
        "if not state.is_step_complete('setup_completed'):\n",
        "    logger.error(\"Setup step not completed. Please run 0_setup_generalized.ipynb first.\")\n",
        "    raise RuntimeError(\"Pipeline setup required. Run 0_setup_generalized.ipynb first.\")\n",
        "\n",
        "print(\"✅ Setup verification passed\")\n",
        "\n",
        "# Load data configuration\n",
        "data_config = config.get('data', {})\n",
        "print(f\"📊 Data Configuration:\")\n",
        "print(f\"   📁 Raw data path: {data_config.get('raw_data_path', 'Not set')}\")\n",
        "print(f\"   📁 Processed data dir: {data_config.get('processed_data_dir', 'Not set')}\")\n",
        "print(f\"   📈 Validation split: {data_config.get('validation_split', 'Not set')}\")\n",
        "\n",
        "logger.info(\"Data configuration loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-08 16:29:02,386 - pipeline.data_processing - INFO - 📂 Loading raw dataset...\n",
            "2025-08-08 16:29:02,406 - pipeline.data_processing - INFO - Successfully loaded dataset with 4846 samples\n",
            "2025-08-08 16:29:02,406 - pipeline.data_processing - INFO - Successfully loaded dataset with 4846 samples\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📂 Loading Raw Dataset:\n",
            "   ✅ Loaded CSV: 4846 samples\n",
            "   📋 Columns: ['label', 'text']\n",
            "   📝 Sample: label='neutral', text='According to Gran , the company has no plans to mo...'\n"
          ]
        }
      ],
      "source": [
        "# Load and validate raw dataset\n",
        "import pandas as pd\n",
        "\n",
        "logger.info(\"📂 Loading raw dataset...\")\n",
        "\n",
        "print(\"📂 Loading Raw Dataset:\")\n",
        "\n",
        "# Get the main data path from configuration\n",
        "raw_data_path = data_config.get('raw_data_path', '')\n",
        "if not raw_data_path:\n",
        "    logger.error(\"No raw data path configured\")\n",
        "    raise RuntimeError(\"No raw data path found in configuration\")\n",
        "\n",
        "full_path = Path(f\"../{raw_data_path}\")\n",
        "\n",
        "if not full_path.exists():\n",
        "    logger.error(f\"Data file not found: {full_path}\")\n",
        "    raise FileNotFoundError(f\"Data file not found: {full_path}\")\n",
        "\n",
        "# Load the dataset\n",
        "if full_path.suffix.lower() == '.csv':\n",
        "    df = pd.read_csv(full_path, encoding='unicode_escape', names=['label', 'text'])\n",
        "    print(f\"   ✅ Loaded CSV: {len(df)} samples\")\n",
        "    print(f\"   📋 Columns: {list(df.columns)}\")\n",
        "    \n",
        "    # Display sample\n",
        "    if len(df) > 0:\n",
        "        print(f\"   📝 Sample: label='{df.iloc[0]['label']}', text='{df.iloc[0]['text'][:50]}...'\")\n",
        "    \n",
        "else:\n",
        "    logger.error(f\"Unsupported file format: {full_path.suffix}\")\n",
        "    raise ValueError(f\"Unsupported file format: {full_path.suffix}\")\n",
        "\n",
        "logger.info(f\"Successfully loaded dataset with {len(df)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-08 16:29:02,417 - pipeline.data_processing - INFO - 🔄 Processing and standardizing dataset...\n",
            "2025-08-08 16:29:02,434 - pipeline.data_processing - INFO - Successfully processed dataset\n",
            "2025-08-08 16:29:02,434 - pipeline.data_processing - INFO - Successfully processed dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Data Preprocessing:\n",
            "   📋 Data structure recognized: label column = 'label', text column = 'text'\n",
            "   🧹 Cleaning text data...\n",
            "   🔄 Stripped quotes from text\n",
            "   🏷️ Standardizing labels...\n",
            "   📊 Label distribution:\n",
            "      neutral: 2879 samples (59.4%)\n",
            "      positive: 1363 samples (28.1%)\n",
            "      negative: 604 samples (12.5%)\n",
            "   ✅ Processed 4846 samples\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing and standardization\n",
        "logger.info(\"🔄 Processing and standardizing dataset...\")\n",
        "\n",
        "print(\"🔄 Data Preprocessing:\")\n",
        "\n",
        "# Get label columns configuration\n",
        "label_columns = data_config.get('label_columns', ['label', 'sentence'])\n",
        "text_preprocessing = data_config.get('text_preprocessing', {})\n",
        "\n",
        "# Check if we have the expected columns\n",
        "if len(df.columns) >= 2:\n",
        "    # Data is already in correct format: first column = label, second column = text\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    print(f\"   📋 Data structure recognized: label column = '{df.columns[0]}', text column = '{df.columns[1]}'\")\n",
        "    \n",
        "    # Clean and preprocess text\n",
        "    print(f\"   🧹 Cleaning text data...\")\n",
        "    df_processed['text'] = df_processed['text'].astype(str)\n",
        "    df_processed['text'] = df_processed['text'].str.strip()\n",
        "    \n",
        "    # Remove quotes if configured\n",
        "    if text_preprocessing.get('strip_quotes', True):\n",
        "        df_processed['text'] = df_processed['text'].str.strip('\"\\'')\n",
        "        print(f\"   🔄 Stripped quotes from text\")\n",
        "    \n",
        "    # Remove empty texts\n",
        "    before_count = len(df_processed)\n",
        "    df_processed = df_processed[df_processed['text'].str.len() > 0]\n",
        "    after_count = len(df_processed)\n",
        "    \n",
        "    if before_count != after_count:\n",
        "        print(f\"   🗑️ Removed {before_count - after_count} empty texts\")\n",
        "    \n",
        "    # Standardize labels\n",
        "    print(f\"   🏷️ Standardizing labels...\")\n",
        "    df_processed['label'] = df_processed['label'].astype(str).str.lower().str.strip()\n",
        "    \n",
        "    # Display label distribution\n",
        "    label_counts = df_processed['label'].value_counts()\n",
        "    print(f\"   📊 Label distribution:\")\n",
        "    for label, count in label_counts.items():\n",
        "        print(f\"      {label}: {count} samples ({count/len(df_processed)*100:.1f}%)\")\n",
        "    \n",
        "    # Add metadata\n",
        "    df_processed['processed_timestamp'] = datetime.now().isoformat()\n",
        "    \n",
        "    print(f\"   ✅ Processed {len(df_processed)} samples\")\n",
        "    logger.info(\"Successfully processed dataset\")\n",
        "    \n",
        "else:\n",
        "    logger.error(\"Dataset does not have enough columns\")\n",
        "    raise ValueError(\"Dataset must have at least 2 columns (text and label)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-08 16:29:02,451 - pipeline.data_processing - INFO - 📊 Splitting data into train/validation sets...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 Data Splitting:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-08 16:29:03,151 - pipeline.data_processing - INFO - Successfully split dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   📋 Splitting configuration:\n",
            "      📈 Validation split: 0.1\n",
            "      🎲 Random seed: 42\n",
            "   📈 Stratified split applied\n",
            "   📊 Train set: 4361 samples\n",
            "   📊 Validation set: 485 samples\n",
            "   🏷️ Train label distribution:\n",
            "      neutral: 2591 (59.4%)\n",
            "      positive: 1227 (28.1%)\n",
            "      negative: 543 (12.5%)\n",
            "   🏷️ Validation label distribution:\n",
            "      neutral: 288 (59.4%)\n",
            "      positive: 136 (28.0%)\n",
            "      negative: 61 (12.6%)\n"
          ]
        }
      ],
      "source": [
        "# Data splitting and final preparation\n",
        "logger.info(\"📊 Splitting data into train/validation sets...\")\n",
        "\n",
        "print(\"📊 Data Splitting:\")\n",
        "\n",
        "# Import sklearn only when needed\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Get splitting configuration\n",
        "validation_split = data_config.get('validation_split', 0.1)\n",
        "random_seed = config.get('pipeline.random_seed', 42)\n",
        "\n",
        "print(f\"   📋 Splitting configuration:\")\n",
        "print(f\"      📈 Validation split: {validation_split}\")\n",
        "print(f\"      🎲 Random seed: {random_seed}\")\n",
        "\n",
        "# Stratified split to maintain label distribution\n",
        "# Check if stratified split is possible (each class needs at least 2 samples)\n",
        "label_counts = df_processed['label'].value_counts()\n",
        "min_class_count = label_counts.min()\n",
        "\n",
        "if len(df_processed['label'].unique()) > 1 and min_class_count >= 2:\n",
        "    try:\n",
        "        train_df, val_df = train_test_split(\n",
        "            df_processed,\n",
        "            test_size=validation_split,\n",
        "            random_state=random_seed,\n",
        "            stratify=df_processed['label']\n",
        "        )\n",
        "        print(f\"   📈 Stratified split applied\")\n",
        "    except ValueError:\n",
        "        # Fall back to random split if stratification fails\n",
        "        train_df, val_df = train_test_split(\n",
        "            df_processed,\n",
        "            test_size=validation_split,\n",
        "            random_state=random_seed\n",
        "        )\n",
        "        print(f\"   📈 Random split applied (stratification failed)\")\n",
        "else:\n",
        "    train_df, val_df = train_test_split(\n",
        "        df_processed,\n",
        "        test_size=validation_split,\n",
        "        random_state=random_seed\n",
        "    )\n",
        "    if len(df_processed['label'].unique()) == 1:\n",
        "        print(f\"   📈 Random split applied (single label)\")\n",
        "    else:\n",
        "        print(f\"   📈 Random split applied (insufficient samples for stratification)\")\n",
        "        print(f\"       Minimum class count: {min_class_count}, need at least 2\")\n",
        "\n",
        "print(f\"   📊 Train set: {len(train_df)} samples\")\n",
        "print(f\"   📊 Validation set: {len(val_df)} samples\")\n",
        "\n",
        "# Display label distribution in splits\n",
        "print(f\"   🏷️ Train label distribution:\")\n",
        "train_labels = train_df['label'].value_counts()\n",
        "for label, count in train_labels.items():\n",
        "    print(f\"      {label}: {count} ({count/len(train_df)*100:.1f}%)\")\n",
        "\n",
        "print(f\"   🏷️ Validation label distribution:\")\n",
        "val_labels = val_df['label'].value_counts()\n",
        "for label, count in val_labels.items():\n",
        "    print(f\"      {label}: {count} ({count/len(val_df)*100:.1f}%)\")\n",
        "\n",
        "logger.info(\"Successfully split dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-08 16:29:03,163 - pipeline.data_processing - INFO - 💾 Saving processed datasets...\n",
            "2025-08-08 16:29:03,210 - pipeline.data_processing - INFO - Saved processed datasets\n",
            "2025-08-08 16:29:03,213 - pipeline.data_processing - INFO - ✅ Data processing completed successfully\n",
            "2025-08-08 16:29:03,210 - pipeline.data_processing - INFO - Saved processed datasets\n",
            "2025-08-08 16:29:03,213 - pipeline.data_processing - INFO - ✅ Data processing completed successfully\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💾 Saving Processed Data:\n",
            "   ✅ Dataset saved:\n",
            "      📁 Full dataset: ../data/processed/full_processed.csv\n",
            "      📁 Train set: ../data/processed/train.csv\n",
            "      📁 Validation set: ../data/processed/validation.csv\n",
            "\n",
            "============================================================\n",
            "🎉 DATA PROCESSING COMPLETED SUCCESSFULLY!\n",
            "============================================================\n",
            "📝 Next Steps:\n",
            "1. Run 2_train_models_generalized.ipynb to train the models\n",
            "2. Continue with the sequential pipeline: 3 → 4 → 5 → 6\n",
            "\n",
            "📊 Processing Summary:\n",
            "   📋 Total samples: 4846\n",
            "   📊 Train samples: 4361\n",
            "   📊 Validation samples: 485\n",
            "   🏷️ Labels: ['neutral', 'positive', 'negative']\n",
            "\n",
            "📄 Processing report saved to: ../results/data_processing_report.json\n"
          ]
        }
      ],
      "source": [
        "# Save processed datasets and complete data processing step\n",
        "import json\n",
        "\n",
        "logger.info(\"💾 Saving processed datasets...\")\n",
        "\n",
        "print(\"💾 Saving Processed Data:\")\n",
        "\n",
        "# Get processed data directory from configuration\n",
        "processed_data_dir = data_config.get('processed_data_dir', 'data/processed')\n",
        "processed_dir = Path(f\"../{processed_data_dir}\")\n",
        "processed_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Save train, validation, and full datasets\n",
        "train_path = processed_dir / \"train.csv\"\n",
        "val_path = processed_dir / \"validation.csv\"\n",
        "full_path = processed_dir / \"full_processed.csv\"\n",
        "\n",
        "train_df.to_csv(train_path, index=False)\n",
        "val_df.to_csv(val_path, index=False)\n",
        "df_processed.to_csv(full_path, index=False)\n",
        "\n",
        "print(f\"   ✅ Dataset saved:\")\n",
        "print(f\"      📁 Full dataset: {full_path}\")\n",
        "print(f\"      📁 Train set: {train_path}\")\n",
        "print(f\"      📁 Validation set: {val_path}\")\n",
        "\n",
        "logger.info(\"Saved processed datasets\")\n",
        "\n",
        "# Create data processing summary\n",
        "processing_summary = {\n",
        "    'processing_timestamp': datetime.now().isoformat(),\n",
        "    'total_samples': len(df_processed),\n",
        "    'train_samples': len(train_df),\n",
        "    'validation_samples': len(val_df),\n",
        "    'labels': df_processed['label'].value_counts().to_dict(),\n",
        "    'validation_split': len(val_df) / len(df_processed),\n",
        "    'data_paths': {\n",
        "        'train': str(train_path),\n",
        "        'validation': str(val_path),\n",
        "        'full': str(full_path)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Update pipeline state\n",
        "state.mark_step_complete('data_processing_completed', **processing_summary)\n",
        "\n",
        "# Save processing report\n",
        "results_dir = Path(\"../results\")\n",
        "results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "with open(results_dir / 'data_processing_report.json', 'w') as f:\n",
        "    json.dump(processing_summary, f, indent=2)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"🎉 DATA PROCESSING COMPLETED SUCCESSFULLY!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(\"📝 Next Steps:\")\n",
        "print(\"1. Run 2_train_models_generalized.ipynb to train the models\")\n",
        "print(\"2. Continue with the sequential pipeline: 3 → 4 → 5 → 6\")\n",
        "\n",
        "print(f\"\\n📊 Processing Summary:\")\n",
        "print(f\"   📋 Total samples: {processing_summary['total_samples']}\")\n",
        "print(f\"   📊 Train samples: {processing_summary['train_samples']}\")\n",
        "print(f\"   📊 Validation samples: {processing_summary['validation_samples']}\")\n",
        "print(f\"   🏷️ Labels: {list(processing_summary['labels'].keys())}\")\n",
        "\n",
        "print(f\"\\n📄 Processing report saved to: {results_dir / 'data_processing_report.json'}\")\n",
        "\n",
        "logger.info(\"✅ Data processing completed successfully\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
