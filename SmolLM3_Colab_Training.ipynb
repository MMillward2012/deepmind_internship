{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df5ee27c",
   "metadata": {},
   "source": [
    "# 🚀 SmolLM3 Financial Sentiment Training - Google Colab\n",
    "\n",
    "This notebook trains SmolLM3-3B on financial sentiment data in Google Colab.\n",
    "\n",
    "**Optimized for Colab T4 GPU (15GB VRAM)**\n",
    "\n",
    "## 📋 Instructions:\n",
    "1. **Upload your data**: Upload `all-data.csv` to Colab Files\n",
    "2. **Run all cells**: Execute cells sequentially\n",
    "3. **Download model**: Get the trained model ZIP file\n",
    "\n",
    "## ⚡ Why Colab for SmolLM3:\n",
    "- **Memory**: 15GB GPU vs 9GB MacBook MPS limit\n",
    "- **Speed**: GPU training ~10-50x faster than CPU\n",
    "- **Free**: No local resource usage\n",
    "- **No thermal throttling**: Unlike laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f4e85",
   "metadata": {},
   "source": [
    "## 🔧 Environment Setup\n",
    "\n",
    "Install required packages and check GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f66cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets accelerate torch -q\n",
    "!pip install scikit-learn matplotlib seaborn pandas numpy -q\n",
    "\n",
    "print(\"✅ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd0bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and environment\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"🖥️ Environment Check:\")\n",
    "print(f\"Python version: {os.sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"✅ GPU: {gpu_name}\")\n",
    "    print(f\"💾 GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    if gpu_memory >= 12:\n",
    "        print(\"🎉 Perfect! Sufficient memory for SmolLM3-3B training\")\n",
    "    else:\n",
    "        print(\"⚠️ Limited GPU memory - will use aggressive optimizations\")\n",
    "else:\n",
    "    print(\"❌ No GPU detected - training will be very slow!\")\n",
    "    response = input(\"Continue with CPU training? (y/N): \")\n",
    "    if response.lower() != 'y':\n",
    "        raise RuntimeError(\"GPU required for efficient SmolLM3 training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ca589",
   "metadata": {},
   "source": [
    "## 📊 Data Loading and Preprocessing\n",
    "\n",
    "Load and prepare financial sentiment data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ece8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Try to find the data file in common locations\n",
    "data_paths = [\n",
    "    \"/content/all-data.csv\",\n",
    "    \"/content/data/FinancialPhraseBank/all-data.csv\",\n",
    "    \"all-data.csv\",\n",
    "    \"/content/drive/MyDrive/all-data.csv\"\n",
    "]\n",
    "\n",
    "data_df = None\n",
    "for path in data_paths:\n",
    "    if Path(path).exists():\n",
    "        print(f\"📂 Found data at: {path}\")\n",
    "        try:\n",
    "            data_df = pd.read_csv(path, \n",
    "                                names=['text', 'label'], \n",
    "                                encoding='utf-8', \n",
    "                                on_bad_lines='skip')\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "if data_df is None:\n",
    "    print(\"❌ No data file found!\")\n",
    "    print(\"📤 Please upload your 'all-data.csv' file to Colab:\")\n",
    "    print(\"   1. Click the Files tab (📁) on the left\")\n",
    "    print(\"   2. Click Upload (📤)\")\n",
    "    print(\"   3. Select your all-data.csv file\")\n",
    "    print(\"   4. Re-run this cell\")\n",
    "    raise FileNotFoundError(\"Data file not found\")\n",
    "\n",
    "print(f\"✅ Loaded {len(data_df)} samples\")\n",
    "print(f\"📊 Data shape: {data_df.shape}\")\n",
    "print(f\"🔍 Sample data:\")\n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11c01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess data\n",
    "print(\"🧹 Cleaning data...\")\n",
    "\n",
    "# Remove any rows with missing data\n",
    "initial_size = len(data_df)\n",
    "data_df = data_df.dropna()\n",
    "print(f\"   Removed {initial_size - len(data_df)} rows with missing data\")\n",
    "\n",
    "# Ensure text and labels are strings\n",
    "data_df['text'] = data_df['text'].astype(str)\n",
    "data_df['label'] = data_df['label'].astype(str)\n",
    "\n",
    "# Check unique labels\n",
    "unique_labels = data_df['label'].unique()\n",
    "print(f\"🏷️ Found labels: {unique_labels}\")\n",
    "\n",
    "# Map sentiment labels to integers\n",
    "if set(unique_labels).issubset({'positive', 'neutral', 'negative'}):\n",
    "    # Standard sentiment labels\n",
    "    label_mapping = {\n",
    "        'negative': 0,\n",
    "        'neutral': 1, \n",
    "        'positive': 2\n",
    "    }\n",
    "else:\n",
    "    # Auto-create mapping for other label formats\n",
    "    sorted_labels = sorted(unique_labels)\n",
    "    label_mapping = {label: i for i, label in enumerate(sorted_labels)}\n",
    "    print(f\"🔄 Auto-mapped labels: {label_mapping}\")\n",
    "\n",
    "data_df['label_id'] = data_df['label'].map(label_mapping)\n",
    "data_df = data_df.dropna(subset=['label_id'])\n",
    "data_df['label_id'] = data_df['label_id'].astype(int)\n",
    "\n",
    "print(f\"✅ Final dataset: {len(data_df)} samples\")\n",
    "print(f\"📊 Label distribution:\")\n",
    "label_counts = data_df['label_id'].value_counts().sort_index()\n",
    "for label_id, count in label_counts.items():\n",
    "    label_name = [k for k, v in label_mapping.items() if v == label_id][0]\n",
    "    print(f\"   {label_name} ({label_id}): {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6919ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split\n",
    "print(\"🔄 Creating train/validation split...\")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    data_df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=data_df['label_id']\n",
    ")\n",
    "\n",
    "print(f\"📊 Split complete:\")\n",
    "print(f\"   Training: {len(train_df)} samples\")\n",
    "print(f\"   Validation: {len(val_df)} samples\")\n",
    "\n",
    "# Visualize data distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Training distribution\n",
    "train_counts = train_df['label_id'].value_counts().sort_index()\n",
    "axes[0].bar(train_counts.index, train_counts.values)\n",
    "axes[0].set_title('Training Set Distribution')\n",
    "axes[0].set_xlabel('Label')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Validation distribution\n",
    "val_counts = val_df['label_id'].value_counts().sort_index()\n",
    "axes[1].bar(val_counts.index, val_counts.values)\n",
    "axes[1].set_title('Validation Set Distribution')\n",
    "axes[1].set_xlabel('Label')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Data preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01263501",
   "metadata": {},
   "source": [
    "## 🤖 SmolLM3 Model Setup\n",
    "\n",
    "Load and configure SmolLM3-3B for financial sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a4ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "MODEL_ID = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "MODEL_NAME = \"smollm3-financial-sentiment\"\n",
    "NUM_LABELS = len(label_mapping)\n",
    "\n",
    "print(f\"🚀 Loading SmolLM3 model: {MODEL_ID}\")\n",
    "print(f\"📊 Number of labels: {NUM_LABELS}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"📥 Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Ensure pad token exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"🔧 Set pad_token to eos_token\")\n",
    "\n",
    "print(f\"✅ Tokenizer loaded: {len(tokenizer)} tokens in vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2122551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(\"📥 Loading SmolLM3 model...\")\n",
    "print(\"⏳ This may take a few minutes (downloading ~6GB)...\")\n",
    "\n",
    "# Create reverse mapping for model\n",
    "id2label = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=id2label,\n",
    "    label2id=label_mapping,\n",
    "    torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "    device_map=\"auto\"  # Automatically place on GPU\n",
    ")\n",
    "\n",
    "print(f\"✅ Model loaded: {model.config.__class__.__name__}\")\n",
    "print(f\"📊 Parameters: {model.num_parameters():,}\")\n",
    "print(f\"💾 Model size: ~{model.num_parameters() * 2 / 1e9:.1f}GB (FP16)\")\n",
    "\n",
    "# Check model device\n",
    "device = next(model.parameters()).device\n",
    "print(f\"🔧 Model device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29222654",
   "metadata": {},
   "source": [
    "## 🔄 Dataset Preparation\n",
    "\n",
    "Tokenize and prepare datasets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a5d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "MAX_LENGTH = 512  # SmolLM3 can handle longer sequences\n",
    "BATCH_SIZE = 2    # Conservative for 15GB GPU\n",
    "GRADIENT_ACCUMULATION = 8  # Effective batch size = 16\n",
    "LEARNING_RATE = 1e-5  # Lower for large model\n",
    "NUM_EPOCHS = 2    # Quick training\n",
    "\n",
    "print(f\"⚙️ Training Configuration:\")\n",
    "print(f\"   Max length: {MAX_LENGTH}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Gradient accumulation: {GRADIENT_ACCUMULATION}\")\n",
    "print(f\"   Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "def create_dataset(df):\n",
    "    \"\"\"Create HuggingFace dataset from pandas DataFrame.\"\"\"\n",
    "    dataset = Dataset.from_pandas(\n",
    "        df[['text', 'label_id']].rename(columns={'label_id': 'labels'})\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=['text'],\n",
    "        num_proc=1\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "print(\"🔄 Tokenizing datasets...\")\n",
    "train_dataset = create_dataset(train_df)\n",
    "val_dataset = create_dataset(val_df)\n",
    "\n",
    "print(f\"✅ Datasets created:\")\n",
    "print(f\"   Training: {len(train_dataset)} samples\")\n",
    "print(f\"   Validation: {len(val_dataset)} samples\")\n",
    "print(f\"   Features: {train_dataset.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc32f1",
   "metadata": {},
   "source": [
    "## 🏋️ Model Training\n",
    "\n",
    "Train SmolLM3 on financial sentiment data with GPU optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24129657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training arguments\n",
    "output_dir = f\"/content/{MODEL_NAME}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=None,  # Disable wandb\n",
    "    fp16=True,  # Enable mixed precision\n",
    "    dataloader_num_workers=2,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "print(\"🔧 Training setup complete!\")\n",
    "print(f\"📁 Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512a1782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer created successfully!\")\n",
    "print(f\"🔧 Model device: {trainer.model.device}\")\n",
    "\n",
    "# Estimate training time\n",
    "steps_per_epoch = len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION)\n",
    "total_steps = steps_per_epoch * NUM_EPOCHS\n",
    "estimated_minutes = total_steps * 2 // 60  # ~2 seconds per step estimate\n",
    "\n",
    "print(f\"⏱️ Training estimates:\")\n",
    "print(f\"   Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"   Total steps: {total_steps}\")\n",
    "print(f\"   Estimated time: ~{estimated_minutes} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"🚀 Starting SmolLM3 training...\")\n",
    "print(f\"⏳ Estimated completion: ~{estimated_minutes} minutes\")\n",
    "print(\"📊 Monitor GPU usage in the sidebar →\")\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"✅ Training completed!\")\n",
    "print(f\"📊 Training metrics:\")\n",
    "print(f\"   Final loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"   Training time: {train_result.metrics.get('train_runtime', 0):.1f}s\")\n",
    "print(f\"   Samples/second: {train_result.metrics.get('train_samples_per_second', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34842fd7",
   "metadata": {},
   "source": [
    "## 📊 Model Evaluation\n",
    "\n",
    "Evaluate the trained model on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a131f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "print(\"📊 Running final evaluation...\")\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "print(f\"🎯 Final Results:\")\n",
    "print(f\"   Validation accuracy: {eval_result['eval_accuracy']:.4f}\")\n",
    "print(f\"   Validation loss: {eval_result['eval_loss']:.4f}\")\n",
    "\n",
    "# Get detailed predictions for analysis\n",
    "predictions = trainer.predict(val_dataset)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\n📈 Detailed Classification Report:\")\n",
    "target_names = [id2label[i] for i in sorted(id2label.keys())]\n",
    "report = classification_report(true_labels, predicted_labels, target_names=target_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae458051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names, \n",
    "            yticklabels=target_names)\n",
    "plt.title('SmolLM3 Financial Sentiment - Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Sample predictions\n",
    "print(f\"\\n🔍 Sample Predictions:\")\n",
    "sample_indices = np.random.choice(len(val_df), 5, replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    text = val_df.iloc[idx]['text'][:100] + \"...\"\n",
    "    true_label = id2label[true_labels[idx]]\n",
    "    pred_label = id2label[predicted_labels[idx]]\n",
    "    confidence = np.max(predictions.predictions[idx])\n",
    "    \n",
    "    print(f\"\\n{i+1}. Text: {text}\")\n",
    "    print(f\"   True: {true_label} | Predicted: {pred_label} | Confidence: {confidence:.3f}\")\n",
    "    if true_label != pred_label:\n",
    "        print(\"   ❌ Incorrect prediction\")\n",
    "    else:\n",
    "        print(\"   ✅ Correct prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2579c",
   "metadata": {},
   "source": [
    "## 💾 Save and Download Model\n",
    "\n",
    "Save the trained model and prepare for download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db87ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "print(\"💾 Saving trained model...\")\n",
    "\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save training configuration and results\n",
    "results = {\n",
    "    'training_timestamp': datetime.now().isoformat(),\n",
    "    'model_id': MODEL_ID,\n",
    "    'model_name': MODEL_NAME,\n",
    "    'num_labels': NUM_LABELS,\n",
    "    'label_mapping': label_mapping,\n",
    "    'id2label': id2label,\n",
    "    'training_config': {\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'gradient_accumulation': GRADIENT_ACCUMULATION,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "    },\n",
    "    'results': {\n",
    "        'train_loss': train_result.training_loss,\n",
    "        'eval_loss': eval_result['eval_loss'],\n",
    "        'eval_accuracy': eval_result['eval_accuracy'],\n",
    "        'train_runtime': train_result.metrics.get('train_runtime', 0),\n",
    "        'samples_per_second': train_result.metrics.get('train_samples_per_second', 0)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results JSON\n",
    "with open(f\"{output_dir}/training_results.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Save label mapping separately\n",
    "import pickle\n",
    "with open(f\"{output_dir}/label_encoder.pkl\", 'wb') as f:\n",
    "    pickle.dump({'label2id': label_mapping, 'id2label': id2label}, f)\n",
    "\n",
    "print(f\"✅ Model saved to: {output_dir}\")\n",
    "print(f\"📁 Files saved:\")\n",
    "print(f\"   - config.json\")\n",
    "print(f\"   - model.safetensors\")\n",
    "print(f\"   - tokenizer files\")\n",
    "print(f\"   - training_results.json\")\n",
    "print(f\"   - label_encoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create downloadable ZIP file\n",
    "print(\"📦 Creating download package...\")\n",
    "\n",
    "!zip -r {MODEL_NAME}.zip {output_dir}\n",
    "\n",
    "import os\n",
    "zip_size = os.path.getsize(f\"{MODEL_NAME}.zip\") / (1024**3)  # Size in GB\n",
    "\n",
    "print(f\"✅ Created {MODEL_NAME}.zip\")\n",
    "print(f\"📦 Package size: {zip_size:.2f} GB\")\n",
    "print(f\"\")\n",
    "print(f\"📥 To download your trained model:\")\n",
    "print(f\"   1. Click the Files tab (📁) on the left\")\n",
    "print(f\"   2. Find '{MODEL_NAME}.zip'\")\n",
    "print(f\"   3. Click the download icon (⬇️)\")\n",
    "print(f\"\")\n",
    "print(f\"🎊 Training complete! Your SmolLM3 model is ready.\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"🏆 SMOLLM3 TRAINING COMPLETED!\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"📊 Final Performance:\")\n",
    "print(f\"   🎯 Accuracy: {eval_result['eval_accuracy']:.1%}\")\n",
    "print(f\"   📉 Loss: {eval_result['eval_loss']:.4f}\")\n",
    "print(f\"   ⏱️ Training time: {train_result.metrics.get('train_runtime', 0)/60:.1f} minutes\")\n",
    "print(f\"\")\n",
    "print(f\"🎉 Your SmolLM3 model is now trained for financial sentiment analysis!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
